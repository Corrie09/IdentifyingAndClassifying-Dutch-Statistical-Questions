{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1. Load & Preprocess Data\n",
    "script_dir = os.getcwd() # Ga Ã©Ã©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_4004\\2837290246.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"context\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "if \"TXT_file_name\" in df.columns:\n",
    "    df = df.drop(columns=[\"TXT_file_name\"])\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "df[\"context\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\b[a-z]\\)\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\.\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = (df[\"context\"] + \" \" + df[\"question\"]).apply(clean_text)\n",
    "\n",
    "# âœ… Now: drop rare themes using original theme names\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 2].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# âœ… Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb6cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All theme_ids: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36)]\n",
      "num_labels: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"All theme_ids:\", sorted(df[\"theme_id\"].unique()))\n",
    "print(\"num_labels:\", df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling: Counter({15: 5147, 4: 1763, 9: 1699, 26: 1216, 12: 1124, 6: 949, 21: 860, 0: 855, 20: 736, 16: 641, 28: 622, 14: 612, 8: 553, 1: 527, 17: 433, 23: 332, 22: 330, 11: 323, 31: 253, 27: 253, 25: 253, 5: 253, 24: 253, 2: 253, 19: 253, 29: 253, 18: 253, 13: 253, 7: 253, 3: 253, 32: 253, 10: 253, 30: 253, 33: 253, 34: 253, 35: 253, 36: 253})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "# Compute class counts and use median as balancing target\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "median_count = theme_counts.median()\n",
    "\n",
    "# Define strategy: only oversample underrepresented classes\n",
    "sampling_strategy = {\n",
    "    theme: int(median_count)\n",
    "    for theme in theme_counts.index\n",
    "    if theme_counts[theme] < median_count\n",
    "}\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df[[\"clean_text\"]], train_df[\"theme_id\"])\n",
    "\n",
    "# Extract oversampled train lists\n",
    "train_texts_resampled = X_resampled[\"clean_text\"].tolist()\n",
    "train_labels_resampled = y_resampled.tolist()\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution after oversampling:\", Counter(train_labels_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts_resampled, train_labels_resampled, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|â–ˆâ–Ž        | 2942/23536 [10:03<1:10:54,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2549, 'grad_norm': 27.439586639404297, 'learning_rate': 1.7501699524133243e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|â–ˆâ–Ž        | 2942/23536 [10:36<1:10:54,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5774040818214417, 'eval_accuracy': 0.8469117089806745, 'eval_precision': 0.8534180514016733, 'eval_recall': 0.8469117089806745, 'eval_f1': 0.8448863153229106, 'eval_runtime': 32.6199, 'eval_samples_per_second': 161.803, 'eval_steps_per_second': 20.233, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5884/23536 [20:44<53:34,  5.49it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3911, 'grad_norm': 0.01684476248919964, 'learning_rate': 1.5003399048266487e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 5884/23536 [21:17<53:34,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4223193824291229, 'eval_accuracy': 0.892004547176961, 'eval_precision': 0.896252573460413, 'eval_recall': 0.892004547176961, 'eval_f1': 0.8914847785082272, 'eval_runtime': 33.3144, 'eval_samples_per_second': 158.43, 'eval_steps_per_second': 19.811, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8826/23536 [31:34<50:46,  4.83it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.239, 'grad_norm': 0.07002907246351242, 'learning_rate': 1.250509857239973e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8826/23536 [32:07<50:46,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4351058602333069, 'eval_accuracy': 0.9037514209928003, 'eval_precision': 0.9190272477342063, 'eval_recall': 0.9037514209928003, 'eval_f1': 0.9071099049278853, 'eval_runtime': 33.1616, 'eval_samples_per_second': 159.16, 'eval_steps_per_second': 19.903, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11768/23536 [42:24<40:51,  4.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1877, 'grad_norm': 0.024935776367783546, 'learning_rate': 1.0005948334466351e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11768/23536 [42:57<40:51,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4210189878940582, 'eval_accuracy': 0.9105721864342554, 'eval_precision': 0.9239065923337575, 'eval_recall': 0.9105721864342554, 'eval_f1': 0.9135813285090802, 'eval_runtime': 32.9112, 'eval_samples_per_second': 160.371, 'eval_steps_per_second': 20.054, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14710/23536 [53:11<30:33,  4.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1662, 'grad_norm': 0.016496390104293823, 'learning_rate': 7.5067980965329715e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14710/23536 [53:44<30:33,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40811023116111755, 'eval_accuracy': 0.911898446381205, 'eval_precision': 0.9168264792434633, 'eval_recall': 0.911898446381205, 'eval_f1': 0.9116081347360636, 'eval_runtime': 32.9026, 'eval_samples_per_second': 160.413, 'eval_steps_per_second': 20.059, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17652/23536 [1:04:00<17:57,  5.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1575, 'grad_norm': 0.002360285958275199, 'learning_rate': 5.0076478585995925e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17652/23536 [1:04:33<17:57,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4208798408508301, 'eval_accuracy': 0.9100037893141342, 'eval_precision': 0.9149758010786558, 'eval_recall': 0.9100037893141342, 'eval_f1': 0.9096044899179712, 'eval_runtime': 32.9657, 'eval_samples_per_second': 160.106, 'eval_steps_per_second': 20.021, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17652/23536 [1:04:35<21:31,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3875.4729, 'train_samples_per_second': 48.57, 'train_steps_per_second': 6.073, 'train_loss': 0.3993839425563056, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17652, training_loss=0.3993839425563056, metrics={'train_runtime': 3875.4729, 'train_samples_per_second': 48.57, 'train_steps_per_second': 6.073, 'total_flos': 3.715611280229376e+16, 'train_loss': 0.3993839425563056, 'epoch': 6.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # ðŸ”¥ Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# âœ… 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# âœ… 11. Train Model with Early Stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and tokenizer\n",
    "model.save_pretrained(\"./ImprovedModelGroNLP\")\n",
    "tokenizer.save_pretrained(\"./ImprovedModelGroNLP\")\n",
    "# Save the mapping of theme IDs to themes\n",
    "with open(\"./final_model/theme_mapping.txt\", \"w\") as f:\n",
    "    for theme_id, theme in id_to_theme.items():\n",
    "        f.write(f\"{theme_id}\\t{theme}\\n\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e3d03",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e250902",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions.predictions.argmax(axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_labels = sorted(id_to_theme.keys())  # [0, 1, 2, ..., 34]\n",
    "all_names = [id_to_theme[i] for i in all_labels]\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels=all_labels, target_names=all_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=id_to_theme.values(), yticklabels=id_to_theme.values(), cbar=True)\n",
    "plt.xlabel(\"Predicted Labels\", fontsize=14)\n",
    "plt.ylabel(\"True Labels\", fontsize=14)\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=30, ha=\"right\", fontsize=10)  # Rotate x-axis labels slightly\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32184c57",
   "metadata": {},
   "source": [
    "Handle extremely short or vague questions by labeling them \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e68926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 13. Make Predictions (With Dynamic Confidence Threshold & Short Question Handling)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probabilities = F.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "\n",
    "# âœ… Dynamically Adjust the Confidence Threshold (1st Percentile)\n",
    "confidence_values = torch.max(probabilities, dim=1)[0].tolist()\n",
    "dynamic_threshold = np.percentile(confidence_values, 1)  # âœ… Set threshold at the 5th percentile\n",
    "print(f\"Dynamic Threshold: {dynamic_threshold}\")  # âœ… Print the new threshold\n",
    "\n",
    "# âœ… Predict Themes with \"Unknown\" for Unclear Questions\n",
    "predicted_labels = []\n",
    "for i in range(len(probabilities)):\n",
    "    max_prob = torch.max(probabilities[i]).item()\n",
    "    pred_label = torch.argmax(probabilities[i]).item()\n",
    "    question_text = test_texts[i]\n",
    "\n",
    "    # âœ… If question is too short and lacks context, assign \"Unknown\"\n",
    "    if len(question_text.split()) < 4:\n",
    "        predicted_labels.append(\"Unknown\")\n",
    "    elif max_prob < dynamic_threshold:\n",
    "        predicted_labels.append(\"Unknown\")  # âœ… Filter out low-confidence predictions\n",
    "    else:\n",
    "        predicted_labels.append(id_to_theme[pred_label])  # âœ… Assign label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ffbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_count = predicted_labels.count(\"Unknown\")\n",
    "print(f\"Unknown predictions: {unknown_count} / {len(predicted_labels)} ({unknown_count/len(predicted_labels)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 14. Save Predictions \n",
    "output_df = pd.DataFrame({\n",
    "    \"Text\": test_texts,\n",
    "    \"True_Theme\": [id_to_theme[label] for label in test_labels],\n",
    "    \"Predicted_Theme\": predicted_labels\n",
    "})\n",
    "output_df[\"Correct\"] = output_df[\"True_Theme\"] == output_df[\"Predicted_Theme\"]\n",
    "output_df[\"Was_Unknown\"] = output_df[\"Predicted_Theme\"] == \"Unknown\"\n",
    "\n",
    "\n",
    "output_df.to_excel(\"BertGroNLP-theme_classification.xlsx\", index=False)\n",
    "print(\"âœ… Model Training Completed! Predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a9947-7d3f-4a26-bd4e-87f4b0659dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 15. Visualize Distribution of Predicted Themes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… Replace this with your actual predictions DataFrame\n",
    "# Example: If your predictions are stored in a variable `predicted_labels`\n",
    "# Convert it into a DataFrame for visualization\n",
    "df = pd.DataFrame({\"Predicted_Theme\": predicted_labels})\n",
    "\n",
    "# âœ… Count occurrences of each predicted theme\n",
    "label_counts = df[\"Predicted_Theme\"].value_counts()\n",
    "\n",
    "# âœ… Create the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Predicted Theme\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Assigned Labels in Model Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c19499-cb20-491f-8915-0719ff5d96f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e0b2ed8",
   "metadata": {},
   "source": [
    "TO GET ATTENTION SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b451f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_and_prediction(clean_text):\n",
    "    \"\"\"Extracts attention scores and model prediction for a given question.\"\"\"\n",
    "    model.config.output_attentions = True  # Ensure attention is enabled\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenizer_inputs = tokenizer(clean_text, return_tensors=\"pt\")  \n",
    "    tokenizer_inputs = {key: val.to(device) for key, val in tokenizer_inputs.items()}  # Move to GPU if available\n",
    "\n",
    "    # Forward pass to get attention scores and logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenizer_inputs)\n",
    "\n",
    "    attentions = outputs.attentions  # Extract attention scores\n",
    "    logits = outputs.logits  # Model prediction scores\n",
    "\n",
    "    predicted_class_id = logits.argmax(dim=1).item()  # Get predicted class ID\n",
    "    predicted_class_name = id_to_theme.get(predicted_class_id, \"Unknown\")  # Convert ID to actual class name\n",
    "\n",
    "    return attentions, predicted_class_id, predicted_class_name\n",
    "\n",
    "\n",
    "test_question = \"Hoeveel subsidies zijn toegekend aan bedrijven?\"\n",
    "attention_scores, predicted_class_id, predicted_class_name = get_attention_and_prediction(test_question)\n",
    "\n",
    "print(f\"âœ… Model predicted class: {predicted_class_name} (ID: {predicted_class_id})\")\n",
    "print(f\"ðŸ” Total Attention Layers Extracted: {len(attention_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de532c-4264-4ad0-9939-14b471ee539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention_with_class(question):\n",
    "    \"\"\"Visualizes attention scores and shows the predicted class.\"\"\"\n",
    "    attentions, predicted_class_id, predicted_class_name = get_attention_and_prediction(question)\n",
    "    \n",
    "    num_layers = len(attentions)\n",
    "    layer = num_layers - 1  # Last layer\n",
    "    head = 0  # Choose the first attention head\n",
    "\n",
    "    attention_matrix = attentions[layer][0, head].cpu().numpy()\n",
    "    tokens = tokenizer.tokenize(question)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.colorbar(label=\"Attention Score\")\n",
    "    plt.title(f\"Predicted Class: {predicted_class_name} | Attention Heatmap (Layer {layer+1}, Head {head+1})\")\n",
    "    plt.show()\n",
    "\n",
    "correctly_classified_questions = []\n",
    "\n",
    "for _, row in df.sample(100, random_state=42).iterrows():  # Test 100 random samples\n",
    "    question = row[\"clean_text\"]\n",
    "    true_class = row[\"theme\"]  # The actual correct theme\n",
    "\n",
    "    _, predicted_class_id, predicted_class_name = get_attention_and_prediction(question)\n",
    "\n",
    "    if predicted_class_name == true_class:  # âœ… Now we check for correct predictions\n",
    "        correctly_classified_questions.append((question, true_class, predicted_class_name))\n",
    "\n",
    "# Print first few correctly classified questions\n",
    "print(\"âœ… Correctly Classified Questions:\")\n",
    "for q, actual, predicted in correctly_classified_questions[:5]:\n",
    "    print(f\"ðŸ” Question: {q}\")\n",
    "    print(f\"âœ… Actual Class: {actual}\")\n",
    "    print(f\"âœ… Predicted Class: {predicted}\\n\")\n",
    "\n",
    "# âœ… If there are correct predictions, visualize one\n",
    "if correctly_classified_questions:\n",
    "    sample_correct = correctly_classified_questions[0]  # Pick first correct prediction\n",
    "    question, actual_class, predicted_class = sample_correct\n",
    "\n",
    "    print(f\"âœ… Correctly Classified Example:\")\n",
    "    print(f\"ðŸ” Question: {question}\")\n",
    "    print(f\"âœ… Actual Class: {actual_class}\")\n",
    "    print(f\"âœ… Predicted Class: {predicted_class}\")\n",
    "\n",
    "    # Visualize attention for correctly classified question\n",
    "    visualize_attention_with_class(question)\n",
    "else:\n",
    "    print(\"âŒ No correctly classified questions found in the sample!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe5b3a-2517-46b9-abf1-4843944317bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# âœ… 1. Load & Preprocess Data\n",
    "script_dir = os.getcwd()\n",
    "project_root = os.path.dirname(script_dir)\n",
    "\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "file_path = os.path.join(data_folder, \"Grote_data.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Recreate train-test split to match training\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"theme\"]\n",
    ")\n",
    "\n",
    "# Combine context and question for model input\n",
    "df_test[\"context_question\"] = df_test[\"context\"].astype(str) + \" \" + df_test[\"question\"].astype(str)\n",
    "\n",
    "# âœ… 2. Load fine-tuned model and tokenizer\n",
    "model_path = \"results/checkpoint-8823\"  # Adjust as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# âœ… 3. Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… 4. Function to get CLS embedding\n",
    "def get_cls_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.cpu().squeeze().numpy()\n",
    "\n",
    "# âœ… 5. Sample and embed from test set\n",
    "sample_df = df_test.sample(n=2000, random_state=42)\n",
    "embeddings = []\n",
    "\n",
    "print(\"Generating fine-tuned embeddings (from test set)...\")\n",
    "for text in tqdm(sample_df[\"context_question\"]):\n",
    "    emb = get_cls_embedding(text)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# âœ… 6. UMAP dimensionality reduction\n",
    "print(\"Running UMAP...\")\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "# âœ… 7. Plot UMAP\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(sample_df[\"theme\"])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels, cmap=\"tab20\", s=10, alpha=0.8)\n",
    "plt.title(\"UMAP of GroNLP BERT Embeddings (After Fine-tuning on Test Set)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.colorbar(scatter, ticks=range(len(le.classes_)), label=\"Theme\")\n",
    "plt.clim(-0.5, len(le.classes_)-0.5)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d7abe-52df-4539-8637-4f1aaa72b654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270b9cd-c990-4890-8db4-755e198372ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
