{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a038e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#met een validation en test set nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24278f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64572 entries, 0 to 64571\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      64262 non-null  object\n",
      " 1   question     64572 non-null  object\n",
      " 2   statistical  64572 non-null  int64 \n",
      " 3   theme        64572 non-null  object\n",
      " 4   file_name    64572 non-null  object\n",
      " 5   clean_text   64572 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 3.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>statistical</th>\n",
       "      <th>theme</th>\n",
       "      <th>file_name</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>2.Kan de minister toelichten op welke manier B...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister toelichten op welke manier Bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>3.Kan de minister in het bijzonder toelichten ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister in het bijzonder toelichten op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>4.Kan de minister in het bijzonder toelichten ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister in het bijzonder toelichten op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752906.txt</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "1  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "2  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "3  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "4  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "\n",
       "                                            question  statistical  \\\n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
       "1  2.Kan de minister toelichten op welke manier B...            0   \n",
       "2  3.Kan de minister in het bijzonder toelichten ...            0   \n",
       "3  4.Kan de minister in het bijzonder toelichten ...            0   \n",
       "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
       "\n",
       "                        theme    file_name  \\\n",
       "0  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "1  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "2  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "3  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "4  Brussel en de Vlaamse Rand  1752906.txt   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
       "1  Kan de minister toelichten op welke manier Bru...  \n",
       "2  Kan de minister in het bijzonder toelichten op...  \n",
       "3  Kan de minister in het bijzonder toelichten op...  \n",
       "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga Ã©Ã©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemesENnoWords9.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# âœ… Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "\n",
    "# âœ… Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. First split into train + temp\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(),\n",
    "    test_size=0.3, stratify=df[\"theme_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# 2. Then split temp into validation + test (e.g., 15% val, 15% test)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels,\n",
    "    test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : Counter({9: 12014, 2: 7021, 13: 5398, 10: 4649, 4: 3636, 6: 2900, 8: 1628, 12: 1603, 1: 1548, 14: 979, 3: 754, 7: 603, 11: 599, 16: 548, 15: 531, 0: 392, 5: 284, 17: 113})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution :\", Counter(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts, train_labels, tokenizer)\n",
    "validation_dataset = ThemeDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6578fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# âœ… Clip only the upper bound at 10\n",
    "clipped_weights = np.clip(class_weights, None, 10.0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.tensor(clipped_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f5060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training with learning rate: 5e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|â–ˆâ–Ž        | 5650/45200 [20:27<2:30:54,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8953, 'grad_norm': 13.102964401245117, 'learning_rate': 4.861602753195674e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|â–ˆâ–Ž        | 5650/45200 [21:35<2:30:54,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.488334059715271, 'eval_accuracy': 0.6395829031591989, 'eval_precision': 0.6831154781396065, 'eval_recall': 0.6395829031591989, 'eval_f1': 0.6487417233120407, 'eval_runtime': 67.2731, 'eval_samples_per_second': 143.98, 'eval_steps_per_second': 18.001, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 11300/45200 [43:21<2:10:32,  4.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2355, 'grad_norm': 13.390702247619629, 'learning_rate': 4.1674041297935105e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 11300/45200 [44:28<2:10:32,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3459296226501465, 'eval_accuracy': 0.7040057815403675, 'eval_precision': 0.7306624971544343, 'eval_recall': 0.7040057815403675, 'eval_f1': 0.7088926637883736, 'eval_runtime': 67.534, 'eval_samples_per_second': 143.424, 'eval_steps_per_second': 17.932, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 16950/45200 [1:06:09<1:51:34,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8227, 'grad_norm': 33.40299987792969, 'learning_rate': 3.473205506391347e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 16950/45200 [1:07:17<1:51:34,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4084465503692627, 'eval_accuracy': 0.7216601280198224, 'eval_precision': 0.743284822991627, 'eval_recall': 0.7216601280198224, 'eval_f1': 0.7261078858930118, 'eval_runtime': 67.3272, 'eval_samples_per_second': 143.865, 'eval_steps_per_second': 17.987, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22600/45200 [1:28:56<1:26:14,  4.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5175, 'grad_norm': 6.622307777404785, 'learning_rate': 2.7791297935103243e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22600/45200 [1:30:03<1:26:14,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6551704406738281, 'eval_accuracy': 0.7433409043981003, 'eval_precision': 0.7536451057076006, 'eval_recall': 0.7433409043981003, 'eval_f1': 0.7464229520466337, 'eval_runtime': 67.2859, 'eval_samples_per_second': 143.953, 'eval_steps_per_second': 17.998, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 28250/45200 [1:51:41<1:04:48,  4.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3057, 'grad_norm': 4.0491204261779785, 'learning_rate': 2.0849311701081615e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 28250/45200 [1:52:48<1:04:48,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9227967262268066, 'eval_accuracy': 0.7542845343795168, 'eval_precision': 0.7671956498712869, 'eval_recall': 0.7542845343795168, 'eval_f1': 0.7565983245598523, 'eval_runtime': 67.172, 'eval_samples_per_second': 144.197, 'eval_steps_per_second': 18.028, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33900/45200 [2:14:24<42:02,  4.48it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1677, 'grad_norm': 11.098971366882324, 'learning_rate': 1.3907325467059981e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33900/45200 [2:15:31<42:02,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2748053073883057, 'eval_accuracy': 0.7657443733223209, 'eval_precision': 0.7681360195808099, 'eval_recall': 0.7657443733223209, 'eval_f1': 0.7649349757392672, 'eval_runtime': 67.3787, 'eval_samples_per_second': 143.755, 'eval_steps_per_second': 17.973, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 39550/45200 [2:37:05<21:35,  4.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0795, 'grad_norm': 0.006274411920458078, 'learning_rate': 6.965339233038348e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 39550/45200 [2:38:13<21:35,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5209121704101562, 'eval_accuracy': 0.7731777823663019, 'eval_precision': 0.7726802400752839, 'eval_recall': 0.7731777823663019, 'eval_f1': 0.7713115915719093, 'eval_runtime': 68.4263, 'eval_samples_per_second': 141.554, 'eval_steps_per_second': 17.698, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 39692/45200 [2:38:47<20:47,  4.41it/s]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     16\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     17\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     warmup_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedTrainer(\n\u001b[0;32m     36\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[0;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)],\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     46\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n",
      "File \u001b[1;32mc:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mWeightedTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39m\u001b[43mclass_weights_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fct(logits, labels)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# âœ… Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# âœ… Loop over learning rates\n",
    "learning_rates = [5e-5, 3e-5, 2e-5, 1e-5]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nðŸš€ Training with learning rate: {lr}\\n\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/lr_{lr}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=8,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"./logs/lr_{lr}\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        warmup_ratio=0.1,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"learning_rate\"] = lr\n",
    "    results.append(metrics)\n",
    "\n",
    "# Print summary\n",
    "for res in results:\n",
    "    print(f\"LR: {res['learning_rate']:.1e} | F1: {res['f1']:.4f} | Accuracy: {res['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9cbf7",
   "metadata": {},
   "source": [
    "SAVE MODEL TO KUL DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b892a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1211/1211 [00:56<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved in: C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\\Run_2025-04-23_18-28\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# === Create timestamped save path in OneDrive ===\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "base_path = \"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\"\n",
    "save_path = os.path.join(base_path, f\"Run_{run_id}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Save model using Trainer ===\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# === Get predictions ===\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,  # make sure test_texts is defined\n",
    "    \"true_label\": test_labels,  # make sure test_labels is defined\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(save_path, \"test_predictions.csv\")\n",
    "output_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Optional: Save a description file ===\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "geen context meegegeven in zowel train als test\n",
    "8 epochs \n",
    "\n",
    "oversampling via jef zijn weights manier\n",
    "\n",
    "data set : Grote_data_NoDupsLessThemesENnoWords9\n",
    "\n",
    "Train test en validation set!!\n",
    "\n",
    "Meerdere learning rates geprobeerd: 5e-5, 3e-5, 2e-5, 1e-5!! \n",
    "\"\"\"\n",
    "\n",
    "desc_path = os.path.join(save_path, \"model_description.txt\")\n",
    "with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "# === Save label mappings ===\n",
    "mappings_path = os.path.join(save_path, \"label_mappings.json\")\n",
    "with open(mappings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"theme_to_id\": theme_to_id,\n",
    "        \"id_to_theme\": {str(k): v for k, v in id_to_theme.items()},  # keys must be str for JSON\n",
    "        \"unique_themes\": unique_themes  # Save the unique themes list\n",
    "\n",
    "    }, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Everything saved in: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
