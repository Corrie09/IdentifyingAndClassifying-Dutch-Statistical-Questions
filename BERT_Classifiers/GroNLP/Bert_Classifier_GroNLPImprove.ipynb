{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92503 entries, 0 to 92502\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      92010 non-null  object\n",
      " 1   question     92503 non-null  object\n",
      " 2   statistical  92503 non-null  int64 \n",
      " 3   theme        92503 non-null  object\n",
      " 4   file_name    92503 non-null  object\n",
      " 5   clean_text   92503 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>statistical</th>\n",
       "      <th>theme</th>\n",
       "      <th>file_name</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>a)Wat liep er moeilijk?</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Wat liep er moeilijk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Met welke  uitdagingen  werd BruZEL het afgelo...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Met welke uitdagingen werd BruZEL het afgelope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Hoe kunnen  die uitdagingen worden aangepakt?</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Hoe kunnen die uitdagingen worden aangepakt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>b)Wat liep er goed?</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Wat liep er goed?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "1  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "2  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "3  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "4  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "\n",
       "                                            question  statistical  \\\n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
       "1                            a)Wat liep er moeilijk?            0   \n",
       "2  Met welke  uitdagingen  werd BruZEL het afgelo...            0   \n",
       "3      Hoe kunnen  die uitdagingen worden aangepakt?            0   \n",
       "4                                b)Wat liep er goed?            0   \n",
       "\n",
       "                        theme    file_name  \\\n",
       "0  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "1  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "2  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "3  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "4  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
       "1                              Wat liep er moeilijk?  \n",
       "2  Met welke uitdagingen werd BruZEL het afgelope...  \n",
       "3       Hoe kunnen die uitdagingen worden aangepakt?  \n",
       "4                                  Wat liep er goed?  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga één map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_cleaned.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# ✅ Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "\n",
    "# ✅ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : Counter({9: 20094, 2: 11404, 13: 8481, 10: 7612, 4: 6457, 6: 4601, 1: 2595, 8: 2543, 12: 2538, 14: 1588, 3: 1124, 11: 1018, 7: 990, 16: 896, 15: 826, 0: 619, 5: 418, 17: 198})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution :\", Counter(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6578fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# ✅ Clip only the upper bound at 10\n",
    "clipped_weights = np.clip(class_weights, None, 10.0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.tensor(clipped_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f5060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 26/74008 [00:06<4:20:56,  4.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 9251/74008 [34:31<3:51:59,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0657, 'grad_norm': 90.77843475341797, 'learning_rate': 1.9446304442476017e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|█▎        | 9251/74008 [36:50<3:51:59,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.553849458694458, 'eval_accuracy': 0.6053726825576996, 'eval_precision': 0.6450895435390142, 'eval_recall': 0.6053726825576996, 'eval_f1': 0.6140857039654198, 'eval_runtime': 139.1515, 'eval_samples_per_second': 132.956, 'eval_steps_per_second': 16.622, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 18502/74008 [1:11:20<3:11:33,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3173, 'grad_norm': 0.5872083306312561, 'learning_rate': 1.666941913012146e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 25%|██▌       | 18502/74008 [1:13:39<3:11:33,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3499045372009277, 'eval_accuracy': 0.6739635695367818, 'eval_precision': 0.6917114186021248, 'eval_recall': 0.6739635695367818, 'eval_f1': 0.6788657336975742, 'eval_runtime': 139.1097, 'eval_samples_per_second': 132.996, 'eval_steps_per_second': 16.627, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 27753/74008 [1:46:13<2:40:30,  4.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8873, 'grad_norm': 54.290000915527344, 'learning_rate': 1.3892834086507426e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|███▊      | 27753/74008 [1:48:04<2:40:30,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.342568278312683, 'eval_accuracy': 0.7026647208259013, 'eval_precision': 0.7190580786999905, 'eval_recall': 0.7026647208259013, 'eval_f1': 0.7074832115433233, 'eval_runtime': 110.9042, 'eval_samples_per_second': 166.82, 'eval_steps_per_second': 20.856, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 37004/74008 [2:20:23<2:06:03,  4.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5864, 'grad_norm': 101.9086685180664, 'learning_rate': 1.1115948774152866e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|█████     | 37004/74008 [2:22:13<2:06:03,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5601619482040405, 'eval_accuracy': 0.7253121452894438, 'eval_precision': 0.7349472296626066, 'eval_recall': 0.7253121452894438, 'eval_f1': 0.7276191946643764, 'eval_runtime': 109.9423, 'eval_samples_per_second': 168.279, 'eval_steps_per_second': 21.038, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 46255/74008 [2:54:30<1:35:16,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3741, 'grad_norm': 369.644287109375, 'learning_rate': 8.339363730538833e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|██████▎   | 46255/74008 [2:56:22<1:35:16,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7569177150726318, 'eval_accuracy': 0.7422301497216367, 'eval_precision': 0.7460527666175719, 'eval_recall': 0.7422301497216367, 'eval_f1': 0.7432775393845932, 'eval_runtime': 112.0934, 'eval_samples_per_second': 165.05, 'eval_steps_per_second': 20.635, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 55506/74008 [3:28:11<1:03:11,  4.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2404, 'grad_norm': 38.62129592895508, 'learning_rate': 5.562778686924798e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 75%|███████▌  | 55506/74008 [3:30:00<1:03:11,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0182695388793945, 'eval_accuracy': 0.7520134046808281, 'eval_precision': 0.751670076006354, 'eval_recall': 0.7520134046808281, 'eval_f1': 0.7509167257397147, 'eval_runtime': 109.8365, 'eval_samples_per_second': 168.441, 'eval_steps_per_second': 21.059, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 64757/74008 [4:02:13<33:49,  4.56it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1482, 'grad_norm': 1.813162922859192, 'learning_rate': 2.7861936433107633e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|████████▊ | 64757/74008 [4:04:37<33:49,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2889840602874756, 'eval_accuracy': 0.7539051943138209, 'eval_precision': 0.7555678774791654, 'eval_recall': 0.7539051943138209, 'eval_f1': 0.7539029866301145, 'eval_runtime': 144.0263, 'eval_samples_per_second': 128.456, 'eval_steps_per_second': 16.06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74008/74008 [4:39:15<00:00,  4.64it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0903, 'grad_norm': 0.07522494345903397, 'learning_rate': 9.308330956205805e-09, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 74008/74008 [4:41:34<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3903555870056152, 'eval_accuracy': 0.7556348305497, 'eval_precision': 0.7549339611341016, 'eval_recall': 0.7556348305497, 'eval_f1': 0.7547462129256524, 'eval_runtime': 139.5657, 'eval_samples_per_second': 132.561, 'eval_steps_per_second': 16.573, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74008/74008 [4:41:38<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16898.7918, 'train_samples_per_second': 35.033, 'train_steps_per_second': 4.379, 'train_loss': 0.7137152676246783, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=74008, training_loss=0.7137152676246783, metrics={'train_runtime': 16898.7918, 'train_samples_per_second': 35.033, 'train_steps_per_second': 4.379, 'total_flos': 1.5578833150446797e+17, 'train_loss': 0.7137152676246783, 'epoch': 8.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,  # 🔥 Warmup ratio of 10% \n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ✅ 11. Train Model with Early Stopping\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9cbf7",
   "metadata": {},
   "source": [
    "SAVE MODEL TO KUL DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b892a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313/2313 [02:17<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved in: C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\\Run_2025-04-16_16-30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# === Create timestamped save path in OneDrive ===\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "base_path = \"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\"\n",
    "save_path = os.path.join(base_path, f\"Run_{run_id}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Save model using Trainer ===\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# === Get predictions ===\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,  # make sure test_texts is defined\n",
    "    \"true_label\": test_labels,  # make sure test_labels is defined\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(save_path, \"test_predictions.csv\")\n",
    "output_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Optional: Save a description file ===\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "geen context meegegeven in zowel train als test\n",
    "8 epochs \n",
    "\n",
    "oversampling via jef zijn weights manier\n",
    "\n",
    "data set : Grote_data_cleaned\n",
    "\"\"\"\n",
    "\n",
    "desc_path = os.path.join(save_path, \"model_description.txt\")\n",
    "with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "# === Save label mappings ===\n",
    "mappings_path = os.path.join(save_path, \"label_mappings.json\")\n",
    "with open(mappings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"theme_to_id\": theme_to_id,\n",
    "        \"id_to_theme\": {str(k): v for k, v in id_to_theme.items()},  # keys must be str for JSON\n",
    "        \"unique_themes\": unique_themes  # Save the unique themes list\n",
    "\n",
    "    }, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Everything saved in: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
