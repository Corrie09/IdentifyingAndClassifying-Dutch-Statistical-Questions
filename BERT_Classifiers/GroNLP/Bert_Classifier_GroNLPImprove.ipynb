{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga één map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "#nu nog naar Oud in deze mapm gaan \n",
    "data_folder = os.path.join(data_folder, \"OUD\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Kleine_data.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_19300\\4055204460.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"context\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "if \"TXT_file_name\" in df.columns:\n",
    "    df = df.drop(columns=[\"TXT_file_name\"])\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "df[\"context\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text)  # Replace newlines with spaces\n",
    "    text = re.sub(r'\\b[a-z]\\)\\s+', ' ', text)  # Remove patterns like 'a)', 'b)', etc.\n",
    "    text = re.sub(r'\\b\\d+\\.\\b', '', text)  # Remove patterns like '1.', '2.', etc.\n",
    "    text = re.sub(r'\\b\\d+\\)\\b', '', text)  # Remove patterns like '1)', '2)', etc.\n",
    "    text = re.sub(r'\\b[i]+[.)]\\b', '', text, flags=re.IGNORECASE)  # Remove patterns like 'i.', 'ii.', 'i)', etc.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and trim\n",
    "    text = re.sub(r'\\b\\d+[.)]\\s*', '', text) # Remove numeric list markers like 1., 2. or 1) 2)\n",
    "    text = re.sub(r'\\b[ivxlcdm]+\\s*[.)]\\s*', '', text, flags=re.IGNORECASE)# Remove roman numerals like i. ii. iii. or i) ii) iii)\n",
    "    return text\n",
    "\n",
    "# df[\"clean_text\"] = (df[\"context\"] + \" \" + df[\"question\"]).apply(clean_text)\n",
    "df[\"clean_text\"] = (df[\"question\"]).apply(clean_text) \n",
    "\n",
    "# Group by 'clean_text' and count unique themes\n",
    "duplicates_with_diff_themes = df.groupby(\"clean_text\")[\"theme\"].nunique().reset_index()\n",
    "\n",
    "# Filter rows where the number of unique themes is greater than 1\n",
    "duplicates_with_diff_themes = duplicates_with_diff_themes[duplicates_with_diff_themes[\"theme\"] > 1]\n",
    "\n",
    "# Merge back with the original dataframe to get all rows with these 'clean_text'\n",
    "filtered_df = df[df[\"clean_text\"].isin(duplicates_with_diff_themes[\"clean_text\"])]\n",
    "# Exclude rows with these 'clean_text' from the original dataframe\n",
    "df = df[~df[\"clean_text\"].isin(duplicates_with_diff_themes[\"clean_text\"])]\n",
    "\n",
    "\n",
    "# ✅ Now: drop rare themes using original theme names\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 2].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# ✅ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "#amount of rows \n",
    "print(f\"Number of rows after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb6cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All theme_ids: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33)]\n",
      "num_labels: 34\n"
     ]
    }
   ],
   "source": [
    "print(\"All theme_ids:\", sorted(df[\"theme_id\"].unique()))\n",
    "print(\"num_labels:\", df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling: Counter({6: 199, 15: 62, 3: 53, 2: 49, 4: 38, 16: 33, 0: 30, 13: 30, 12: 26, 21: 22, 10: 20, 11: 20, 7: 18, 14: 17, 1: 16, 28: 11, 26: 10, 19: 8, 30: 8, 5: 8, 32: 8, 31: 8, 20: 8, 27: 8, 24: 8, 8: 8, 25: 8, 9: 8, 18: 8, 23: 8, 22: 8, 29: 8, 17: 8, 33: 8})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "# Compute class counts and use median as balancing target\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "median_count = theme_counts.median()\n",
    "\n",
    "# Define strategy: only oversample underrepresented classes\n",
    "sampling_strategy = {\n",
    "    theme: int(median_count)\n",
    "    for theme in theme_counts.index\n",
    "    if theme_counts[theme] < median_count\n",
    "}\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df[[\"clean_text\"]], train_df[\"theme_id\"])\n",
    "\n",
    "# Extract oversampled train lists\n",
    "train_texts_resampled = X_resampled[\"clean_text\"].tolist()\n",
    "train_labels_resampled = y_resampled.tolist()\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution after oversampling:\", Counter(train_labels_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts_resampled, train_labels_resampled, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|█▎        | 99/792 [00:20<02:11,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0625, 'grad_norm': 10.260591506958008, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 12%|█▎        | 99/792 [00:21<02:11,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8146800994873047, 'eval_accuracy': 0.2702702702702703, 'eval_precision': 0.8027757487216947, 'eval_recall': 0.2702702702702703, 'eval_f1': 0.11500862564692352, 'eval_runtime': 1.093, 'eval_samples_per_second': 169.252, 'eval_steps_per_second': 21.957, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 198/792 [00:44<02:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7118, 'grad_norm': 14.043609619140625, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|██▌       | 198/792 [00:45<02:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6637485027313232, 'eval_accuracy': 0.31891891891891894, 'eval_precision': 0.6799429809556392, 'eval_recall': 0.31891891891891894, 'eval_f1': 0.18965448965448964, 'eval_runtime': 1.4696, 'eval_samples_per_second': 125.887, 'eval_steps_per_second': 16.331, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 297/792 [01:09<01:33,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3378, 'grad_norm': 14.297027587890625, 'learning_rate': 1.25e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 38%|███▊      | 297/792 [01:10<01:33,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5509469509124756, 'eval_accuracy': 0.3621621621621622, 'eval_precision': 0.6641560595358362, 'eval_recall': 0.3621621621621622, 'eval_f1': 0.26580772544651815, 'eval_runtime': 1.0797, 'eval_samples_per_second': 171.339, 'eval_steps_per_second': 22.228, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 396/792 [01:35<01:22,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9624, 'grad_norm': 21.137094497680664, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 396/792 [01:37<01:22,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.461827516555786, 'eval_accuracy': 0.3837837837837838, 'eval_precision': 0.6568033984575338, 'eval_recall': 0.3837837837837838, 'eval_f1': 0.29150190942509574, 'eval_runtime': 1.3999, 'eval_samples_per_second': 132.156, 'eval_steps_per_second': 17.145, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 495/792 [02:02<01:09,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6685, 'grad_norm': 15.373709678649902, 'learning_rate': 7.500000000000001e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 62%|██████▎   | 495/792 [02:04<01:09,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4651951789855957, 'eval_accuracy': 0.40540540540540543, 'eval_precision': 0.5602755661899465, 'eval_recall': 0.40540540540540543, 'eval_f1': 0.31591162113965227, 'eval_runtime': 1.7804, 'eval_samples_per_second': 103.907, 'eval_steps_per_second': 13.48, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 594/792 [02:28<00:41,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4512, 'grad_norm': 14.498296737670898, 'learning_rate': 5e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 75%|███████▌  | 594/792 [02:30<00:41,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.467015027999878, 'eval_accuracy': 0.4, 'eval_precision': 0.5470438383111833, 'eval_recall': 0.4, 'eval_f1': 0.31802950202950203, 'eval_runtime': 1.5025, 'eval_samples_per_second': 123.126, 'eval_steps_per_second': 15.973, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 693/792 [02:54<00:20,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3081, 'grad_norm': 17.768634796142578, 'learning_rate': 2.5e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 88%|████████▊ | 693/792 [02:56<00:20,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4550976753234863, 'eval_accuracy': 0.40540540540540543, 'eval_precision': 0.5261819101267075, 'eval_recall': 0.40540540540540543, 'eval_f1': 0.3286798396261859, 'eval_runtime': 1.3805, 'eval_samples_per_second': 134.007, 'eval_steps_per_second': 17.385, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [03:24<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2062, 'grad_norm': 11.219011306762695, 'learning_rate': 0.0, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 792/792 [03:25<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4522008895874023, 'eval_accuracy': 0.3945945945945946, 'eval_precision': 0.5426872102734172, 'eval_recall': 0.3945945945945946, 'eval_f1': 0.3196785550621868, 'eval_runtime': 1.57, 'eval_samples_per_second': 117.836, 'eval_steps_per_second': 15.287, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [03:29<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 209.4418, 'train_samples_per_second': 30.175, 'train_steps_per_second': 3.781, 'train_loss': 1.9635659420129024, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [03:29<00:00,  3.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=792, training_loss=1.9635659420129024, metrics={'train_runtime': 209.4418, 'train_samples_per_second': 30.175, 'train_steps_per_second': 3.781, 'total_flos': 1663339635179520.0, 'train_loss': 1.9635659420129024, 'epoch': 8.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ✅ 11. Train Model with Early Stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9cbf7",
   "metadata": {},
   "source": [
    "SAVE MODEL TO KUL DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_path = f\"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP/Run_{run_id}\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "# Add a description of the model and training details\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "geen context meegegeven in zowel train als test\n",
    "\"\"\"\n",
    "\n",
    "# Save the description to a text file\n",
    "description_file = f\"{save_path}/model_description.txt\"\n",
    "with open(description_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b892a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 23.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved in: C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\\Run_2025-04-10_12-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Create timestamped save path in OneDrive ===\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "base_path = \"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\"\n",
    "save_path = os.path.join(base_path, f\"Run_{run_id}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Save model using Trainer ===\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# === Get predictions ===\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,  # make sure test_texts is defined\n",
    "    \"true_label\": test_labels,  # make sure test_labels is defined\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(save_path, \"test_predictions.csv\")\n",
    "output_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Optional: Save a description file ===\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "[Add your description here, e.g., which layers were frozen, data splits, notes...]\n",
    "\"\"\"\n",
    "\n",
    "desc_path = os.path.join(save_path, \"model_description.txt\")\n",
    "with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "print(f\"Everything saved in: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
