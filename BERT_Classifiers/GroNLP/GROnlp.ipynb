{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a038e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#met een validation en test set nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24278f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64572 entries, 0 to 64571\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      64262 non-null  object\n",
      " 1   question     64572 non-null  object\n",
      " 2   statistical  64572 non-null  int64 \n",
      " 3   theme        64572 non-null  object\n",
      " 4   file_name    64572 non-null  object\n",
      " 5   clean_text   64572 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 3.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>statistical</th>\n",
       "      <th>theme</th>\n",
       "      <th>file_name</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>2.Kan de minister toelichten op welke manier B...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister toelichten op welke manier Bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>3.Kan de minister in het bijzonder toelichten ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister in het bijzonder toelichten op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>4.Kan de minister in het bijzonder toelichten ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752898.txt</td>\n",
       "      <td>Kan de minister in het bijzonder toelichten op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ondertussen is de eerstelijnszone BruZEL al me...</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Brussel en de Vlaamse Rand</td>\n",
       "      <td>1752906.txt</td>\n",
       "      <td>Zoals alle eerstelijnszones kreeg ook BruZEL h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "1  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "2  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "3  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "4  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
       "\n",
       "                                            question  statistical  \\\n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
       "1  2.Kan de minister toelichten op welke manier B...            0   \n",
       "2  3.Kan de minister in het bijzonder toelichten ...            0   \n",
       "3  4.Kan de minister in het bijzonder toelichten ...            0   \n",
       "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
       "\n",
       "                        theme    file_name  \\\n",
       "0  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "1  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "2  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "3  Brussel en de Vlaamse Rand  1752898.txt   \n",
       "4  Brussel en de Vlaamse Rand  1752906.txt   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
       "1  Kan de minister toelichten op welke manier Bru...  \n",
       "2  Kan de minister in het bijzonder toelichten op...  \n",
       "3  Kan de minister in het bijzonder toelichten op...  \n",
       "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga één map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemesENnoWords9.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# ✅ Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "\n",
    "# ✅ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. First split into train + temp\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(),\n",
    "    test_size=0.3, stratify=df[\"theme_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# 2. Then split temp into validation + test (e.g., 15% val, 15% test)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels,\n",
    "    test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : Counter({9: 12014, 2: 7021, 13: 5398, 10: 4649, 4: 3636, 6: 2900, 8: 1628, 12: 1603, 1: 1548, 14: 979, 3: 754, 7: 603, 11: 599, 16: 548, 15: 531, 0: 392, 5: 284, 17: 113})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution :\", Counter(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts, train_labels, tokenizer)\n",
    "validation_dataset = ThemeDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6578fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# ✅ Clip only the upper bound at 10\n",
    "clipped_weights = np.clip(class_weights, None, 10.0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.tensor(clipped_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f5060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corne\\anaconda3\\envs\\thesis-env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 12%|█▎        | 5650/45200 [20:33<2:23:27,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0439, 'grad_norm': 16.686609268188477, 'learning_rate': 1.9446411012782697e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|█▎        | 5650/45200 [21:36<2:23:27,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4395548105239868, 'eval_accuracy': 0.6529010943629981, 'eval_precision': 0.6780556239266772, 'eval_recall': 0.6529010943629981, 'eval_f1': 0.6581150489069686, 'eval_runtime': 62.3778, 'eval_samples_per_second': 155.28, 'eval_steps_per_second': 19.414, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 11300/45200 [42:26<2:08:41,  4.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2096, 'grad_norm': 10.938276290893555, 'learning_rate': 1.6669616519174043e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 25%|██▌       | 11300/45200 [43:28<2:08:41,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2796365022659302, 'eval_accuracy': 0.7033863307867024, 'eval_precision': 0.7269870438805526, 'eval_recall': 0.7033863307867024, 'eval_f1': 0.7099340321827443, 'eval_runtime': 62.4945, 'eval_samples_per_second': 154.99, 'eval_steps_per_second': 19.378, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 16950/45200 [1:03:45<1:35:53,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7541, 'grad_norm': 21.23824691772461, 'learning_rate': 1.3893313667649953e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|███▊      | 16950/45200 [1:04:42<1:35:53,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.335954189300537, 'eval_accuracy': 0.7310551311170762, 'eval_precision': 0.7431317367187728, 'eval_recall': 0.7310551311170762, 'eval_f1': 0.7337043842369924, 'eval_runtime': 57.0162, 'eval_samples_per_second': 169.882, 'eval_steps_per_second': 21.24, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 22600/45200 [1:24:28<1:17:32,  4.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4531, 'grad_norm': 5.463985919952393, 'learning_rate': 1.1116519174041298e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|█████     | 22600/45200 [1:25:26<1:17:32,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5749586820602417, 'eval_accuracy': 0.7505678298575263, 'eval_precision': 0.7600054660783852, 'eval_recall': 0.7505678298575263, 'eval_f1': 0.7526683978506985, 'eval_runtime': 57.76, 'eval_samples_per_second': 167.694, 'eval_steps_per_second': 20.966, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 28250/45200 [1:45:01<1:00:19,  4.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2602, 'grad_norm': 30.12631607055664, 'learning_rate': 8.340216322517208e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|██████▎   | 28250/45200 [1:46:02<1:00:19,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8713264465332031, 'eval_accuracy': 0.7544910179640718, 'eval_precision': 0.762680873473628, 'eval_recall': 0.7544910179640718, 'eval_f1': 0.7558934517794202, 'eval_runtime': 60.9015, 'eval_samples_per_second': 159.044, 'eval_steps_per_second': 19.885, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 33900/45200 [2:06:26<40:40,  4.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1499, 'grad_norm': 22.22987174987793, 'learning_rate': 5.5629301868239925e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 75%|███████▌  | 33900/45200 [2:07:26<40:40,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1746084690093994, 'eval_accuracy': 0.7641957464381581, 'eval_precision': 0.7644934260196995, 'eval_recall': 0.7641957464381581, 'eval_f1': 0.762540854816869, 'eval_runtime': 60.8159, 'eval_samples_per_second': 159.267, 'eval_steps_per_second': 19.913, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 39550/45200 [2:26:54<19:30,  4.83it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0798, 'grad_norm': 0.08211297541856766, 'learning_rate': 2.7861356932153396e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|████████▊ | 39550/45200 [2:27:52<19:30,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.302046298980713, 'eval_accuracy': 0.7683254181292587, 'eval_precision': 0.7680860796601368, 'eval_recall': 0.7683254181292587, 'eval_f1': 0.7675714476194, 'eval_runtime': 58.1683, 'eval_samples_per_second': 166.517, 'eval_steps_per_second': 20.819, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45200/45200 [2:48:02<00:00,  4.61it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0446, 'grad_norm': 0.07500765472650528, 'learning_rate': 8.849557522123894e-09, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 45200/45200 [2:49:03<00:00,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4323322772979736, 'eval_accuracy': 0.769977286805699, 'eval_precision': 0.7694784774420106, 'eval_recall': 0.769977286805699, 'eval_f1': 0.7689270370917042, 'eval_runtime': 61.0658, 'eval_samples_per_second': 158.616, 'eval_steps_per_second': 19.831, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45200/45200 [2:49:05<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10144.8834, 'train_samples_per_second': 35.644, 'train_steps_per_second': 4.455, 'train_loss': 0.6243972700676031, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45200, training_loss=0.6243972700676031, metrics={'train_runtime': 10144.8834, 'train_samples_per_second': 35.644, 'train_steps_per_second': 4.455, 'total_flos': 9.51546253344768e+16, 'train_loss': 0.6243972700676031, 'epoch': 8.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,  # 🔥 Warmup ratio of 10% \n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ✅ 11. Train Model with Early Stopping\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9cbf7",
   "metadata": {},
   "source": [
    "SAVE MODEL TO KUL DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b892a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1211/1211 [00:56<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved in: C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\\Run_2025-04-23_18-28\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# === Create timestamped save path in OneDrive ===\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "base_path = \"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/GroNLP\"\n",
    "save_path = os.path.join(base_path, f\"Run_{run_id}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Save model using Trainer ===\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# === Get predictions ===\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,  # make sure test_texts is defined\n",
    "    \"true_label\": test_labels,  # make sure test_labels is defined\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(save_path, \"test_predictions.csv\")\n",
    "output_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Optional: Save a description file ===\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "geen context meegegeven in zowel train als test\n",
    "8 epochs \n",
    "\n",
    "oversampling via jef zijn weights manier\n",
    "\n",
    "data set : Grote_data_NoDupsLessThemesENnoWords9\n",
    "\n",
    "Train test en validation set!! \n",
    "\"\"\"\n",
    "\n",
    "desc_path = os.path.join(save_path, \"model_description.txt\")\n",
    "with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "# === Save label mappings ===\n",
    "mappings_path = os.path.join(save_path, \"label_mappings.json\")\n",
    "with open(mappings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"theme_to_id\": theme_to_id,\n",
    "        \"id_to_theme\": {str(k): v for k, v in id_to_theme.items()},  # keys must be str for JSON\n",
    "        \"unique_themes\": unique_themes  # Save the unique themes list\n",
    "\n",
    "    }, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Everything saved in: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
