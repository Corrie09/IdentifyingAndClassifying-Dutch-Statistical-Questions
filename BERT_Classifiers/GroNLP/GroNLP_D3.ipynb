{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga √©√©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemesENnoWords9.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 64572\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# ‚úÖ Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# ‚úÖ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "# Amount of rows\n",
    "print(f\"Number of rows after filtering: {len(df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb6cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All theme_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "num_labels: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"All theme_ids:\", sorted(df[\"theme_id\"].unique()))\n",
    "print(\"num_labels:\", df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc80de23-489f-4535-867f-66bf7938048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Initial full split: 70% train, 30% temp (to be split into val + test)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(),\n",
    "    df[\"theme_id\"].tolist(),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"theme_id\"]\n",
    ")\n",
    "\n",
    "# === 2. Split temp into 15% val, 15% test\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # 50% of 30% = 15%\n",
    "    random_state=42,\n",
    "    stratify=temp_labels\n",
    ")\n",
    "\n",
    "# === 3. Save test set to Excel\n",
    "test_df = pd.DataFrame({\n",
    "    \"clean_text\": test_texts,\n",
    "    \"theme_id\": test_labels\n",
    "})\n",
    "test_df.to_excel(\"Test_data_HeldOut_15percent.xlsx\", index=False)\n",
    "\n",
    "# === Now you can continue using train_texts/train_labels and val_texts/val_labels as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : Counter({9: 12014, 2: 7021, 13: 5398, 10: 4649, 4: 3636, 6: 2900, 8: 1628, 12: 1603, 1: 1548, 14: 979, 3: 754, 7: 603, 11: 599, 16: 548, 15: 531, 0: 392, 5: 284, 17: 113})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution :\", Counter(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"GroNLP/bert-base-dutch-cased\"  # pak juistee model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = ThemeDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c47b58a8-7d86-480e-bd57-7d4d80c93341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# ‚úÖ Clip only the upper bound at 10\n",
    "clipped_weights = np.clip(class_weights, None, 10.0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.tensor(clipped_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372ca4ae-ca78-4169-9d52-4190e7cbe0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Final weights used in training:\n",
      "Class 0 ‚Üí 6.4059\n",
      "Class 1 ‚Üí 1.6222\n",
      "Class 2 ‚Üí 0.3577\n",
      "Class 3 ‚Üí 3.3304\n",
      "Class 4 ‚Üí 0.6906\n",
      "Class 5 ‚Üí 8.8419\n",
      "Class 6 ‚Üí 0.8659\n",
      "Class 7 ‚Üí 4.1644\n",
      "Class 8 ‚Üí 1.5425\n",
      "Class 9 ‚Üí 0.2090\n",
      "Class 10 ‚Üí 0.5401\n",
      "Class 11 ‚Üí 4.1922\n",
      "Class 12 ‚Üí 1.5665\n",
      "Class 13 ‚Üí 0.4652\n",
      "Class 14 ‚Üí 2.5650\n",
      "Class 15 ‚Üí 4.7290\n",
      "Class 16 ‚Üí 4.5823\n",
      "Class 17 ‚Üí 10.0000\n"
     ]
    }
   ],
   "source": [
    "# Final check before training\n",
    "print(\"‚úîÔ∏è Final weights used in training:\")\n",
    "for i, weight in enumerate(class_weights_tensor):\n",
    "    print(f\"Class {i} ‚Üí {weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f75c12-c783-4f0e-8f7d-33dac4de0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45200' max='45200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45200/45200 2:39:07, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.058400</td>\n",
       "      <td>1.456639</td>\n",
       "      <td>0.648565</td>\n",
       "      <td>0.673132</td>\n",
       "      <td>0.648565</td>\n",
       "      <td>0.652550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.209100</td>\n",
       "      <td>1.285230</td>\n",
       "      <td>0.706690</td>\n",
       "      <td>0.722224</td>\n",
       "      <td>0.706690</td>\n",
       "      <td>0.710868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>1.389260</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>0.741352</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>0.729150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>1.575372</td>\n",
       "      <td>0.752426</td>\n",
       "      <td>0.756742</td>\n",
       "      <td>0.752426</td>\n",
       "      <td>0.753226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>1.869353</td>\n",
       "      <td>0.757382</td>\n",
       "      <td>0.763142</td>\n",
       "      <td>0.757382</td>\n",
       "      <td>0.758061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>2.218310</td>\n",
       "      <td>0.769151</td>\n",
       "      <td>0.768985</td>\n",
       "      <td>0.769151</td>\n",
       "      <td>0.767338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>2.365569</td>\n",
       "      <td>0.769977</td>\n",
       "      <td>0.770272</td>\n",
       "      <td>0.769977</td>\n",
       "      <td>0.768970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>2.412918</td>\n",
       "      <td>0.772249</td>\n",
       "      <td>0.770876</td>\n",
       "      <td>0.772249</td>\n",
       "      <td>0.770697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45200, training_loss=0.6212003677503197, metrics={'train_runtime': 9548.5294, 'train_samples_per_second': 37.87, 'train_steps_per_second': 4.734, 'total_flos': 9.51546253344768e+16, 'train_loss': 0.6212003677503197, 'epoch': 8.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # üëà Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # üî• Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# ‚úÖ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ‚úÖ 11. Train Model with Early Stopping\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396b662",
   "metadata": {},
   "source": [
    "SAVE MODEL NAAR KUL DRIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae01019-726b-4ec3-913c-eac107730a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"output/grobertd30\")#naam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "762c4c18-4b05-43f6-83e4-69e8afb80976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved in: C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\mBERT\\Run_2025-04-23_20-11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# === Create timestamped save path in OneDrive ===\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_path = rf\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\mBERT\\Run_{run_id}\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Save model using Trainer ===\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# === Get predictions ===\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "logits = preds_output.predictions\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": val_texts,  # make sure val_texts is defined\n",
    "    \"true_label\": val_labels,  # make sure val_labels is defined\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(save_path, \"val_predictions.csv\")\n",
    "output_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Optional: Save a description file ===\n",
    "description = \"\"\"\n",
    "Model: GroNLP BERT-based Dutch Cased\n",
    "Training Details:\n",
    "geen context meegegeven in zowel train als val\n",
    "5 epochs \n",
    "\n",
    "data set : Grote_data_NoDupsLessThemesENnoWords9\n",
    "\"\"\"\n",
    "\n",
    "desc_path = os.path.join(save_path, \"model_description.txt\")\n",
    "with open(desc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(description)\n",
    "\n",
    "# === Save label mappings ===\n",
    "mappings_path = os.path.join(save_path, \"label_mappings.json\")\n",
    "with open(mappings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"theme_to_id\": theme_to_id,\n",
    "        \"id_to_theme\": {str(k): v for k, v in id_to_theme.items()},  # keys must be str for JSON\n",
    "        \"unique_themes\": unique_themes  # Save the unique themes list\n",
    "\n",
    "    }, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Everything saved in: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97059c32-6cec-43c9-bd47-3cb9a052e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
