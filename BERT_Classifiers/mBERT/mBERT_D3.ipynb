{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga √©√©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemesENnoWords9.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 64572\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# ‚úÖ Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# ‚úÖ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "# Amount of rows\n",
    "print(f\"Number of rows after filtering: {len(df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb6cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All theme_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "num_labels: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"All theme_ids:\", sorted(df[\"theme_id\"].unique()))\n",
    "print(\"num_labels:\", df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : Counter({9: 13730, 2: 8024, 13: 6169, 10: 5314, 4: 4156, 6: 3314, 8: 1860, 12: 1832, 1: 1769, 14: 1119, 3: 862, 7: 689, 11: 684, 16: 626, 15: 607, 0: 448, 5: 324, 17: 130})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution :\", Counter(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"bert-base-multilingual-cased\"  # pak juistee model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47b58a8-7d86-480e-bd57-7d4d80c93341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# ‚úÖ Clip only the upper bound at 10\n",
    "clipped_weights = np.clip(class_weights, None, 10.0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.tensor(clipped_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372ca4ae-ca78-4169-9d52-4190e7cbe0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Final weights used in training:\n",
      "Class 0 ‚Üí 6.4059\n",
      "Class 1 ‚Üí 1.6223\n",
      "Class 2 ‚Üí 0.3577\n",
      "Class 3 ‚Üí 3.3293\n",
      "Class 4 ‚Üí 0.6905\n",
      "Class 5 ‚Üí 8.8575\n",
      "Class 6 ‚Üí 0.8660\n",
      "Class 7 ‚Üí 4.1652\n",
      "Class 8 ‚Üí 1.5429\n",
      "Class 9 ‚Üí 0.2090\n",
      "Class 10 ‚Üí 0.5401\n",
      "Class 11 ‚Üí 4.1957\n",
      "Class 12 ‚Üí 1.5665\n",
      "Class 13 ‚Üí 0.4652\n",
      "Class 14 ‚Üí 2.5646\n",
      "Class 15 ‚Üí 4.7279\n",
      "Class 16 ‚Üí 4.5844\n",
      "Class 17 ‚Üí 10.0000\n"
     ]
    }
   ],
   "source": [
    "# Final check before training\n",
    "print(\"‚úîÔ∏è Final weights used in training:\")\n",
    "for i, weight in enumerate(class_weights_tensor):\n",
    "    print(f\"Class {i} ‚Üí {weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f75c12-c783-4f0e-8f7d-33dac4de0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51664' max='51664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51664/51664 3:30:03, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.181600</td>\n",
       "      <td>1.665231</td>\n",
       "      <td>0.616338</td>\n",
       "      <td>0.633832</td>\n",
       "      <td>0.616338</td>\n",
       "      <td>0.619618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.469600</td>\n",
       "      <td>1.465736</td>\n",
       "      <td>0.672396</td>\n",
       "      <td>0.691204</td>\n",
       "      <td>0.672396</td>\n",
       "      <td>0.673164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.117600</td>\n",
       "      <td>1.326249</td>\n",
       "      <td>0.702749</td>\n",
       "      <td>0.719405</td>\n",
       "      <td>0.702749</td>\n",
       "      <td>0.705966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>1.344424</td>\n",
       "      <td>0.715679</td>\n",
       "      <td>0.738099</td>\n",
       "      <td>0.715679</td>\n",
       "      <td>0.721119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.632200</td>\n",
       "      <td>1.492513</td>\n",
       "      <td>0.742702</td>\n",
       "      <td>0.752797</td>\n",
       "      <td>0.742702</td>\n",
       "      <td>0.745805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>1.628850</td>\n",
       "      <td>0.752458</td>\n",
       "      <td>0.758505</td>\n",
       "      <td>0.752458</td>\n",
       "      <td>0.754223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>1.833287</td>\n",
       "      <td>0.763995</td>\n",
       "      <td>0.764983</td>\n",
       "      <td>0.763995</td>\n",
       "      <td>0.763637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>1.932284</td>\n",
       "      <td>0.767944</td>\n",
       "      <td>0.767084</td>\n",
       "      <td>0.767944</td>\n",
       "      <td>0.767149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51664, training_loss=0.9019814541776249, metrics={'train_runtime': 12604.4937, 'train_samples_per_second': 32.786, 'train_steps_per_second': 4.099, 'total_flos': 1.0874784249785549e+17, 'train_loss': 0.9019814541776249, 'epoch': 8.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # üëà Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # üî• Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# ‚úÖ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ‚úÖ 11. Train Model with Early Stopping\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396b662",
   "metadata": {},
   "source": [
    "SAVE MODEL NAAR KUL DRIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae01019-726b-4ec3-913c-eac107730a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"output/mbertd3\")#naam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b491b88-db56-4791-8747-82e5127928a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract logits (raw model outputs)\n",
    "logits = preds_output.predictions  # shape: (num_samples, num_classes)\n",
    "\n",
    "# Get predicted class labels\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Save everything in a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"true_label\": test_labels,\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()  # Save logits for softmax + threshold later\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"output/mbertd3p.csv\", index=False)#naam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4e3fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\jefva\\\\Documents\\\\Master\\\\Thesis_s2\\\\results\\\\mBERT\\\\Run_2025-04-17_04-09\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\jefva\\\\Documents\\\\Master\\\\Thesis_s2\\\\results\\\\mBERT\\\\Run_2025-04-17_04-09\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\jefva\\\\Documents\\\\Master\\\\Thesis_s2\\\\results\\\\mBERT\\\\Run_2025-04-17_04-09\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\jefva\\\\Documents\\\\Master\\\\Thesis_s2\\\\results\\\\mBERT\\\\Run_2025-04-17_04-09\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_path = rf\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\mBERT\\Run_{run_id}\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c4c18-4b05-43f6-83e4-69e8afb80976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "0 S3021>Q hup-/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
