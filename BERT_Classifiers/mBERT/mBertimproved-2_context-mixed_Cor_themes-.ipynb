{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f802ec-808c-4c4e-b6da-7d0e38ab4de0",
   "metadata": {},
   "source": [
    "**Bert model op basis van Bert van Universiteit van Groningen. Context handling moet nog aangepast worden. Oversampling a.d.h.v de mediaan. dynamisch treshhold zoeken voor unknown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24278f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd()) # Ga één map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_cleaned.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b18e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\AppData\\Local\\Temp\\ipykernel_4652\\2724978222.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"context\"].fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 26844\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [ \"file_name\", \"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "df[\"context\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# Keep only the first occurrence of each unique context\n",
    "df = df.drop_duplicates(subset=\"context\", keep=\"first\")\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\b[a-z]\\)\\s*', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\.\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\)\\b', '', text)\n",
    "    text = re.sub(r'\\b[i]+[.)]\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+[.)]\\s*', '', text)\n",
    "    text = re.sub(r'\\b[ivxlcdm]+\\s*[.)]\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'•', '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Combine context and question, then clean\n",
    "df[\"clean_text\"] = (df[\"context\"] + \" \" + df[\"question\"]).apply(clean_text)\n",
    "\n",
    "# Remove rows with the same clean_text but different themes\n",
    "duplicates_with_diff_themes = df.groupby(\"clean_text\")[\"theme\"].nunique().reset_index()\n",
    "duplicates_with_diff_themes = duplicates_with_diff_themes[duplicates_with_diff_themes[\"theme\"] > 1]\n",
    "df = df[~df[\"clean_text\"].isin(duplicates_with_diff_themes[\"clean_text\"])]\n",
    "\n",
    "# Map detailed themes to broader theme clusters\n",
    "theme_merge_map = {\n",
    "    # Bestuur en Beleid\n",
    "    \"Lokale overheden en Binnenlands bestuur\": \"Bestuur en Beleid\",\n",
    "    \"Vlaamse administratie\": \"Bestuur en Beleid\",\n",
    "    \"Staatshervorming en Verhoudingen binnen de Belgische federale staat\": \"Bestuur en Beleid\",\n",
    "    \"Vlaamse Regering\": \"Bestuur en Beleid\",\n",
    "    \"Vlaams Parlement\": \"Bestuur en Beleid\",\n",
    "\n",
    "    # Mobiliteit en Infrastructuur\n",
    "    \"Mobiliteit en Verkeer\": \"Mobiliteit en Infrastructuur\",\n",
    "    \"Openbare werken\": \"Mobiliteit en Infrastructuur\",\n",
    "    \"Ruimtelijke ordening\": \"Mobiliteit en Infrastructuur\",\n",
    "\n",
    "    # Economie en Arbeid\n",
    "    \"Werk\": \"Economie en Arbeid\",\n",
    "    \"Economie\": \"Economie en Arbeid\",\n",
    "    \"Sociale economie\": \"Economie en Arbeid\",\n",
    "    \"Internationaal ondernemen\": \"Economie en Arbeid\",\n",
    "\n",
    "    # Welzijn en Gezondheid\n",
    "    \"Welzijn en Gezin\": \"Welzijn en Gezondheid\",\n",
    "    \"Gezondheid\": \"Welzijn en Gezondheid\",\n",
    "    \"Armoedebeleid\": \"Welzijn en Gezondheid\",\n",
    "\n",
    "    # Cultuur en Communicatie\n",
    "    \"Cultuur\": \"Cultuur en Communicatie\",\n",
    "    \"Media\": \"Cultuur en Communicatie\",\n",
    "    \"Taalgebruik\": \"Cultuur en Communicatie\",\n",
    "\n",
    "    # Onderwijs en Samenleving\n",
    "    \"Onderwijs en Vorming\": \"Onderwijs en Samenleving\",\n",
    "    \"Gelijke kansen\": \"Onderwijs en Samenleving\",\n",
    "    \"Jeugdbeleid\": \"Onderwijs en Samenleving\",\n",
    "    \"Integratie en Inburgering\": \"Onderwijs en Samenleving\",\n",
    "\n",
    "    # Milieu en Landbouw\n",
    "    \"Natuur en Milieu\": \"Milieu en Landbouw\",\n",
    "    \"Landbouw\": \"Milieu en Landbouw\",\n",
    "    \"Dierenwelzijn\": \"Milieu en Landbouw\",\n",
    "\n",
    "    # Internationaal Beleid\n",
    "    \"Buitenlands beleid\": \"Internationaal Beleid\",\n",
    "    \"Europese instellingen\": \"Internationaal Beleid\",\n",
    "    \"Ontwikkelingssamenwerking\": \"Internationaal Beleid\",\n",
    "    \"Oekraïnecrisis\": \"Internationaal Beleid\",\n",
    "\n",
    "    # Overige\n",
    "    \"Financiën\": \"Financiën\",\n",
    "    \"Begroting\": \"Begroting\",\n",
    "    \"Wetenschap en Innovatie\": \"Wetenschap en Innovatie\",\n",
    "    \"Toerisme\": \"Toerisme\",\n",
    "    \"Justitie en Handhaving\": \"Justitie en Handhaving\",\n",
    "    \"Brussel en de Vlaamse Rand\": \"Brussel en de Vlaamse Rand\",\n",
    "    \"Sport\": \"Sport\",\n",
    "    \"Onroerend erfgoed\": \"Onroerend erfgoed\",\n",
    "    \"Energie\": \"Energie\",\n",
    "    \"Wonen\": \"Wonen\",\n",
    "}\n",
    "\n",
    "# Apply theme mapping\n",
    "df[\"theme\"] = df[\"theme\"].map(theme_merge_map).fillna(\"Onbekend\")\n",
    "\n",
    "# Remove rare themes\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 2].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# Recompute theme IDs\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "# Final row count\n",
    "print(f\"Number of rows after filtering: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb6cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All theme_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "num_labels: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"All theme_ids:\", sorted(df[\"theme_id\"].unique()))\n",
    "print(\"num_labels:\", df[\"theme_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 5. Split Data into Train & Test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), df[\"theme_id\"].tolist(), test_size=0.2, random_state=42, stratify=df[\"theme_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8a6ccc-1c79-45ea-b1f6-c1cf5a03137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔻 Top class 9 has 5418 samples\n",
      "✅ Removed 2709 samples from top class 9\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Step 1: Identify the top class\n",
    "theme_counts = Counter(train_labels)\n",
    "top_class = max(theme_counts, key=theme_counts.get)\n",
    "top_class_count = theme_counts[top_class]\n",
    "print(f\"🔻 Top class {top_class} has {top_class_count} samples\")\n",
    "\n",
    "# Step 2: Get all indices for that class\n",
    "top_class_indices = [i for i, label in enumerate(train_labels) if label == top_class]\n",
    "\n",
    "# Step 3: Randomly remove half\n",
    "random.seed(42)\n",
    "drop_count = len(top_class_indices) // 2\n",
    "drop_indices = set(random.sample(top_class_indices, drop_count))\n",
    "\n",
    "# Step 4: Filter train_texts and train_labels\n",
    "train_texts = [text for i, text in enumerate(train_texts) if i not in drop_indices]\n",
    "train_labels = [label for i, label in enumerate(train_labels) if i not in drop_indices]\n",
    "\n",
    "print(f\"✅ Removed {drop_count} samples from top class {top_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2626eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling: Counter({2: 3221, 9: 2709, 13: 2706, 10: 2413, 4: 1837, 6: 1198, 8: 873, 12: 805, 1: 752, 11: 572, 17: 572, 15: 572, 7: 572, 14: 572, 0: 572, 16: 572, 3: 572, 5: 572})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame from train lists\n",
    "train_df = pd.DataFrame({\n",
    "    \"clean_text\": train_texts,\n",
    "    \"theme_id\": train_labels\n",
    "})\n",
    "\n",
    "# Compute class counts and use median as balancing target\n",
    "theme_counts = train_df[\"theme_id\"].value_counts()\n",
    "median_count = theme_counts.median()\n",
    "\n",
    "# Define strategy: only oversample underrepresented classes\n",
    "sampling_strategy = {\n",
    "    theme: int(median_count)\n",
    "    for theme in theme_counts.index\n",
    "    if theme_counts[theme] < median_count and theme_counts[theme] > 50 \n",
    "}\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df[[\"clean_text\"]], train_df[\"theme_id\"])\n",
    "\n",
    "# Extract oversampled train lists\n",
    "train_texts_resampled = X_resampled[\"clean_text\"].tolist()\n",
    "train_labels_resampled = y_resampled.tolist()\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Class distribution after oversampling:\", Counter(train_labels_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b392dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. Load BERT Tokenizer & Define Dataset Class\n",
    "model_name = \"bert-base-multilingual-cased\"  # mBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class ThemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "train_dataset = ThemeDataset(train_texts_resampled, train_labels_resampled, tokenizer)\n",
    "test_dataset = ThemeDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebcd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 8. Load BERT Model for Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=df[\"theme_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b89b50-9404-4854-8a49-29f7dd3485da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "# Assuming train_labels is a list of class indices\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels_resampled),\n",
    "    y=train_labels_resampled\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "clipped_weights = torch.clamp(class_weights, min=0.5, max=10.0)#normalizing is ook een optie maar zitten hier met enorm skewed data dus clipping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b604b5cf-536e-4b49-8bf1-93e79cb8c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=clipped_weights.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7423c6f4-03c2-42c7-93bd-ca27f376ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21664' max='21664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21664/21664 1:32:55, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.363600</td>\n",
       "      <td>1.037689</td>\n",
       "      <td>0.742596</td>\n",
       "      <td>0.764577</td>\n",
       "      <td>0.742596</td>\n",
       "      <td>0.750306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.954611</td>\n",
       "      <td>0.789719</td>\n",
       "      <td>0.793773</td>\n",
       "      <td>0.789719</td>\n",
       "      <td>0.790845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>0.989457</td>\n",
       "      <td>0.800149</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>0.800149</td>\n",
       "      <td>0.801951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>1.142808</td>\n",
       "      <td>0.808344</td>\n",
       "      <td>0.816379</td>\n",
       "      <td>0.808344</td>\n",
       "      <td>0.809066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>1.175871</td>\n",
       "      <td>0.818402</td>\n",
       "      <td>0.822422</td>\n",
       "      <td>0.818402</td>\n",
       "      <td>0.818848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>1.303174</td>\n",
       "      <td>0.826970</td>\n",
       "      <td>0.827599</td>\n",
       "      <td>0.826970</td>\n",
       "      <td>0.826348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>1.391179</td>\n",
       "      <td>0.826411</td>\n",
       "      <td>0.829281</td>\n",
       "      <td>0.826411</td>\n",
       "      <td>0.826457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>1.416813</td>\n",
       "      <td>0.829577</td>\n",
       "      <td>0.830971</td>\n",
       "      <td>0.829577</td>\n",
       "      <td>0.829483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21664, training_loss=0.38979324667633375, metrics={'train_runtime': 5575.898, 'train_samples_per_second': 31.079, 'train_steps_per_second': 3.885, 'total_flos': 4.560264367246541e+16, 'train_loss': 0.38979324667633375, 'epoch': 8.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 9. Define Training Arguments (With Early Stopping)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ✅ 11. Train Model with Early Stopping\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9be543-99e6-4455-a0c7-125788cb1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"output/mixed-bert-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66cac160-49ce-4964-a846-afe1868a5ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract logits (raw model outputs)\n",
    "logits = preds_output.predictions  # shape: (num_samples, num_classes)\n",
    "\n",
    "# Get predicted class labels\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Save everything in a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"true_label\": test_labels,\n",
    "    \"predicted_label\": predictions,\n",
    "    \"logits\": logits.tolist()  # Save logits for softmax + threshold later\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"output/test_predictions_mixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25664543-e879-40a0-b928-b73621d50cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Final weights used in training:\n",
      "Class 0 → 2.1039\n",
      "Class 1 → 1.6003\n",
      "Class 2 → 0.3736\n",
      "Class 3 → 2.1039\n",
      "Class 4 → 0.6551\n",
      "Class 5 → 2.1039\n",
      "Class 6 → 1.0045\n",
      "Class 7 → 2.1039\n",
      "Class 8 → 1.3785\n",
      "Class 9 → 0.4442\n",
      "Class 10 → 0.4987\n",
      "Class 11 → 2.1039\n",
      "Class 12 → 1.4950\n",
      "Class 13 → 0.4447\n",
      "Class 14 → 2.1039\n",
      "Class 15 → 2.1039\n",
      "Class 16 → 2.1039\n",
      "Class 17 → 2.1039\n"
     ]
    }
   ],
   "source": [
    "# Final check before training\n",
    "print(\"✔️ Final weights used in training:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"Class {i} → {weight.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340a4b5d-2079-4b1f-8af8-84795f4d20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Final weights used in training:\n",
      "Class 0 → 0.5527\n",
      "Class 1 → 0.5000\n",
      "Class 2 → 0.6322\n",
      "Class 3 → 0.5000\n",
      "Class 4 → 7.4268\n",
      "Class 5 → 8.7315\n",
      "Class 6 → 0.7557\n",
      "Class 7 → 1.2795\n",
      "Class 8 → 1.7654\n",
      "Class 9 → 0.5000\n",
      "Class 10 → 1.0191\n",
      "Class 11 → 4.0132\n",
      "Class 12 → 2.3842\n",
      "Class 13 → 2.6054\n",
      "Class 14 → 2.2280\n",
      "Class 15 → 4.4870\n",
      "Class 16 → 0.5000\n",
      "Class 17 → 0.9049\n",
      "Class 18 → 0.6335\n",
      "Class 19 → 0.5000\n",
      "Class 20 → 10.0000\n",
      "Class 21 → 1.1684\n",
      "Class 22 → 1.0112\n",
      "Class 23 → 1.6525\n",
      "Class 24 → 1.9759\n",
      "Class 25 → 0.5000\n",
      "Class 26 → 0.8755\n",
      "Class 27 → 1.2571\n",
      "Class 28 → 4.1957\n",
      "Class 29 → 10.0000\n",
      "Class 30 → 10.0000\n",
      "Class 31 → 10.0000\n",
      "Class 32 → 4.0637\n",
      "Class 33 → 5.2531\n",
      "Class 34 → 10.0000\n",
      "Class 35 → 2.6159\n",
      "Class 36 → 7.5132\n",
      "Class 37 → 10.0000\n"
     ]
    }
   ],
   "source": [
    "# Final check before training\n",
    "print(\"✔️ Final weights used in training:\")\n",
    "for i, weight in enumerate(clipped_weights):\n",
    "    print(f\"Class {i} → {weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396b662",
   "metadata": {},
   "source": [
    "SAVE MODEL NAAR KUL DRIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a4e3fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m run_id \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#save_path = f\"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/mBERT/Run_{run_id}\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[43msave_path\u001b[49m)\n\u001b[0;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(save_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "#save_path = f\"C:/Users/corne/OneDrive - KU Leuven/Thesis/Working Code/SAVED-Models/mBERT/Run_{run_id}\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
