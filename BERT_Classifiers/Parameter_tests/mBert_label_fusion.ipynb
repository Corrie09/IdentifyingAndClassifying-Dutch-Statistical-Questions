{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cc9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0881c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\AppData\\Local\\Temp\\ipykernel_11224\\3487412192.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"context\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd())\n",
    "project_root = os.path.dirname(script_dir)\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "file_path = os.path.join(data_folder, \"Grote_data.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "if \"TXT_file_name\" in df.columns:\n",
    "    df = df.drop(columns=[\"TXT_file_name\"])\n",
    "\n",
    "df = df.dropna(subset=[\"question\"])\n",
    "df[\"context\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# âœ… 2. Clean text\n",
    "dutch_stopwords = {\"de\", \"het\", \"een\", \"en\", \"van\", \"ik\", \"te\", \"dat\", \"die\", \"in\", \"je\", \"is\",\n",
    "                   \"niet\", \"op\", \"aan\", \"met\", \"als\", \"voor\", \"zijn\", \"was\", \"heeft\", \"heb\",\n",
    "                   \"om\", \"bij\", \"of\", \"geen\", \"dan\", \"toch\", \"maar\", \"wel\", \"meer\", \"doen\",\n",
    "                   \"ook\", \"kan\", \"mijn\", \"zo\", \"dus\", \"zou\", \"kunnen\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\b[a-z]\\)\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\.\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = \" \".join([word for word in text.split() if word not in dutch_stopwords])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = (df[\"context\"] + \" \" + df[\"question\"]).apply(clean_text)\n",
    "\n",
    "# âœ… 3. Encode theme labels\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5da32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… 4. Oversampling to fix class imbalance\n",
    "theme_counts = df[\"theme_id\"].value_counts()\n",
    "median_count = theme_counts.median()\n",
    "\n",
    "sampling_strategy = {theme: int(median_count) for theme in theme_counts.index if theme_counts[theme] < median_count}\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(df[[\"clean_text\"]], df[\"theme_id\"])\n",
    "df_resampled = pd.DataFrame({\"clean_text\": X_resampled[\"clean_text\"], \"theme_id\": y_resampled})\n",
    "\n",
    "# âœ… 5. Split Data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_resampled[\"clean_text\"].tolist(),\n",
    "    df_resampled[\"theme_id\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=df_resampled[\"theme_id\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# âœ… 6. Load tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "label_texts = [tokenizer.cls_token + \" \" + theme for theme in unique_themes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50ad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabelFusionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, label_texts, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_texts = label_texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_input = \" \".join([tokenizer.sep_token + \" \" + label_text for label_text in self.label_texts])\n",
    "        full_input = input_text + \" \" + label_input\n",
    "        encoding = self.tokenizer(\n",
    "            full_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        encoding[\"label_id\"] = torch.tensor(label)\n",
    "        return encoding\n",
    "\n",
    "train_dataset = LabelFusionDataset(train_texts, train_labels, tokenizer, unique_themes)\n",
    "test_dataset = LabelFusionDataset(test_texts, test_labels, tokenizer, unique_themes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7258b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabelFusionBERT(nn.Module):\n",
    "    def __init__(self, model_name, label_texts, tokenizer):\n",
    "        super(LabelFusionBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_texts = label_texts\n",
    "        self.label_embeddings = self.get_label_embeddings()\n",
    "\n",
    "    def get_label_embeddings(self):\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                self.label_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=20\n",
    "            )\n",
    "            outputs = self.bert(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_id=None):\n",
    "        device = input_ids.device\n",
    "        label_embeddings = self.label_embeddings.to(device)  # move to same device as inputs\n",
    "    \n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = output.last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "    \n",
    "        similarities = F.cosine_similarity(cls_embedding.unsqueeze(1), label_embeddings.unsqueeze(0), dim=-1)\n",
    "    \n",
    "        if label_id is not None:\n",
    "            loss = F.cross_entropy(similarities, label_id)\n",
    "            return {\"loss\": loss, \"logits\": similarities}\n",
    "        return {\"logits\": similarities}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf425d1-ced3-4271-ab5e-74108b61b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelFusionBERT(model_name, unique_themes, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0ffb29-9c78-40e2-af71-46ab8fb894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b30fbb8-0fc5-495a-9fab-2e4b53f3c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8574f2-8993-42c1-862f-3a7d6fdabbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d494c4bb-21cc-448b-9739-8a3a4aea7794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20594' max='29410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20594/29410 2:50:13 < 1:12:52, 2.02 it/s, Epoch 7.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.417600</td>\n",
       "      <td>3.350542</td>\n",
       "      <td>0.736779</td>\n",
       "      <td>0.772856</td>\n",
       "      <td>0.736779</td>\n",
       "      <td>0.733119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.339400</td>\n",
       "      <td>3.323322</td>\n",
       "      <td>0.818398</td>\n",
       "      <td>0.849661</td>\n",
       "      <td>0.818398</td>\n",
       "      <td>0.827357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.317100</td>\n",
       "      <td>3.314909</td>\n",
       "      <td>0.846625</td>\n",
       "      <td>0.876541</td>\n",
       "      <td>0.846625</td>\n",
       "      <td>0.855769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.306400</td>\n",
       "      <td>3.307475</td>\n",
       "      <td>0.871280</td>\n",
       "      <td>0.896039</td>\n",
       "      <td>0.871280</td>\n",
       "      <td>0.879005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.300200</td>\n",
       "      <td>3.304561</td>\n",
       "      <td>0.881143</td>\n",
       "      <td>0.909917</td>\n",
       "      <td>0.881143</td>\n",
       "      <td>0.890619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.296000</td>\n",
       "      <td>3.301999</td>\n",
       "      <td>0.887944</td>\n",
       "      <td>0.917199</td>\n",
       "      <td>0.887944</td>\n",
       "      <td>0.897625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.293000</td>\n",
       "      <td>3.301331</td>\n",
       "      <td>0.890495</td>\n",
       "      <td>0.917087</td>\n",
       "      <td>0.890495</td>\n",
       "      <td>0.899262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\trainer.py:2550\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m-> 2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311e06a9-6c27-4dc4-bf20-8a0149435d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.30133056640625,\n",
       " 'eval_accuracy': 0.8904948138071757,\n",
       " 'eval_precision': 0.9170868244134099,\n",
       " 'eval_recall': 0.8904948138071757,\n",
       " 'eval_f1': 0.8992617803215309}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d8f28-d40c-4f13-87da-c63bc4eba784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
