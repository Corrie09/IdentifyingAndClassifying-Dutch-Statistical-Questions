{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38aa0383-3196-4786-8538-7a83ae3c1b91",
   "metadata": {},
   "source": [
    "**Een Identifier die gebruik maakt van active learning. 500 vragen zijn zelf gelabeled. Vervolgens fine tunen we een bert model hierop. We laten deze getunede bert dan zelf 1000 ongeziene vragen labelen. We kijken naar de 20% waar het model het minst zeker van is. Die kijken we handmatig na. De gecorrigeerde voegen we dan toe aan de training data en zo herhalen we dit proces tot het bert model naar behoren werkt.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcdcad8-89e7-4a00-a616-6beae8aea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457b33d1-be39-4fd0-8a00-ff8d4044e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load labeled data\n",
    "df = pd.read_excel(\"Trainig_data.xlsx\")  \n",
    "df.columns = ['question', 'label']  \n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['question'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df90c5da-3f2c-46f4-ab38-65d32c469b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 44\n",
      "Tokens: ['Zal', 'de', 'minister', 'initiatie', '##ven', 'nemen', 'om', 'ervoor', 'te', 'zorgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zorgt', '##ra', '##ject', 'in', 'de', 'toekomst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'tegemoetkoming', 'van', 'hulpmiddel', '##en', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'termijn', '?']\n",
      "Number of tokens: 55\n",
      "Tokens: ['Za', '##l', 'de', 'minister', 'init', '##iati', '##even', 'nemen', 'om', 'ervoor', 'te', 'zo', '##rgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zo', '##rgt', '##raj', '##ect', 'in', 'de', 'toe', '##komst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'te', '##gem', '##oe', '##tkom', '##ing', 'van', 'hulp', '##mid', '##delen', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'term', '##ijn', '?']\n",
      "Number of tokens: 40\n",
      "Tokens: ['Zal', 'Ä de', 'Ä minister', 'Ä initiatieven', 'Ä nemen', 'Ä om', 'Ä ervoor', 'Ä te', 'Ä zorgen', 'Ä dat', 'Ä personen', 'Ä in', 'Ä een', 'Ä pall', 'i', 'atief', 'Ä zorgt', 'raject', 'Ä in', 'Ä de', 'Ä toekomst', 'Ä wel', 'Ä aanspraak', 'Ä kunnen', 'Ä maken', 'Ä op', 'Ä een', 'Ä tegemoetkoming', 'Ä van', 'Ä hulpmiddelen', '?', 'Ä Zo', 'Ä ja', ',', 'Ä welke', 'Ä en', 'Ä binnen', 'Ä welke', 'Ä termijn', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Een random lange zin om te zien of we met 128 tokens per zin goed zitten. Deze zin bevat 44 dus 128 zou ok moeten zijn \n",
    "'''\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\" )\n",
    "tokenizer3 = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "text = \"Zal de minister initiatieven nemen om ervoor te zorgen dat personen in een palliatief zorgtraject in de toekomst wel aanspraak kunnen maken op een tegemoetkoming van hulpmiddelen? Zo ja, welke en binnen welke termijn?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens2 = tokenizer2.tokenize(text)\n",
    "tokens3 = tokenizer3.tokenize(text)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens2))\n",
    "print(\"Tokens:\", tokens2)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens3))\n",
    "print(\"Tokens:\", tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bbf1b70-1be0-409c-8206-2b06b147b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3936c0c9-dc6d-4874-a141-c74042e86b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0100932-3e75-4f62-b457-4b989981e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 01:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.358749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.403158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=717, training_loss=0.3160602989888424, metrics={'train_runtime': 108.8975, 'train_samples_per_second': 52.563, 'train_steps_per_second': 6.584, 'total_flos': 376511920220160.0, 'train_loss': 0.3160602989888424, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee221e9-e0c9-455b-84c9-e8ac5f3c41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "unlabeled_df = pd.read_excel(\"unlabeled_questions.xlsx\")\n",
    "questions = unlabeled_df['question'].tolist()\n",
    "encodings = tokenize(questions)\n",
    "encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "# Run model on the data\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    predicted_probs, predicted_labels = torch.max(probs, dim=1)\n",
    "    uncertainty = 1 - predicted_probs  # lower confidence = more uncertain\n",
    "\n",
    "# Add results to DataFrame\n",
    "unlabeled_df['predicted_label'] = predicted_labels.cpu().numpy()\n",
    "unlabeled_df['confidence'] = predicted_probs.cpu().numpy()\n",
    "unlabeled_df['uncertainty'] = uncertainty.cpu().numpy()\n",
    "\n",
    "# Sort by uncertainty and get top 20%\n",
    "top_uncertain = unlabeled_df.sort_values(by='uncertainty', ascending=False).head(int(0.2 * len(unlabeled_df)))\n",
    "\n",
    "# Save to Excel for manual labeling\n",
    "top_uncertain.to_excel(\"to_label_eng.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c49be3-9cef-45ce-909e-75d5020f72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "\n",
    "# Clean and convert to string\n",
    "df_test = df_test.dropna(subset=['question', 'label'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Extract questions and labels\n",
    "test_texts = df_test['question'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4e01d0-04b9-4cfb-a389-887316ff420f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28848540782928467,\n",
       " 'eval_runtime': 1.9543,\n",
       " 'eval_samples_per_second': 255.333,\n",
       " 'eval_steps_per_second': 32.236,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Assuming you already have a `trainer` object that was used for training\n",
    "trainer.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1432bdca-0272-40cd-8875-bbbde4f37a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       362\n",
      "           1       0.85      0.89      0.87       137\n",
      "\n",
      "    accuracy                           0.93       499\n",
      "   macro avg       0.91      0.92      0.91       499\n",
      "weighted avg       0.93      0.93      0.93       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c515a7b-a509-4261-be4c-8ea8d30938b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Set 1 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 2 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.728     0.961     0.829       362\n",
      "           1      0.333     0.051     0.089       137\n",
      "\n",
      "    accuracy                          0.711       499\n",
      "   macro avg      0.531     0.506     0.459       499\n",
      "weighted avg      0.620     0.711     0.625       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 3 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.953     0.839       362\n",
      "           1      0.564     0.161     0.250       137\n",
      "\n",
      "    accuracy                          0.735       499\n",
      "   macro avg      0.657     0.557     0.545       499\n",
      "weighted avg      0.699     0.735     0.678       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 4 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.725     0.997     0.840       362\n",
      "           1      0.000     0.000     0.000       137\n",
      "\n",
      "    accuracy                          0.723       499\n",
      "   macro avg      0.362     0.499     0.420       499\n",
      "weighted avg      0.526     0.723     0.609       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 5 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.728     0.983     0.837       362\n",
      "           1      0.400     0.029     0.054       137\n",
      "\n",
      "    accuracy                          0.721       499\n",
      "   macro avg      0.564     0.506     0.446       499\n",
      "weighted avg      0.638     0.721     0.622       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 6 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.926     0.870     0.897       362\n",
      "           1      0.704     0.818     0.757       137\n",
      "\n",
      "    accuracy                          0.856       499\n",
      "   macro avg      0.815     0.844     0.827       499\n",
      "weighted avg      0.866     0.856     0.859       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 7 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.884     0.887     0.886       362\n",
      "           1      0.699     0.693     0.696       137\n",
      "\n",
      "    accuracy                          0.834       499\n",
      "   macro avg      0.791     0.790     0.791       499\n",
      "weighted avg      0.833     0.834     0.833       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 8 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.884     0.887     0.886       362\n",
      "           1      0.699     0.693     0.696       137\n",
      "\n",
      "    accuracy                          0.834       499\n",
      "   macro avg      0.791     0.790     0.791       499\n",
      "weighted avg      0.833     0.834     0.833       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 9 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Your regex sets\n",
    "statistical_sets = {\n",
    "    \"Set 1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    \"Set 2\": [\n",
    "        r\"\\b(hoe vaak|hoe groot|gemiddelde van|mediaan van|ratio van|procent van)\\b\",\n",
    "        r\"\\b(stijging van|daling van|verandering in|ontwikkeling in|schommeling van|impact op)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|analyse van)?\\s*(de|een)?\\s*(cijfers|gegevens|statistieken|tellingen)\\b\"\n",
    "    ],\n",
    "    \"Set 3\": [\n",
    "        r\"\\b(bruto|netto|inkomen|uitgaven|begroting|subsidies|belasting|tarief|BBP|inflatie|schulden|werkloosheid|bestedingen|consumptie)\\b\",\n",
    "        r\"\\b(bevolking|leeftijdsgroep|demografie|migratie|emigratie|immigratie|huishoudens|gezinnen|verhouding tussen|dichtheid)\\b\",\n",
    "        r\"\\b(aantal|hoeveelheid|grootte van|gemiddelde|mediaan|percentage|spreiding|percentiel|kwartiel|standaarddeviatie)\\b\"\n",
    "    ],\n",
    "    \"Set 4\": [\n",
    "        r\"\\b(zorgkosten|patiÃ«nten|ziekenhuisopnames|sterftecijfers|levensverwachting|gezondheidsuitgaven|vaccinaties|epidemieÃ«n|medicatiegebruik)\\b\",\n",
    "        r\"\\b(reistijd|filedruk|kilometers afgelegd|verkeersongevallen|CO2-uitstoot|luchtvervuiling|hernieuwbare energie|klimaatverandering|waterkwaliteit)\\b\",\n",
    "        r\"\\b(grondprijzen|woningmarkt|huurprijzen|hypotheken|verkoopcijfers|bouwvergunningen|energieverbruik)\\b\"\n",
    "    ],\n",
    "    \"Set 5\": [\n",
    "        r\"\\b(vergeleken met|ten opzichte van|in vergelijking met|in het verleden|sinds \\d{4}|tussen \\d{4} en \\d{4})\\b\",\n",
    "        r\"\\b(ontwikkeling sinds|historische gegevens|trendanalyse|jaarverslagen|statistische rapporten)\\b\"\n",
    "    ],\n",
    "    \"Set 6\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage|statistieken|cijfers|gegevens|data)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(per\\s+\\w+|per\\s+\\d+|in\\s+\\d{4}|tussen\\s+\\d{4}\\s+en\\s+\\d{4})\\b\",  \n",
    "        r\"\\b(wat is het aantal|hoe groot is|hoe vaak|gemiddelde van|ratio van)\\b\"\n",
    "    ],\n",
    "    \"Set 7\": [\n",
    "        r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "        r\"\\b(totale budget|welk budget|eenzelfde hoogte|gemiddeld aantal|geÃ«volueerd in de periode|jaarlijkse kostprijs |het aantal)\\b\",\n",
    "        r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    ],\n",
    "    \"Set 8\": [\n",
    "        r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "        r\"\\b(totale budget|welk budget|cijfer over|eenzelfde hoogte|gemiddeld aantal|geÃ«volueerd in de periode|jaarlijkse kostprijs|het aantal)\\b\",\n",
    "        r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    ],\n",
    "    \"Set 9\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules to each question\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    df_test[f\"{set_name}_match\"] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\nðŸ§ª {set_name} â€” Evaluation against true labels:\")\n",
    "        print(classification_report(df_test['label'], df_test[f\"{set_name}_match\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1d4fb5-856b-4218-8f73-b7f6b3f96fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Naive Bayes:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.787     0.961     0.866       362\n",
      "           1      0.754     0.314     0.443       137\n",
      "\n",
      "    accuracy                          0.784       499\n",
      "   macro avg      0.771     0.638     0.654       499\n",
      "weighted avg      0.778     0.784     0.750       499\n",
      "\n",
      "\n",
      "ðŸ§ª Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.933     0.923     0.928       362\n",
      "           1      0.801     0.825     0.813       137\n",
      "\n",
      "    accuracy                          0.896       499\n",
      "   macro avg      0.867     0.874     0.870       499\n",
      "weighted avg      0.897     0.896     0.896       499\n",
      "\n",
      "\n",
      "ðŸ§ª SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.893     0.964     0.927       362\n",
      "           1      0.880     0.693     0.776       137\n",
      "\n",
      "    accuracy                          0.890       499\n",
      "   macro avg      0.886     0.829     0.851       499\n",
      "weighted avg      0.889     0.890     0.885       499\n",
      "\n",
      "\n",
      "ðŸ§ª Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.856     0.972     0.911       362\n",
      "           1      0.886     0.569     0.693       137\n",
      "\n",
      "    accuracy                          0.862       499\n",
      "   macro avg      0.871     0.771     0.802       499\n",
      "weighted avg      0.865     0.862     0.851       499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª XGBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.945     0.919       362\n",
      "           1      0.829     0.708     0.764       137\n",
      "\n",
      "    accuracy                          0.880       499\n",
      "   macro avg      0.862     0.826     0.842       499\n",
      "weighted avg      0.877     0.880     0.877       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_train.columns = df_test.columns = ['question', 'label']\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(df_train['question'])\n",
    "y_train = df_train['label']\n",
    "X_test = vectorizer.transform(df_test['question'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Models with your parameters\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(alpha=0.01),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        C=10, class_weight='balanced', penalty='l2', solver='saga', max_iter=1000\n",
    "    ),\n",
    "    \"SVM\": SVC(C=1, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        class_weight='balanced', max_depth=None, n_estimators=250\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.3, n_estimators=250,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\nðŸ§ª {name}:\\n\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bba31-0e7b-4bef-be01-10b8f2d53f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
