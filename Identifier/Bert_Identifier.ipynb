{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38aa0383-3196-4786-8538-7a83ae3c1b91",
   "metadata": {},
   "source": [
    "**Een Identifier die gebruik maakt van active learning. 500 vragen zijn zelf gelabeled. Vervolgens fine tunen we een bert model hierop. We laten deze getunede bert dan zelf 1000 ongeziene vragen labelen. We kijken naar de 20% waar het model het minst zeker van is. Die kijken we handmatig na. De gecorrigeerde voegen we dan toe aan de training data en zo herhalen we dit proces tot het bert model naar behoren werkt.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcdcad8-89e7-4a00-a616-6beae8aea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457b33d1-be39-4fd0-8a00-ff8d4044e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load labeled data\n",
    "df = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df.columns = ['question', 'label']\n",
    "\n",
    "# Remove duplicate questions\n",
    "df = df.drop_duplicates(subset='question')\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['question'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df90c5da-3f2c-46f4-ab38-65d32c469b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 44\n",
      "Tokens: ['Zal', 'de', 'minister', 'initiatie', '##ven', 'nemen', 'om', 'ervoor', 'te', 'zorgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zorgt', '##ra', '##ject', 'in', 'de', 'toekomst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'tegemoetkoming', 'van', 'hulpmiddel', '##en', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'termijn', '?']\n",
      "Number of tokens: 55\n",
      "Tokens: ['Za', '##l', 'de', 'minister', 'init', '##iati', '##even', 'nemen', 'om', 'ervoor', 'te', 'zo', '##rgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zo', '##rgt', '##raj', '##ect', 'in', 'de', 'toe', '##komst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'te', '##gem', '##oe', '##tkom', '##ing', 'van', 'hulp', '##mid', '##delen', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'term', '##ijn', '?']\n",
      "Number of tokens: 40\n",
      "Tokens: ['Zal', 'Ġde', 'Ġminister', 'Ġinitiatieven', 'Ġnemen', 'Ġom', 'Ġervoor', 'Ġte', 'Ġzorgen', 'Ġdat', 'Ġpersonen', 'Ġin', 'Ġeen', 'Ġpall', 'i', 'atief', 'Ġzorgt', 'raject', 'Ġin', 'Ġde', 'Ġtoekomst', 'Ġwel', 'Ġaanspraak', 'Ġkunnen', 'Ġmaken', 'Ġop', 'Ġeen', 'Ġtegemoetkoming', 'Ġvan', 'Ġhulpmiddelen', '?', 'ĠZo', 'Ġja', ',', 'Ġwelke', 'Ġen', 'Ġbinnen', 'Ġwelke', 'Ġtermijn', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Een random lange zin om te zien of we met 128 tokens per zin goed zitten. Deze zin bevat 44 dus 128 zou ok moeten zijn \n",
    "'''\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\" )\n",
    "tokenizer3 = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "text = \"Zal de minister initiatieven nemen om ervoor te zorgen dat personen in een palliatief zorgtraject in de toekomst wel aanspraak kunnen maken op een tegemoetkoming van hulpmiddelen? Zo ja, welke en binnen welke termijn?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens2 = tokenizer2.tokenize(text)\n",
    "tokens3 = tokenizer3.tokenize(text)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens2))\n",
    "print(\"Tokens:\", tokens2)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens3))\n",
    "print(\"Tokens:\", tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf1b70-1be0-409c-8206-2b06b147b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc0d9fb-64fe-4e5f-9051-3066afe4c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# def remove_hoeveel(text):\n",
    "#     return ' '.join([w for w in text.split() if w.lower() != 'hoeveel'])\n",
    "\n",
    "# train_texts = [remove_hoeveel(t) for t in train_texts]\n",
    "# val_texts = [remove_hoeveel(t) for t in val_texts]\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "# def tokenize(texts):\n",
    "#     return tokenizer(\n",
    "#         texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "#     )\n",
    "\n",
    "# train_encodings = tokenize(train_texts)\n",
    "# val_encodings = tokenize(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3936c0c9-dc6d-4874-a141-c74042e86b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0100932-3e75-4f62-b457-4b989981e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1584' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1584/1584 04:40, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.285572</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.884785</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.885546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.327880</td>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.876406</td>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.873104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.464820</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.551833</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.892582</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.891795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.613993</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.893648</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.892214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.625638</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.899614</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.899145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.678512</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.892582</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>0.891795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.700469</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.895597</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.894518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1584, training_loss=0.14717642225400365, metrics={'train_runtime': 280.7652, 'train_samples_per_second': 44.963, 'train_steps_per_second': 5.642, 'total_flos': 830378490716160.0, 'train_loss': 0.14717642225400365, 'epoch': 8.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eef09e7-7b99-4f1b-9145-d9addc00ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"hoeveel\" in ' '.join(train_texts).lower())  # Should print False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee221e9-e0c9-455b-84c9-e8ac5f3c41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import pandas as pd\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Load data\n",
    "# unlabeled_df = pd.read_excel(\"unlabeled_questions.xlsx\")\n",
    "# questions = unlabeled_df['question'].tolist()\n",
    "# encodings = tokenize(questions)\n",
    "# encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "# # Run model on the data\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**encodings)\n",
    "#     probs = F.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "#     predicted_probs, predicted_labels = torch.max(probs, dim=1)\n",
    "#     uncertainty = 1 - predicted_probs  # lower confidence = more uncertain\n",
    "\n",
    "# # Add results to DataFrame\n",
    "# unlabeled_df['predicted_label'] = predicted_labels.cpu().numpy()\n",
    "# unlabeled_df['confidence'] = predicted_probs.cpu().numpy()\n",
    "# unlabeled_df['uncertainty'] = uncertainty.cpu().numpy()\n",
    "\n",
    "# # Sort by uncertainty and get top 20%\n",
    "# top_uncertain = unlabeled_df.sort_values(by='uncertainty', ascending=False).head(int(0.2 * len(unlabeled_df)))\n",
    "\n",
    "# # Save to Excel for manual labeling\n",
    "# top_uncertain.to_excel(\"to_label_eng.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c49be3-9cef-45ce-909e-75d5020f72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "\n",
    "# Clean and convert to string\n",
    "df_test = df_test.dropna(subset=['question', 'label'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Extract questions and labels\n",
    "test_texts = df_test['question'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4e01d0-04b9-4cfb-a389-887316ff420f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Run predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Get raw logits\n",
    "logits = predictions_output.predictions\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "# Load test questions (in same order)\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test = df_test.dropna(subset=['question']).reset_index(drop=True)\n",
    "df_test.columns = ['question', 'true_label']\n",
    "\n",
    "# Create final DataFrame with question + true + predicted\n",
    "df = pd.DataFrame({\n",
    "    \"question\": df_test[\"question\"],\n",
    "    \"true_label\": df_test[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"bert_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1432bdca-0272-40cd-8875-bbbde4f37a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       362\n",
      "           1       0.83      0.83      0.83       137\n",
      "\n",
      "    accuracy                           0.91       499\n",
      "   macro avg       0.88      0.88      0.88       499\n",
      "weighted avg       0.91      0.91      0.91       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c515a7b-a509-4261-be4c-8ea8d30938b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Set 1 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "🧪 Set 2 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Your regex sets\n",
    "statistical_sets = {\n",
    "    \"Set 1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    # \"Set 1.1\": [\n",
    "    #     r\"\\b(aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "    #     r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    # ],\n",
    "    # \"Set 2\": [\n",
    "    #     r\"\\b(hoe vaak|hoe groot|gemiddelde van|mediaan van|ratio van|procent van)\\b\",\n",
    "    #     r\"\\b(stijging van|daling van|verandering in|ontwikkeling in|schommeling van|impact op)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|analyse van)?\\s*(de|een)?\\s*(cijfers|gegevens|statistieken|tellingen)\\b\"\n",
    "    # ],\n",
    "    # \"Set 3\": [\n",
    "    #     r\"\\b(bruto|netto|inkomen|uitgaven|begroting|subsidies|belasting|tarief|BBP|inflatie|schulden|werkloosheid|bestedingen|consumptie)\\b\",\n",
    "    #     r\"\\b(bevolking|leeftijdsgroep|demografie|migratie|emigratie|immigratie|huishoudens|gezinnen|verhouding tussen|dichtheid)\\b\",\n",
    "    #     r\"\\b(aantal|hoeveelheid|grootte van|gemiddelde|mediaan|percentage|spreiding|percentiel|kwartiel|standaarddeviatie)\\b\"\n",
    "    # ],\n",
    "    # \"Set 4\": [\n",
    "    #     r\"\\b(zorgkosten|patiënten|ziekenhuisopnames|sterftecijfers|levensverwachting|gezondheidsuitgaven|vaccinaties|epidemieën|medicatiegebruik)\\b\",\n",
    "    #     r\"\\b(reistijd|filedruk|kilometers afgelegd|verkeersongevallen|CO2-uitstoot|luchtvervuiling|hernieuwbare energie|klimaatverandering|waterkwaliteit)\\b\",\n",
    "    #     r\"\\b(grondprijzen|woningmarkt|huurprijzen|hypotheken|verkoopcijfers|bouwvergunningen|energieverbruik)\\b\"\n",
    "    # ],\n",
    "    # \"Set 5\": [\n",
    "    #     r\"\\b(vergeleken met|ten opzichte van|in vergelijking met|in het verleden|sinds \\d{4}|tussen \\d{4} en \\d{4})\\b\",\n",
    "    #     r\"\\b(ontwikkeling sinds|historische gegevens|trendanalyse|jaarverslagen|statistische rapporten)\\b\"\n",
    "    # ],\n",
    "    # \"Set 6\": [\n",
    "    #     r\"\\b(hoeveel|aantal|percentage|statistieken|cijfers|gegevens|data)\\b\",\n",
    "    #     r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "    #     r\"\\b(per\\s+\\w+|per\\s+\\d+|in\\s+\\d{4}|tussen\\s+\\d{4}\\s+en\\s+\\d{4})\\b\",  \n",
    "    #     r\"\\b(wat is het aantal|hoe groot is|hoe vaak|gemiddelde van|ratio van)\\b\"\n",
    "    # ],\n",
    "    # \"Set 7\": [\n",
    "    #     r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "    #     r\"\\b(totale budget|welk budget|eenzelfde hoogte|gemiddeld aantal|geëvolueerd in de periode|jaarlijkse kostprijs |het aantal)\\b\",\n",
    "    #     r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    # ],\n",
    "    # \"Set 8\": [\n",
    "    #     r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "    #     r\"\\b(totale budget|welk budget|cijfer over|eenzelfde hoogte|gemiddeld aantal|geëvolueerd in de periode|jaarlijkse kostprijs|het aantal)\\b\",\n",
    "    #     r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    # ],\n",
    "    \"Set 2\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules to each question\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    df_test[f\"{set_name}_match\"] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\n🧪 {set_name} — Evaluation against true labels:\")\n",
    "        print(classification_report(df_test['label'], df_test[f\"{set_name}_match\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e74c64ae-cf2c-4636-955e-28a123de12fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Set_1 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "🧪 Set_2 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Define regex sets\n",
    "statistical_sets = {\n",
    "    \"Set_1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    \"Set_2\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Save classification reports\n",
    "metrics_list = []\n",
    "\n",
    "# Apply and evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    col_name = f\"{set_name}_match\"\n",
    "    df_test[col_name] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\n🧪 {set_name} — Evaluation against true labels:\")\n",
    "        report = classification_report(df_test['label'], df_test[col_name], digits=3, output_dict=True)\n",
    "        print(classification_report(df_test['label'], df_test[col_name], digits=3))\n",
    "        \n",
    "        # Save key metrics\n",
    "        prf = precision_recall_fscore_support(df_test['label'], df_test[col_name], average='binary')\n",
    "        metrics_list.append({\n",
    "            'Regex_Set': set_name,\n",
    "            'Precision': prf[0],\n",
    "            'Recall': prf[1],\n",
    "            'F1-score': prf[2],\n",
    "            'Support': prf[3]\n",
    "        })\n",
    "\n",
    "# Save predictions and metrics\n",
    "df_test.to_csv(\"regex_predictions.csv\", index=False)\n",
    "pd.DataFrame(metrics_list).to_csv(\"regex_evaluation_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b1d4fb5-856b-4218-8f73-b7f6b3f96fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Naive_Bayes:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.795     0.934     0.859       362\n",
      "           1      0.676     0.365     0.474       137\n",
      "\n",
      "    accuracy                          0.778       499\n",
      "   macro avg      0.735     0.649     0.666       499\n",
      "weighted avg      0.762     0.778     0.753       499\n",
      "\n",
      "\n",
      "🧪 Logistic_Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.931     0.936     0.934       362\n",
      "           1      0.830     0.818     0.824       137\n",
      "\n",
      "    accuracy                          0.904       499\n",
      "   macro avg      0.880     0.877     0.879       499\n",
      "weighted avg      0.903     0.904     0.904       499\n",
      "\n",
      "\n",
      "🧪 SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.911     0.964     0.937       362\n",
      "           1      0.888     0.752     0.814       137\n",
      "\n",
      "    accuracy                          0.906       499\n",
      "   macro avg      0.900     0.858     0.876       499\n",
      "weighted avg      0.905     0.906     0.903       499\n",
      "\n",
      "\n",
      "🧪 Random_Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.909     0.967     0.937       362\n",
      "           1      0.895     0.745     0.813       137\n",
      "\n",
      "    accuracy                          0.906       499\n",
      "   macro avg      0.902     0.856     0.875       499\n",
      "weighted avg      0.905     0.906     0.903       499\n",
      "\n",
      "\n",
      "🧪 XGBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.911     0.956     0.933       362\n",
      "           1      0.866     0.752     0.805       137\n",
      "\n",
      "    accuracy                          0.900       499\n",
      "   macro avg      0.888     0.854     0.869       499\n",
      "weighted avg      0.898     0.900     0.897       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure stopwords are downloaded\n",
    "# import nltk; nltk.download('stopwords')\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_train.columns = df_test.columns = ['question', 'label']\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=dutch_stopwords)\n",
    "X_train = vectorizer.fit_transform(df_train['question'])\n",
    "y_train = df_train['label']\n",
    "X_test = vectorizer.transform(df_test['question'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Store original test set\n",
    "results_df = df_test.copy()\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Naive_Bayes\": MultinomialNB(alpha=0.01),\n",
    "    \"Logistic_Regression\": LogisticRegression(\n",
    "        C=10, class_weight='balanced', penalty='l2', solver='saga', max_iter=1000\n",
    "    ),\n",
    "    \"SVM\": SVC(C=1, class_weight='balanced'),\n",
    "    \"Random_Forest\": RandomForestClassifier(\n",
    "        class_weight='balanced', max_depth=None, n_estimators=250\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.3, n_estimators=250,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train, evaluate, and store predictions\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    results_df[name + '_pred'] = preds\n",
    "    print(f\"\\n🧪 {name}:\\n\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n",
    "\n",
    "# Save predictions to CSV\n",
    "results_df.to_csv(\"all_model_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0a99d9-b2d3-4080-9038-a3af79d78e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "bert_df = pd.read_csv(\"bert_predictions.csv\")\n",
    "models_df = pd.read_csv(\"all_model_predictions.csv\")\n",
    "regex_df = pd.read_csv(\"regex_predictions.csv\")\n",
    "\n",
    "# Add a temporary row index to preserve order\n",
    "bert_df[\"row_index\"] = range(len(bert_df))\n",
    "models_df[\"row_index\"] = range(len(models_df))\n",
    "regex_df[\"row_index\"] = range(len(regex_df))\n",
    "\n",
    "# Standardize column names\n",
    "bert_df = bert_df.rename(columns={\"true_label\": \"label\"})\n",
    "models_df = models_df.rename(columns={\"label\": \"label\"})\n",
    "regex_df = regex_df.rename(columns={\"label\": \"label\"})\n",
    "\n",
    "# Merge on row_index (not question/label) to preserve order\n",
    "combined = pd.merge(bert_df, models_df.drop(columns=[\"question\", \"label\"]), on=\"row_index\")\n",
    "combined = pd.merge(combined, regex_df.drop(columns=[\"question\", \"label\"]), on=\"row_index\")\n",
    "\n",
    "# Drop row_index and reorder columns\n",
    "combined = combined.drop(columns=[\"row_index\"])\n",
    "cols = ['question', 'label'] + [col for col in combined.columns if col not in ['question', 'label']]\n",
    "combined = combined[cols]\n",
    "\n",
    "# Save to file\n",
    "combined.to_csv(\"combined_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831104bc-5906-4d95-ae26-d4ab9854e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic = 116.042, p-value = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "models = [col for col in df.columns if col.endswith('_pred') or col.endswith('_match')]\n",
    "\n",
    "# Binary correct predictions\n",
    "correct_matrix = np.array([(df[model] == df['label']).astype(int) for model in models]).T\n",
    "\n",
    "# Friedman test\n",
    "stat, p = friedmanchisquare(*correct_matrix.T)\n",
    "print(f\"Friedman test statistic = {stat:.3f}, p-value = {p:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e2a21bf-9115-46ba-9c11-b913609a51b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 McNemar's test p-values (lower = significant difference):\n",
      "                          predicted_label  Naive_Bayes_pred  \\\n",
      "predicted_label                    1.0000               0.0   \n",
      "Naive_Bayes_pred                   0.0000               1.0   \n",
      "Logistic_Regression_pred           0.8746               0.0   \n",
      "SVM_pred                           1.0000               0.0   \n",
      "Random_Forest_pred                 1.0000               0.0   \n",
      "XGBoost_pred                       0.6718               0.0   \n",
      "Set_1_match                        0.0440               0.0   \n",
      "Set_2_match                        0.0627               0.0   \n",
      "\n",
      "                          Logistic_Regression_pred  SVM_pred  \\\n",
      "predicted_label                             0.8746    1.0000   \n",
      "Naive_Bayes_pred                            0.0000    0.0000   \n",
      "Logistic_Regression_pred                    1.0000    1.0000   \n",
      "SVM_pred                                    1.0000    1.0000   \n",
      "Random_Forest_pred                          1.0000    1.0000   \n",
      "XGBoost_pred                                0.8642    0.7201   \n",
      "Set_1_match                                 0.0436    0.0357   \n",
      "Set_2_match                                 0.1112    0.0436   \n",
      "\n",
      "                          Random_Forest_pred  XGBoost_pred  Set_1_match  \\\n",
      "predicted_label                       1.0000        0.6718       0.0440   \n",
      "Naive_Bayes_pred                      0.0000        0.0000       0.0000   \n",
      "Logistic_Regression_pred              1.0000        0.8642       0.0436   \n",
      "SVM_pred                              1.0000        0.7201       0.0357   \n",
      "Random_Forest_pred                    1.0000        0.7111       0.0444   \n",
      "XGBoost_pred                          0.7111        1.0000       0.1337   \n",
      "Set_1_match                           0.0444        0.1337       1.0000   \n",
      "Set_2_match                           0.0385        0.1524       1.0000   \n",
      "\n",
      "                          Set_2_match  \n",
      "predicted_label                0.0627  \n",
      "Naive_Bayes_pred               0.0000  \n",
      "Logistic_Regression_pred       0.1112  \n",
      "SVM_pred                       0.0436  \n",
      "Random_Forest_pred             0.0385  \n",
      "XGBoost_pred                   0.1524  \n",
      "Set_1_match                    1.0000  \n",
      "Set_2_match                    1.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from itertools import combinations\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "models = [col for col in df.columns if col.endswith('_label') or col.endswith('_pred') or col.endswith('_match')]\n",
    "\n",
    "# Ground truth\n",
    "y_true = df['label'].values\n",
    "\n",
    "# Prepare p-value result matrix\n",
    "p_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
    "\n",
    "# McNemar test function\n",
    "def mcnemar_p(y_true, pred1, pred2):\n",
    "    table = [[0, 0], [0, 0]]\n",
    "    for y, a, b in zip(y_true, pred1, pred2):\n",
    "        correct_a = int(a == y)\n",
    "        correct_b = int(b == y)\n",
    "        table[correct_b][correct_a] += 1\n",
    "    result = mcnemar(table, exact=True)\n",
    "    return result.pvalue\n",
    "\n",
    "# Compare all model pairs\n",
    "for model1, model2 in combinations(models, 2):\n",
    "    p = mcnemar_p(y_true, df[model1], df[model2])\n",
    "    p_matrix.loc[model1, model2] = p\n",
    "    p_matrix.loc[model2, model1] = p  # symmetric\n",
    "\n",
    "# Fill diagonal with 1s\n",
    "np.fill_diagonal(p_matrix.values, 1.0)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 McNemar's test p-values (lower = significant difference):\")\n",
    "print(p_matrix.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cfe5b69-87bb-4961-aebd-02090c5229ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_contingency(y_true, pred1, pred2, name1, name2):\n",
    "    table = [[0, 0], [0, 0]]\n",
    "    for y, a, b in zip(y_true, pred1, pred2):\n",
    "        correct_a = int(a == y)\n",
    "        correct_b = int(b == y)\n",
    "        table[correct_b][correct_a] += 1\n",
    "    print(f\"\\n{name1} vs {name2} contingency table (McNemar):\")\n",
    "    print(pd.DataFrame(table, columns=[f\"{name2} correct\", f\"{name2} wrong\"], \n",
    "                             index=[f\"{name1} correct\", f\"{name1} wrong\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b128eba0-b1c9-420f-b012-cd1b572721e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT vs LogReg contingency table (McNemar):\n",
      "              LogReg correct  LogReg wrong\n",
      "BERT correct              27            21\n",
      "BERT wrong                19           432\n"
     ]
    }
   ],
   "source": [
    "print_contingency(df['label'], df['predicted_label'], df['Logistic_Regression_pred'], 'BERT', 'LogReg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecdb8b09-8df0-4940-9e4e-203fc8f9a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT predicted correctly on 453 out of 499 samples.\n",
      "Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "# Count correct predictions by BERT\n",
    "bert_correct = (df['predicted_label'] == df['label']).sum()\n",
    "total = len(df)\n",
    "\n",
    "print(f\"BERT predicted correctly on {bert_correct} out of {total} samples.\")\n",
    "print(f\"Accuracy: {bert_correct / total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1877dc26-1b0d-4301-aefa-a0ae371fdfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT correct: 453 / 499\n",
      "LogReg correct: 451 / 499\n",
      "\n",
      "✅ Contingency Table:\n",
      "              LogReg wrong  LogReg correct\n",
      "BERT wrong              27              19\n",
      "BERT correct            21             432\n",
      "\n",
      "McNemar's test p-value: 0.8746293123804207\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Load your combined predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "# Extract relevant predictions\n",
    "y_true = df['label']\n",
    "bert = df['predicted_label']\n",
    "logreg = df['Logistic_Regression_pred']\n",
    "\n",
    "# Count how many predictions each model got correct\n",
    "bert_correct = (bert == y_true)\n",
    "logreg_correct = (logreg == y_true)\n",
    "\n",
    "print(f\"BERT correct: {bert_correct.sum()} / {len(df)}\")\n",
    "print(f\"LogReg correct: {logreg_correct.sum()} / {len(df)}\")\n",
    "\n",
    "# Build McNemar contingency table from scratch\n",
    "# Rows: BERT (correct vs wrong)\n",
    "# Cols: LogReg (correct vs wrong)\n",
    "table = pd.crosstab(bert_correct, logreg_correct)\n",
    "table.index = ['BERT wrong', 'BERT correct']\n",
    "table.columns = ['LogReg wrong', 'LogReg correct']\n",
    "\n",
    "print(\"\\n✅ Contingency Table:\")\n",
    "print(table)\n",
    "\n",
    "# Optional: McNemar p-value\n",
    "print(\"\\nMcNemar's test p-value:\", mcnemar(table.values, exact=True).pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3332aa6a-d32d-41bb-a8af-453fd045f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 BERT vs Logistic Regression — F1 difference:\n",
      "Mean diff: 0.0090\n",
      "95% CI:    [-0.0365, 0.0569]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_f1_diff(y_true, y_pred1, y_pred2, n=1000):\n",
    "    diffs = []\n",
    "    for _ in range(n):\n",
    "        idx = np.random.choice(len(y_true), len(y_true), replace=True)\n",
    "        f1_1 = f1_score(y_true[idx], y_pred1[idx])\n",
    "        f1_2 = f1_score(y_true[idx], y_pred2[idx])\n",
    "        diffs.append(f1_1 - f1_2)\n",
    "    return np.mean(diffs), np.percentile(diffs, [2.5, 97.5])\n",
    "\n",
    "# Example use\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "y_true = df['label'].values\n",
    "bert = df['predicted_label'].values\n",
    "logreg = df['Logistic_Regression_pred'].values\n",
    "\n",
    "mean_diff, ci = bootstrap_f1_diff(y_true, bert, logreg)\n",
    "\n",
    "print(f\"🔍 BERT vs Logistic Regression — F1 difference:\")\n",
    "print(f\"Mean diff: {mean_diff:.4f}\")\n",
    "print(f\"95% CI:    [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283eefea-c588-4237-a5b9-9227cdfbb374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
