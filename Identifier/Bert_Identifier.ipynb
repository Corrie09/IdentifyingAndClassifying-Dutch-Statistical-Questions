{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38aa0383-3196-4786-8538-7a83ae3c1b91",
   "metadata": {},
   "source": [
    "**Een Identifier die gebruik maakt van active learning. 500 vragen zijn zelf gelabeled. Vervolgens fine tunen we een bert model hierop. We laten deze getunede bert dan zelf 1000 ongeziene vragen labelen. We kijken naar de 20% waar het model het minst zeker van is. Die kijken we handmatig na. De gecorrigeerde voegen we dan toe aan de training data en zo herhalen we dit proces tot het bert model naar behoren werkt.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcdcad8-89e7-4a00-a616-6beae8aea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457b33d1-be39-4fd0-8a00-ff8d4044e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load labeled data\n",
    "df = pd.read_excel(\"Trainig_data.xlsx\")  \n",
    "df.columns = ['question', 'label']  \n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['question'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df90c5da-3f2c-46f4-ab38-65d32c469b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 44\n",
      "Tokens: ['Zal', 'de', 'minister', 'initiatie', '##ven', 'nemen', 'om', 'ervoor', 'te', 'zorgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zorgt', '##ra', '##ject', 'in', 'de', 'toekomst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'tegemoetkoming', 'van', 'hulpmiddel', '##en', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'termijn', '?']\n",
      "Number of tokens: 55\n",
      "Tokens: ['Za', '##l', 'de', 'minister', 'init', '##iati', '##even', 'nemen', 'om', 'ervoor', 'te', 'zo', '##rgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zo', '##rgt', '##raj', '##ect', 'in', 'de', 'toe', '##komst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'te', '##gem', '##oe', '##tkom', '##ing', 'van', 'hulp', '##mid', '##delen', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'term', '##ijn', '?']\n",
      "Number of tokens: 40\n",
      "Tokens: ['Zal', 'Ä de', 'Ä minister', 'Ä initiatieven', 'Ä nemen', 'Ä om', 'Ä ervoor', 'Ä te', 'Ä zorgen', 'Ä dat', 'Ä personen', 'Ä in', 'Ä een', 'Ä pall', 'i', 'atief', 'Ä zorgt', 'raject', 'Ä in', 'Ä de', 'Ä toekomst', 'Ä wel', 'Ä aanspraak', 'Ä kunnen', 'Ä maken', 'Ä op', 'Ä een', 'Ä tegemoetkoming', 'Ä van', 'Ä hulpmiddelen', '?', 'Ä Zo', 'Ä ja', ',', 'Ä welke', 'Ä en', 'Ä binnen', 'Ä welke', 'Ä termijn', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Een random lange zin om te zien of we met 128 tokens per zin goed zitten. Deze zin bevat 44 dus 128 zou ok moeten zijn \n",
    "'''\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\" )\n",
    "tokenizer3 = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "text = \"Zal de minister initiatieven nemen om ervoor te zorgen dat personen in een palliatief zorgtraject in de toekomst wel aanspraak kunnen maken op een tegemoetkoming van hulpmiddelen? Zo ja, welke en binnen welke termijn?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens2 = tokenizer2.tokenize(text)\n",
    "tokens3 = tokenizer3.tokenize(text)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens2))\n",
    "print(\"Tokens:\", tokens2)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens3))\n",
    "print(\"Tokens:\", tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf1b70-1be0-409c-8206-2b06b147b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc0d9fb-64fe-4e5f-9051-3066afe4c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "def remove_hoeveel(text):\n",
    "    return ' '.join([w for w in text.split() if w.lower() != 'hoeveel'])\n",
    "\n",
    "train_texts = [remove_hoeveel(t) for t in train_texts]\n",
    "val_texts = [remove_hoeveel(t) for t in val_texts]\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3936c0c9-dc6d-4874-a141-c74042e86b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0100932-3e75-4f62-b457-4b989981e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 01:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.445570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.488587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.633018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=717, training_loss=0.30861406845028927, metrics={'train_runtime': 105.2333, 'train_samples_per_second': 54.393, 'train_steps_per_second': 6.813, 'total_flos': 376511920220160.0, 'train_loss': 0.30861406845028927, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eef09e7-7b99-4f1b-9145-d9addc00ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"hoeveel\" in ' '.join(train_texts).lower())  # Should print False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee221e9-e0c9-455b-84c9-e8ac5f3c41f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'unlabeled_questions.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m unlabeled_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munlabeled_questions.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m questions \u001b[38;5;241m=\u001b[39m unlabeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     12\u001b[0m encodings \u001b[38;5;241m=\u001b[39m tokenize(questions)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unlabeled_questions.xlsx'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "unlabeled_df = pd.read_excel(\"unlabeled_questions.xlsx\")\n",
    "questions = unlabeled_df['question'].tolist()\n",
    "encodings = tokenize(questions)\n",
    "encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "# Run model on the data\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    predicted_probs, predicted_labels = torch.max(probs, dim=1)\n",
    "    uncertainty = 1 - predicted_probs  # lower confidence = more uncertain\n",
    "\n",
    "# Add results to DataFrame\n",
    "unlabeled_df['predicted_label'] = predicted_labels.cpu().numpy()\n",
    "unlabeled_df['confidence'] = predicted_probs.cpu().numpy()\n",
    "unlabeled_df['uncertainty'] = uncertainty.cpu().numpy()\n",
    "\n",
    "# Sort by uncertainty and get top 20%\n",
    "top_uncertain = unlabeled_df.sort_values(by='uncertainty', ascending=False).head(int(0.2 * len(unlabeled_df)))\n",
    "\n",
    "# Save to Excel for manual labeling\n",
    "top_uncertain.to_excel(\"to_label_eng.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8c49be3-9cef-45ce-909e-75d5020f72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "\n",
    "# Clean and convert to string\n",
    "df_test = df_test.dropna(subset=['question', 'label'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Extract questions and labels\n",
    "test_texts = df_test['question'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc4e01d0-04b9-4cfb-a389-887316ff420f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6006466746330261,\n",
       " 'eval_runtime': 2.1834,\n",
       " 'eval_samples_per_second': 228.54,\n",
       " 'eval_steps_per_second': 28.854,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Assuming you already have a `trainer` object that was used for training\n",
    "trainer.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1432bdca-0272-40cd-8875-bbbde4f37a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       362\n",
      "           1       0.86      0.50      0.64       137\n",
      "\n",
      "    accuracy                           0.84       499\n",
      "   macro avg       0.85      0.74      0.77       499\n",
      "weighted avg       0.84      0.84      0.83       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c515a7b-a509-4261-be4c-8ea8d30938b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Set 1 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 1.1 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.735     0.936     0.824       362\n",
      "           1      0.395     0.109     0.171       137\n",
      "\n",
      "    accuracy                          0.709       499\n",
      "   macro avg      0.565     0.523     0.498       499\n",
      "weighted avg      0.642     0.709     0.645       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 2 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.728     0.961     0.829       362\n",
      "           1      0.333     0.051     0.089       137\n",
      "\n",
      "    accuracy                          0.711       499\n",
      "   macro avg      0.531     0.506     0.459       499\n",
      "weighted avg      0.620     0.711     0.625       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 3 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.953     0.839       362\n",
      "           1      0.564     0.161     0.250       137\n",
      "\n",
      "    accuracy                          0.735       499\n",
      "   macro avg      0.657     0.557     0.545       499\n",
      "weighted avg      0.699     0.735     0.678       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 4 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.725     0.997     0.840       362\n",
      "           1      0.000     0.000     0.000       137\n",
      "\n",
      "    accuracy                          0.723       499\n",
      "   macro avg      0.362     0.499     0.420       499\n",
      "weighted avg      0.526     0.723     0.609       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 5 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.728     0.983     0.837       362\n",
      "           1      0.400     0.029     0.054       137\n",
      "\n",
      "    accuracy                          0.721       499\n",
      "   macro avg      0.564     0.506     0.446       499\n",
      "weighted avg      0.638     0.721     0.622       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 6 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.926     0.870     0.897       362\n",
      "           1      0.704     0.818     0.757       137\n",
      "\n",
      "    accuracy                          0.856       499\n",
      "   macro avg      0.815     0.844     0.827       499\n",
      "weighted avg      0.866     0.856     0.859       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 7 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.884     0.887     0.886       362\n",
      "           1      0.699     0.693     0.696       137\n",
      "\n",
      "    accuracy                          0.834       499\n",
      "   macro avg      0.791     0.790     0.791       499\n",
      "weighted avg      0.833     0.834     0.833       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 8 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.884     0.887     0.886       362\n",
      "           1      0.699     0.693     0.696       137\n",
      "\n",
      "    accuracy                          0.834       499\n",
      "   macro avg      0.791     0.790     0.791       499\n",
      "weighted avg      0.833     0.834     0.833       499\n",
      "\n",
      "\n",
      "ðŸ§ª Set 9 â€” Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Your regex sets\n",
    "statistical_sets = {\n",
    "    \"Set 1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    \"Set 1.1\": [\n",
    "        r\"\\b(aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    \"Set 2\": [\n",
    "        r\"\\b(hoe vaak|hoe groot|gemiddelde van|mediaan van|ratio van|procent van)\\b\",\n",
    "        r\"\\b(stijging van|daling van|verandering in|ontwikkeling in|schommeling van|impact op)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|analyse van)?\\s*(de|een)?\\s*(cijfers|gegevens|statistieken|tellingen)\\b\"\n",
    "    ],\n",
    "    \"Set 3\": [\n",
    "        r\"\\b(bruto|netto|inkomen|uitgaven|begroting|subsidies|belasting|tarief|BBP|inflatie|schulden|werkloosheid|bestedingen|consumptie)\\b\",\n",
    "        r\"\\b(bevolking|leeftijdsgroep|demografie|migratie|emigratie|immigratie|huishoudens|gezinnen|verhouding tussen|dichtheid)\\b\",\n",
    "        r\"\\b(aantal|hoeveelheid|grootte van|gemiddelde|mediaan|percentage|spreiding|percentiel|kwartiel|standaarddeviatie)\\b\"\n",
    "    ],\n",
    "    \"Set 4\": [\n",
    "        r\"\\b(zorgkosten|patiÃ«nten|ziekenhuisopnames|sterftecijfers|levensverwachting|gezondheidsuitgaven|vaccinaties|epidemieÃ«n|medicatiegebruik)\\b\",\n",
    "        r\"\\b(reistijd|filedruk|kilometers afgelegd|verkeersongevallen|CO2-uitstoot|luchtvervuiling|hernieuwbare energie|klimaatverandering|waterkwaliteit)\\b\",\n",
    "        r\"\\b(grondprijzen|woningmarkt|huurprijzen|hypotheken|verkoopcijfers|bouwvergunningen|energieverbruik)\\b\"\n",
    "    ],\n",
    "    \"Set 5\": [\n",
    "        r\"\\b(vergeleken met|ten opzichte van|in vergelijking met|in het verleden|sinds \\d{4}|tussen \\d{4} en \\d{4})\\b\",\n",
    "        r\"\\b(ontwikkeling sinds|historische gegevens|trendanalyse|jaarverslagen|statistische rapporten)\\b\"\n",
    "    ],\n",
    "    \"Set 6\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage|statistieken|cijfers|gegevens|data)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(per\\s+\\w+|per\\s+\\d+|in\\s+\\d{4}|tussen\\s+\\d{4}\\s+en\\s+\\d{4})\\b\",  \n",
    "        r\"\\b(wat is het aantal|hoe groot is|hoe vaak|gemiddelde van|ratio van)\\b\"\n",
    "    ],\n",
    "    \"Set 7\": [\n",
    "        r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "        r\"\\b(totale budget|welk budget|eenzelfde hoogte|gemiddeld aantal|geÃ«volueerd in de periode|jaarlijkse kostprijs |het aantal)\\b\",\n",
    "        r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    ],\n",
    "    \"Set 8\": [\n",
    "        r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "        r\"\\b(totale budget|welk budget|cijfer over|eenzelfde hoogte|gemiddeld aantal|geÃ«volueerd in de periode|jaarlijkse kostprijs|het aantal)\\b\",\n",
    "        r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    ],\n",
    "    \"Set 2\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules to each question\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    df_test[f\"{set_name}_match\"] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\nðŸ§ª {set_name} â€” Evaluation against true labels:\")\n",
    "        print(classification_report(df_test['label'], df_test[f\"{set_name}_match\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b1d4fb5-856b-4218-8f73-b7f6b3f96fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Naive Bayes:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.786     0.956     0.863       362\n",
      "           1      0.729     0.314     0.439       137\n",
      "\n",
      "    accuracy                          0.780       499\n",
      "   macro avg      0.758     0.635     0.651       499\n",
      "weighted avg      0.771     0.780     0.746       499\n",
      "\n",
      "\n",
      "ðŸ§ª Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.936     0.933       362\n",
      "           1      0.828     0.810     0.819       137\n",
      "\n",
      "    accuracy                          0.902       499\n",
      "   macro avg      0.879     0.873     0.876       499\n",
      "weighted avg      0.901     0.902     0.901       499\n",
      "\n",
      "\n",
      "ðŸ§ª SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.899     0.959     0.928       362\n",
      "           1      0.867     0.715     0.784       137\n",
      "\n",
      "    accuracy                          0.892       499\n",
      "   macro avg      0.883     0.837     0.856       499\n",
      "weighted avg      0.890     0.892     0.888       499\n",
      "\n",
      "\n",
      "ðŸ§ª Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.970     0.931       362\n",
      "           1      0.897     0.701     0.787       137\n",
      "\n",
      "    accuracy                          0.896       499\n",
      "   macro avg      0.896     0.835     0.859       499\n",
      "weighted avg      0.896     0.896     0.891       499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:24:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª XGBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.902     0.964     0.932       362\n",
      "           1      0.884     0.723     0.795       137\n",
      "\n",
      "    accuracy                          0.898       499\n",
      "   macro avg      0.893     0.843     0.864       499\n",
      "weighted avg      0.897     0.898     0.894       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure you have NLTK stopwords downloaded:\n",
    "# import nltk; nltk.download('stopwords')\n",
    "\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=dutch_stopwords)\n",
    "\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_train.columns = df_test.columns = ['question', 'label']\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "# Vectorize\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train['question'])\n",
    "y_train = df_train['label']\n",
    "X_test = vectorizer.transform(df_test['question'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Models with your parameters\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(alpha=0.01),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        C=10, class_weight='balanced', penalty='l2', solver='saga', max_iter=1000\n",
    "    ),\n",
    "    \"SVM\": SVC(C=1, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        class_weight='balanced', max_depth=None, n_estimators=250\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.3, n_estimators=250,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\nðŸ§ª {name}:\\n\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb548895-f17d-4f34-af86-41194a9d6673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Naive Bayes:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.772     0.961     0.856       362\n",
      "           1      0.708     0.248     0.368       137\n",
      "\n",
      "    accuracy                          0.766       499\n",
      "   macro avg      0.740     0.605     0.612       499\n",
      "weighted avg      0.754     0.766     0.722       499\n",
      "\n",
      "\n",
      "ðŸ§ª Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.841     0.906     0.872       362\n",
      "           1      0.688     0.547     0.610       137\n",
      "\n",
      "    accuracy                          0.808       499\n",
      "   macro avg      0.765     0.727     0.741       499\n",
      "weighted avg      0.799     0.808     0.800       499\n",
      "\n",
      "\n",
      "ðŸ§ª SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.798     0.961     0.872       362\n",
      "           1      0.778     0.358     0.490       137\n",
      "\n",
      "    accuracy                          0.796       499\n",
      "   macro avg      0.788     0.659     0.681       499\n",
      "weighted avg      0.793     0.796     0.767       499\n",
      "\n",
      "\n",
      "ðŸ§ª Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.781     0.978     0.869       362\n",
      "           1      0.826     0.277     0.415       137\n",
      "\n",
      "    accuracy                          0.786       499\n",
      "   macro avg      0.804     0.628     0.642       499\n",
      "weighted avg      0.794     0.786     0.744       499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:31:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª XGBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.794     0.956     0.867       362\n",
      "           1      0.746     0.343     0.470       137\n",
      "\n",
      "    accuracy                          0.788       499\n",
      "   macro avg      0.770     0.649     0.669       499\n",
      "weighted avg      0.781     0.788     0.758       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure you have NLTK stopwords downloaded:\n",
    "# import nltk; nltk.download('stopwords')\n",
    "\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=dutch_stopwords)\n",
    "\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_train.columns = df_test.columns = ['question', 'label']\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "# âœ… Remove 'hoeveel'\n",
    "def remove_hoeveel(text):\n",
    "    return ' '.join([w for w in text.split() if w.lower() != 'hoeveel'])\n",
    "\n",
    "df_train['question'] = df_train['question'].apply(remove_hoeveel)\n",
    "df_test['question'] = df_test['question'].apply(remove_hoeveel)\n",
    "\n",
    "\n",
    "# Vectorize\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train['question'])\n",
    "y_train = df_train['label']\n",
    "X_test = vectorizer.transform(df_test['question'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Models with your parameters\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(alpha=0.01),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        C=10, class_weight='balanced', penalty='l2', solver='saga', max_iter=1000\n",
    "    ),\n",
    "    \"SVM\": SVC(C=1, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        class_weight='balanced', max_depth=None, n_estimators=250\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.3, n_estimators=250,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\nðŸ§ª {name}:\\n\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9954a2-88bb-4fa0-8b29-241a0ccf8779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
