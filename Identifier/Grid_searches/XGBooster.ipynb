{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04903070-63eb-4fa4-a1fb-01c6be3c1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257b6a07-f0eb-43f0-867d-670cff32c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "1  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "2  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "3  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "4  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "\n",
      "                                            question  statistical  \\\n",
      "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
      "1  2.Kan de minister toelichten op welke manier B...            0   \n",
      "2  3.Kan de minister in het bijzonder toelichten ...            0   \n",
      "3  4.Kan de minister in het bijzonder toelichten ...            0   \n",
      "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
      "\n",
      "                        theme    file_name  \\\n",
      "0  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "1  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "2  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "3  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "4  Brussel en de Vlaamse Rand  1752906.txt   \n",
      "\n",
      "                                          clean_text  \n",
      "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
      "1  Kan de minister toelichten op welke manier Bru...  \n",
      "2  Kan de minister in het bijzonder toelichten op...  \n",
      "3  Kan de minister in het bijzonder toelichten op...  \n",
      "4  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64572 entries, 0 to 64571\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      64262 non-null  object\n",
      " 1   question     64572 non-null  object\n",
      " 2   statistical  64572 non-null  int64 \n",
      " 3   theme        64572 non-null  object\n",
      " 4   file_name    64572 non-null  object\n",
      " 5   clean_text   64572 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd())# Ga √©√©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemesENnoWords9.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "#visualize the data\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce69190-494e-44f3-992e-9049e7af4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 64572\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"context\",\"file_name\",\"question\",\"statistical\"]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# ‚úÖ Drop rare themes (appearing < 2 times)\n",
    "theme_counts = df[\"theme\"].value_counts()\n",
    "valid_themes = theme_counts[theme_counts >= 100].index\n",
    "df = df[df[\"theme\"].isin(valid_themes)]\n",
    "\n",
    "# ‚úÖ Recompute label encoding AFTER filtering\n",
    "unique_themes = list(df[\"theme\"].unique())\n",
    "theme_to_id = {theme: idx for idx, theme in enumerate(unique_themes)}\n",
    "id_to_theme = {idx: theme for theme, idx in theme_to_id.items()}\n",
    "df[\"theme_id\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "# Amount of rows\n",
    "print(f\"Number of rows after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcab42d5-5856-4684-903a-a4d45997d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jefva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "# === Setup\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "dutch_stopwords = stopwords.words(\"dutch\")\n",
    "\n",
    "# === Define your custom theme-to-ID mapping\n",
    "theme_to_id = {\n",
    "    \"Brussel en de Vlaamse Rand\": 0,\n",
    "    \"Energie\": 1,\n",
    "    \"Milieu en Landbouw\": 2,\n",
    "    \"Toerisme\": 3,\n",
    "    \"Economie en Arbeid\": 4,\n",
    "    \"Sport\": 5,\n",
    "    \"Bestuur en Beleid\": 6,\n",
    "    \"Justitie en Handhaving\": 7,\n",
    "    \"Cultuur en Communicatie\": 8,\n",
    "    \"Mobiliteit en Infrastructuur\": 9,\n",
    "    \"Welzijn en Gezondheid\": 10,\n",
    "    \"Begroting\": 11,\n",
    "    \"Wonen\": 12,\n",
    "    \"Onderwijs en Samenleving\": 13,\n",
    "    \"Internationaal Beleid\": 14,\n",
    "    \"Onroerend erfgoed\": 15,\n",
    "    \"Financi√´n\": 16,\n",
    "    \"Wetenschap en Innovatie\": 17\n",
    "}\n",
    "\n",
    "# === Apply the mapping manually\n",
    "df[\"label\"] = df[\"theme\"].map(theme_to_id)\n",
    "\n",
    "# Sanity check\n",
    "assert df[\"label\"].isnull().sum() == 0, \"Some themes in df['theme'] are missing in the theme_to_id mapping.\"\n",
    "\n",
    "# Create ID column if not present\n",
    "if \"id\" not in df.columns:\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"id\"] = df.index\n",
    "\n",
    "# === Split the data with row IDs tracked\n",
    "X_train, X_temp, y_train, y_temp, id_train, id_temp = train_test_split(\n",
    "    df[\"clean_text\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    df[\"id\"].tolist(),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test, id_val, id_test = train_test_split(\n",
    "    X_temp, y_temp, id_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# === Save test set with only IDs and theme IDs\n",
    "pd.DataFrame({\n",
    "    \"clean_text\": X_test,\n",
    "    \"label\": y_test\n",
    "}).to_excel(\"Test_data_HeldOut_15percentxgb.xlsx\", index=False)\n",
    "\n",
    "# === TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    stop_words=dutch_stopwords\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df8382c-6c69-49f4-9617-b241f5904f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV wordt uitgevoerd... üöÄ\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:11:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Trainingstijd: 583.41 seconden\n",
      "Beste parameters: {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 400}\n",
      "=== Evaluatie op validatieset ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.42      0.54        84\n",
      "           1       0.73      0.51      0.60       332\n",
      "           2       0.55      0.66      0.60      1504\n",
      "           3       0.70      0.43      0.53       162\n",
      "           4       0.71      0.61      0.65       779\n",
      "           5       0.60      0.25      0.35        60\n",
      "           6       0.69      0.62      0.65       622\n",
      "           7       0.69      0.40      0.51       129\n",
      "           8       0.76      0.53      0.62       348\n",
      "           9       0.67      0.86      0.76      2574\n",
      "          10       0.66      0.60      0.63       996\n",
      "          11       0.81      0.59      0.68       128\n",
      "          12       0.71      0.54      0.61       344\n",
      "          13       0.67      0.61      0.64      1157\n",
      "          14       0.71      0.55      0.62       210\n",
      "          15       0.70      0.39      0.50       114\n",
      "          16       0.75      0.59      0.66       118\n",
      "          17       0.12      0.04      0.06        25\n",
      "\n",
      "    accuracy                           0.66      9686\n",
      "   macro avg       0.67      0.51      0.57      9686\n",
      "weighted avg       0.67      0.66      0.65      9686\n",
      "\n",
      "      Metric     Score\n",
      "0   Accuracy  0.659302\n",
      "1  Precision  0.665596\n",
      "2     Recall  0.659302\n",
      "3         F1  0.652954\n",
      "üìÑ Validatievoorspellingen opgeslagen in 'xgb_val.xlsx'\n",
      "=== Evaluatie op test set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        84\n",
      "           1       0.67      0.49      0.56       331\n",
      "           2       0.54      0.67      0.60      1505\n",
      "           3       0.69      0.42      0.52       161\n",
      "           4       0.73      0.63      0.68       780\n",
      "           5       0.74      0.23      0.35        61\n",
      "           6       0.68      0.61      0.64       621\n",
      "           7       0.80      0.44      0.57       129\n",
      "           8       0.69      0.51      0.59       349\n",
      "           9       0.68      0.85      0.76      2575\n",
      "          10       0.66      0.59      0.62       997\n",
      "          11       0.82      0.58      0.68       128\n",
      "          12       0.68      0.55      0.61       343\n",
      "          13       0.69      0.62      0.65      1157\n",
      "          14       0.69      0.51      0.58       210\n",
      "          15       0.66      0.40      0.50       114\n",
      "          16       0.71      0.56      0.62       117\n",
      "          17       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.66      9686\n",
      "   macro avg       0.66      0.51      0.56      9686\n",
      "weighted avg       0.66      0.66      0.65      9686\n",
      "\n",
      "üìÑ Testvoorspellingen opgeslagen in 'xgb_test_predictions.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # === 1. Define parameter grid\n",
    "# param_grid = {\n",
    "#     \"learning_rate\": [0.1, 0.05],            # 0.1 is standard, 0.05 often helps generalization\n",
    "#     \"max_depth\": [3, 5],                     # 3 avoids overfitting; 5 adds flexibility\n",
    "#     \"gamma\": [0, 1],                         # 0 = no regularization; 1 = mild pruning\n",
    "#     \"n_estimators\": [200, 400],              # train faster, early-stopping can help later\n",
    "#     \"colsample_bytree\": [0.5, 0.8]           # moderate feature subsampling helps regularization\n",
    "# }\n",
    "\n",
    "# # === 2. Instantiate model\n",
    "# xgb = XGBClassifier(\n",
    "#     use_label_encoder=False,\n",
    "#     eval_metric='mlogloss',\n",
    "#     n_jobs=-1,\n",
    "#     verbosity=1,\n",
    "#     random_state=42,\n",
    "#     tree_method='hist',\n",
    "#     device='cuda'      # üëà GPU prediction\n",
    "# )\n",
    "\n",
    "\n",
    "# # === 3. Grid Search\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=\"f1_weighted\",\n",
    "#     cv=3,\n",
    "#     verbose=3,  # üëà shows each fold of each fit\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# Minimal parameter grid for text data\n",
    "param_grid = {\n",
    "    'max_depth': [4],\n",
    "    'learning_rate': [ 0.5],#[ 0.5,0.3, 0.1]\n",
    "    'n_estimators': [400],#[250,400]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1,\n",
    "    verbosity=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"GridSearchCV wordt uitgevoerd... üöÄ\")\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"‚è±Ô∏è Trainingstijd: {end_time - start_time:.2f} seconden\")\n",
    "\n",
    "# === 5. Predict on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_val_vec)\n",
    "\n",
    "print(f\"Beste parameters: {grid_search.best_params_}\")\n",
    "print(\"=== Evaluatie op validatieset ===\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n",
    "\n",
    "# === 6. Summary table\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=1)\n",
    "\n",
    "baseline_results = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Score\": [accuracy, precision, recall, f1]\n",
    "})\n",
    "print(baseline_results)\n",
    "\n",
    "# === 7. Save validation predictions to Excel (with IDs)\n",
    "df_val_predictions = pd.DataFrame({\n",
    "    \"id\": id_val,\n",
    "    \"True Label\": y_val,\n",
    "    \"Predicted Label\": y_pred\n",
    "})\n",
    "df_val_predictions.to_excel(\"xgb_val.xlsx\", index=False)\n",
    "print(\"üìÑ Validatievoorspellingen opgeslagen in 'xgb_val.xlsx'\")\n",
    "\n",
    "# === 8. Predict on test set\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# === 9. Classification report on test set\n",
    "print(\"=== Evaluatie op test set ===\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "# === 10. Save test predictions to Excel (with IDs)\n",
    "df_test_predictions = pd.DataFrame({\n",
    "    \"clean_text\": X_test,\n",
    "    \"True Label\": y_test,\n",
    "    \"Predicted Label\": y_test_pred\n",
    "})\n",
    "df_test_predictions.to_excel(\"xgb_test.xlsx\", index=False)\n",
    "print(\"üìÑ Testvoorspellingen opgeslagen in 'xgb_test_predictions.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c9130-7a16-420b-a02d-a53589733746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import time\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# === 1. Define the objective function ===\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.5, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2000, step=500),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    # === 3-fold CV on training set\n",
    "    score = cross_val_score(model, X_train_vec, y_train, scoring=\"f1_weighted\", cv=3).mean()\n",
    "    return score\n",
    "\n",
    "# === 2. Run Optuna Study ===\n",
    "print(\"üöÄ Optuna hyperparameter search gestart...\")\n",
    "start = time.time()\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # ‚è±Ô∏è Adjust number of trials as needed\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚è±Ô∏è Totale zoektijd: {end - start:.2f} seconden\")\n",
    "print(\"‚úÖ Beste parameters:\", study.best_params)\n",
    "\n",
    "# === 3. Train final model with best parameters ===\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"use_label_encoder\": False,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\"\n",
    "})\n",
    "\n",
    "best_model = XGBClassifier(**best_params)\n",
    "best_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# === 4. Predict on validation set ===\n",
    "y_pred = best_model.predict(X_val_vec)\n",
    "y_val_labels = le.inverse_transform(y_val)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"üìä Evaluatie op validatieset:\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n",
    "\n",
    "# === 5. Score overview ===\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=1)\n",
    "\n",
    "baseline_results = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Score\": [accuracy, precision, recall, f1]\n",
    "})\n",
    "print(baseline_results)\n",
    "\n",
    "# === 6. Predict on test set ===\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "print(\"üìä Evaluatie op testset:\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "# === 7. Save predictions ===\n",
    "df_test_predictions = pd.DataFrame({\n",
    "    \"Question\": X_test,\n",
    "    \"True Label\": y_test_labels,\n",
    "    \"Predicted Label\": y_test_pred_labels\n",
    "})\n",
    "df_test_predictions.to_excel(\"xgb_test_predictions_optuna.xlsx\", index=False)\n",
    "print(\"üìÑ Testvoorspellingen opgeslagen in 'xgb_test_predictions_optuna.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a177c-19ec-42c0-aec1-1112096c35ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
