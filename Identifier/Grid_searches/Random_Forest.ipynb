{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6aef28d-bd58-42ec-8f36-6d12eed7c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8aef582-4bda-4ff7-9314-bc2f5c3fd6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  label\n",
      "0  Hoeveel personen waren er voor de Vlaamse over...      1\n",
      "1  Hoeveel dagen/uren is die delegatie er geweest...      0\n",
      "2                             Hoe verklaart hij dat?      0\n",
      "3  Hoeveel bedroeg de totale factuur voor de Vlaa...      0\n",
      "4  Verder sprak ik de Catalaanse minister van Fin...      0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2386 entries, 0 to 2385\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  2386 non-null   object\n",
      " 1   label     2386 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 1. Load & Preprocess Data\n",
    "script_dir = os.path.dirname(os.getcwd())# Ga √©√©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Identifier\\Trainig_data.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "#visualize the data\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b893350-1cde-447c-8840-9d3c79e0dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jefva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "\n",
    "# === Setup\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "dutch_stopwords = stopwords.words(\"dutch\")\n",
    "\n",
    "\n",
    "\n",
    "# === Split the data with row IDs tracked\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df[\"question\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# === Save test set with only IDs and theme IDs\n",
    "# pd.DataFrame({\n",
    "#     \"clean_text\": X_test,\n",
    "#     \"label\": y_test\n",
    "# }).to_excel(\"Test_data_HeldOut_15percentlogreg.xlsx\", index=False)\n",
    "\n",
    "# === TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    stop_words=dutch_stopwords\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4381082e-dc98-4f60-bf3d-1eded18edf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV wordt uitgevoerd... üöÄ\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "‚è±Ô∏è Trainingstijd: 77.70 seconden\n",
      "Beste parameters: {'class_weight': 'balanced', 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "=== Evaluatie op validatieset ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       282\n",
      "           1       0.70      0.58      0.63        76\n",
      "\n",
      "    accuracy                           0.86       358\n",
      "   macro avg       0.79      0.76      0.77       358\n",
      "weighted avg       0.85      0.86      0.85       358\n",
      "\n",
      "      Metric     Score\n",
      "0   Accuracy  0.857542\n",
      "1  Precision  0.850529\n",
      "2     Recall  0.857542\n",
      "3         F1  0.852485\n",
      "üìÑ Validatievoorspellingen opgeslagen in 'rf_val.xlsx'\n",
      "=== Evaluatie op test set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       283\n",
      "           1       0.70      0.52      0.60        75\n",
      "\n",
      "    accuracy                           0.85       358\n",
      "   macro avg       0.79      0.73      0.75       358\n",
      "weighted avg       0.84      0.85      0.84       358\n",
      "\n",
      "üìÑ Testvoorspellingen opgeslagen in 'rf_test_predictions.xlsx'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Define a small parameter grid for RF\n",
    "param_grid = {\n",
    "    'n_estimators': [200,300,400,600,800],     # Number of trees [200,300,400,600,800]\n",
    "    'max_features': ['sqrt', 'log2'],        # Full trees vs shallower trees ['sqrt', 'log2']\n",
    "    'class_weight': ['balanced']  # Handle imbalance\n",
    "}\n",
    "\n",
    "# 5. GridSearchCV setup\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,  # keep folds lower for speed\n",
    "    verbose=2,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "print(\"GridSearchCV wordt uitgevoerd... üöÄ\")\n",
    "start_time = time.time()\n",
    "grid.fit(X_train_vec, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"‚è±Ô∏è Trainingstijd: {end_time - start_time:.2f} seconden\")\n",
    "\n",
    "# === 5. Predict on validation set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_val_vec)\n",
    "\n",
    "print(f\"Beste parameters: {grid.best_params_}\")\n",
    "print(\"=== Evaluatie op validatieset ===\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n",
    "\n",
    "# === 6. Summary table\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=1)\n",
    "\n",
    "baseline_results = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Score\": [accuracy, precision, recall, f1]\n",
    "})\n",
    "print(baseline_results)\n",
    "\n",
    "# === 7. Save validation predictions to Excel (with IDs)\n",
    "df_val_predictions = pd.DataFrame({\n",
    "    \"True Label\": y_val,\n",
    "    \"Predicted Label\": y_pred\n",
    "})\n",
    "df_val_predictions.to_excel(\"rf_val.xlsx\", index=False)\n",
    "print(\"üìÑ Validatievoorspellingen opgeslagen in 'rf_val.xlsx'\")\n",
    "\n",
    "# === 8. Predict on test set\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# === 9. Classification report on test set\n",
    "print(\"=== Evaluatie op test set ===\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "# === 10. Save test predictions to Excel (with IDs)\n",
    "df_test_predictions = pd.DataFrame({\n",
    "    \"clean_text\": X_test,\n",
    "    \"True Label\": y_test,\n",
    "    \"Predicted Label\": y_test_pred\n",
    "})\n",
    "df_test_predictions.to_excel(\"rf.xlsx\", index=False)\n",
    "print(\"üìÑ Testvoorspellingen opgeslagen in 'rf_test_predictions.xlsx'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa2bd3-4f72-4fe5-82f4-ce40c9664239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
