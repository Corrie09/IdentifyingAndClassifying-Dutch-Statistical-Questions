{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38aa0383-3196-4786-8538-7a83ae3c1b91",
   "metadata": {},
   "source": [
    "**Een Identifier die gebruik maakt van active learning. 500 vragen zijn zelf gelabeled. Vervolgens fine tunen we een bert model hierop. We laten deze getunede bert dan zelf 1000 ongeziene vragen labelen. We kijken naar de 20% waar het model het minst zeker van is. Die kijken we handmatig na. De gecorrigeerde voegen we dan toe aan de training data en zo herhalen we dit proces tot het bert model naar behoren werkt.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcdcad8-89e7-4a00-a616-6beae8aea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457b33d1-be39-4fd0-8a00-ff8d4044e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load labeled data\n",
    "df = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df.columns = ['question', 'label']\n",
    "\n",
    "# Remove duplicate questions\n",
    "df = df.drop_duplicates(subset='question')\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['question'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df90c5da-3f2c-46f4-ab38-65d32c469b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 44\n",
      "Tokens: ['Zal', 'de', 'minister', 'initiatie', '##ven', 'nemen', 'om', 'ervoor', 'te', 'zorgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zorgt', '##ra', '##ject', 'in', 'de', 'toekomst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'tegemoetkoming', 'van', 'hulpmiddel', '##en', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'termijn', '?']\n",
      "Number of tokens: 55\n",
      "Tokens: ['Za', '##l', 'de', 'minister', 'init', '##iati', '##even', 'nemen', 'om', 'ervoor', 'te', 'zo', '##rgen', 'dat', 'personen', 'in', 'een', 'pal', '##lia', '##tief', 'zo', '##rgt', '##raj', '##ect', 'in', 'de', 'toe', '##komst', 'wel', 'aan', '##spraak', 'kunnen', 'maken', 'op', 'een', 'te', '##gem', '##oe', '##tkom', '##ing', 'van', 'hulp', '##mid', '##delen', '?', 'Zo', 'ja', ',', 'welke', 'en', 'binnen', 'welke', 'term', '##ijn', '?']\n",
      "Number of tokens: 40\n",
      "Tokens: ['Zal', 'Ġde', 'Ġminister', 'Ġinitiatieven', 'Ġnemen', 'Ġom', 'Ġervoor', 'Ġte', 'Ġzorgen', 'Ġdat', 'Ġpersonen', 'Ġin', 'Ġeen', 'Ġpall', 'i', 'atief', 'Ġzorgt', 'raject', 'Ġin', 'Ġde', 'Ġtoekomst', 'Ġwel', 'Ġaanspraak', 'Ġkunnen', 'Ġmaken', 'Ġop', 'Ġeen', 'Ġtegemoetkoming', 'Ġvan', 'Ġhulpmiddelen', '?', 'ĠZo', 'Ġja', ',', 'Ġwelke', 'Ġen', 'Ġbinnen', 'Ġwelke', 'Ġtermijn', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Een random lange zin om te zien of we met 128 tokens per zin goed zitten. Deze zin bevat 44 dus 128 zou ok moeten zijn \n",
    "'''\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\" )\n",
    "tokenizer3 = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "text = \"Zal de minister initiatieven nemen om ervoor te zorgen dat personen in een palliatief zorgtraject in de toekomst wel aanspraak kunnen maken op een tegemoetkoming van hulpmiddelen? Zo ja, welke en binnen welke termijn?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens2 = tokenizer2.tokenize(text)\n",
    "tokens3 = tokenizer3.tokenize(text)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens2))\n",
    "print(\"Tokens:\", tokens2)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens3))\n",
    "print(\"Tokens:\", tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c569f4d-d2fc-4670-8e05-d272445a13ca",
   "metadata": {},
   "source": [
    "**GroNLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf1b70-1be0-409c-8206-2b06b147b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3936c0c9-dc6d-4874-a141-c74042e86b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0100932-3e75-4f62-b457-4b989981e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1584' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1584/1584 03:31, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.272517</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.891790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.370214</td>\n",
       "      <td>0.865823</td>\n",
       "      <td>0.884731</td>\n",
       "      <td>0.865823</td>\n",
       "      <td>0.871413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.482546</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.884037</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.877060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.608180</td>\n",
       "      <td>0.881013</td>\n",
       "      <td>0.893657</td>\n",
       "      <td>0.881013</td>\n",
       "      <td>0.884976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.618076</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.890612</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.889495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.632299</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.893835</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.888808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.665578</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.891456</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.892285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.659775</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.887578</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.886763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1584, training_loss=0.14402549706324183, metrics={'train_runtime': 211.5651, 'train_samples_per_second': 59.67, 'train_steps_per_second': 7.487, 'total_flos': 830378490716160.0, 'train_loss': 0.14402549706324183, 'epoch': 8.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c49be3-9cef-45ce-909e-75d5020f72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "\n",
    "# Clean and convert to string\n",
    "df_test = df_test.dropna(subset=['question', 'label'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Extract questions and labels\n",
    "test_texts = df_test['question'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4e01d0-04b9-4cfb-a389-887316ff420f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Run predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Get raw logits\n",
    "logits = predictions_output.predictions\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "# Load test questions (in same order)\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test = df_test.dropna(subset=['question']).reset_index(drop=True)\n",
    "df_test.columns = ['question', 'true_label']\n",
    "\n",
    "# Create final DataFrame with question + true + predicted\n",
    "df = pd.DataFrame({\n",
    "    \"question\": df_test[\"question\"],\n",
    "    \"true_label\": df_test[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"bert_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1432bdca-0272-40cd-8875-bbbde4f37a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       362\n",
      "           1       0.90      0.85      0.88       137\n",
      "\n",
      "    accuracy                           0.93       499\n",
      "   macro avg       0.92      0.91      0.92       499\n",
      "weighted avg       0.93      0.93      0.93       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b76e2-edae-470e-bde0-6fd759e8ad28",
   "metadata": {},
   "source": [
    "**mBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653885d1-4140-45f4-bf3d-e72b08e15cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\" )  # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73855eb-3b1b-4774-a4ca-c4af3d0dbc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1584' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1584/1584 04:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.488089</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.828258</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.683249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.395661</td>\n",
       "      <td>0.863291</td>\n",
       "      <td>0.883684</td>\n",
       "      <td>0.863291</td>\n",
       "      <td>0.838041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.459078</td>\n",
       "      <td>0.815190</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.815190</td>\n",
       "      <td>0.814017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.362548</td>\n",
       "      <td>0.843038</td>\n",
       "      <td>0.862511</td>\n",
       "      <td>0.843038</td>\n",
       "      <td>0.849369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.401146</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.863151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.604320</td>\n",
       "      <td>0.840506</td>\n",
       "      <td>0.863110</td>\n",
       "      <td>0.840506</td>\n",
       "      <td>0.847563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.571859</td>\n",
       "      <td>0.853165</td>\n",
       "      <td>0.868279</td>\n",
       "      <td>0.853165</td>\n",
       "      <td>0.858270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.507878</td>\n",
       "      <td>0.878481</td>\n",
       "      <td>0.876641</td>\n",
       "      <td>0.878481</td>\n",
       "      <td>0.877443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1584, training_loss=0.33859208858374396, metrics={'train_runtime': 291.4924, 'train_samples_per_second': 43.308, 'train_steps_per_second': 5.434, 'total_flos': 830378490716160.0, 'train_loss': 0.33859208858374396, 'epoch': 8.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1e9586-3c0b-414f-afb0-e2004d246b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Run predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Get raw logits\n",
    "logits = predictions_output.predictions\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "# Load test questions (in same order)\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test = df_test.dropna(subset=['question']).reset_index(drop=True)\n",
    "df_test.columns = ['question', 'true_label']\n",
    "\n",
    "# Create final DataFrame with question + true + predicted\n",
    "df = pd.DataFrame({\n",
    "    \"question\": df_test[\"question\"],\n",
    "    \"true_label\": df_test[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"mbert_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a052dba0-42ce-4266-beed-d0861f992df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       362\n",
      "           1       0.84      0.82      0.83       137\n",
      "\n",
      "    accuracy                           0.91       499\n",
      "   macro avg       0.89      0.88      0.89       499\n",
      "weighted avg       0.91      0.91      0.91       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e0330-ae6b-4f01-bb27-45ced74d4248",
   "metadata": {},
   "source": [
    "**robBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd605e7a-0eea-45b0-a127-743bfdac8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer =RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\") # or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604bcdb4-6619-4b9e-bdbc-e30936ddc5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1584' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1584/1584 03:07, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.522381</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.828258</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.683249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.462455</td>\n",
       "      <td>0.787342</td>\n",
       "      <td>0.750003</td>\n",
       "      <td>0.787342</td>\n",
       "      <td>0.743327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.382596</td>\n",
       "      <td>0.868354</td>\n",
       "      <td>0.867123</td>\n",
       "      <td>0.868354</td>\n",
       "      <td>0.853787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.408502</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.839339</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.838631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.638507</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.826341</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.807176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.489926</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.867485</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.572110</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.863843</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.862134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.552783</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>0.870745</td>\n",
       "      <td>0.875949</td>\n",
       "      <td>0.871572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1584, training_loss=0.3383156362206045, metrics={'train_runtime': 187.0494, 'train_samples_per_second': 67.49, 'train_steps_per_second': 8.468, 'total_flos': 830378490716160.0, 'train_loss': 0.3383156362206045, 'epoch': 8.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,                 # 👈 Keep only the last checkpoint\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # 🔥 Log only once per epoch\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# ✅ 10. Define Metrics for Evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5de913-a83d-4479-b2d2-4d2b181fba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = QuestionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Run predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Get raw logits\n",
    "logits = predictions_output.predictions\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "# Load test questions (in same order)\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test = df_test.dropna(subset=['question']).reset_index(drop=True)\n",
    "df_test.columns = ['question', 'true_label']\n",
    "\n",
    "# Create final DataFrame with question + true + predicted\n",
    "df = pd.DataFrame({\n",
    "    \"question\": df_test[\"question\"],\n",
    "    \"true_label\": df_test[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"robbert_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e75a91-5c26-4c69-b5c9-2ef483151569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       362\n",
      "           1       0.84      0.72      0.77       137\n",
      "\n",
      "    accuracy                           0.89       499\n",
      "   macro avg       0.87      0.83      0.85       499\n",
      "weighted avg       0.88      0.89      0.88       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get raw predictions\n",
    "outputs = trainer.predict(test_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Print evaluation report\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cb30a-108e-41d9-9827-b4c081340ef7",
   "metadata": {},
   "source": [
    "**regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c515a7b-a509-4261-be4c-8ea8d30938b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Set 1 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "🧪 Set 2 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Your regex sets\n",
    "statistical_sets = {\n",
    "    \"Set 1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    # \"Set 1.1\": [\n",
    "    #     r\"\\b(aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "    #     r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    # ],\n",
    "    # \"Set 2\": [\n",
    "    #     r\"\\b(hoe vaak|hoe groot|gemiddelde van|mediaan van|ratio van|procent van)\\b\",\n",
    "    #     r\"\\b(stijging van|daling van|verandering in|ontwikkeling in|schommeling van|impact op)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|analyse van)?\\s*(de|een)?\\s*(cijfers|gegevens|statistieken|tellingen)\\b\"\n",
    "    # ],\n",
    "    # \"Set 3\": [\n",
    "    #     r\"\\b(bruto|netto|inkomen|uitgaven|begroting|subsidies|belasting|tarief|BBP|inflatie|schulden|werkloosheid|bestedingen|consumptie)\\b\",\n",
    "    #     r\"\\b(bevolking|leeftijdsgroep|demografie|migratie|emigratie|immigratie|huishoudens|gezinnen|verhouding tussen|dichtheid)\\b\",\n",
    "    #     r\"\\b(aantal|hoeveelheid|grootte van|gemiddelde|mediaan|percentage|spreiding|percentiel|kwartiel|standaarddeviatie)\\b\"\n",
    "    # ],\n",
    "    # \"Set 4\": [\n",
    "    #     r\"\\b(zorgkosten|patiënten|ziekenhuisopnames|sterftecijfers|levensverwachting|gezondheidsuitgaven|vaccinaties|epidemieën|medicatiegebruik)\\b\",\n",
    "    #     r\"\\b(reistijd|filedruk|kilometers afgelegd|verkeersongevallen|CO2-uitstoot|luchtvervuiling|hernieuwbare energie|klimaatverandering|waterkwaliteit)\\b\",\n",
    "    #     r\"\\b(grondprijzen|woningmarkt|huurprijzen|hypotheken|verkoopcijfers|bouwvergunningen|energieverbruik)\\b\"\n",
    "    # ],\n",
    "    # \"Set 5\": [\n",
    "    #     r\"\\b(vergeleken met|ten opzichte van|in vergelijking met|in het verleden|sinds \\d{4}|tussen \\d{4} en \\d{4})\\b\",\n",
    "    #     r\"\\b(ontwikkeling sinds|historische gegevens|trendanalyse|jaarverslagen|statistische rapporten)\\b\"\n",
    "    # ],\n",
    "    # \"Set 6\": [\n",
    "    #     r\"\\b(hoeveel|aantal|percentage|statistieken|cijfers|gegevens|data)\\b\",\n",
    "    #     r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "    #     r\"\\b(per\\s+\\w+|per\\s+\\d+|in\\s+\\d{4}|tussen\\s+\\d{4}\\s+en\\s+\\d{4})\\b\",  \n",
    "    #     r\"\\b(wat is het aantal|hoe groot is|hoe vaak|gemiddelde van|ratio van)\\b\"\n",
    "    # ],\n",
    "    # \"Set 7\": [\n",
    "    #     r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "    #     r\"\\b(totale budget|welk budget|eenzelfde hoogte|gemiddeld aantal|geëvolueerd in de periode|jaarlijkse kostprijs |het aantal)\\b\",\n",
    "    #     r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    # ],\n",
    "    # \"Set 8\": [\n",
    "    #     r\"^(Wat is het aantal|Hoeveel)\\b\",\n",
    "    #     r\"\\b(totale budget|welk budget|cijfer over|eenzelfde hoogte|gemiddeld aantal|geëvolueerd in de periode|jaarlijkse kostprijs|het aantal)\\b\",\n",
    "    #     r\"\\b(bedroeg|welk|Kan de minister|wat was)\\s*(de|een)?\\s*(factuur|budget|overzicht|kostprijs)\\b\",\n",
    "    #     r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van|bedroeg)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers|factuur|overzicht)\\b\"\n",
    "    # ],\n",
    "    \"Set 2\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules to each question\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    df_test[f\"{set_name}_match\"] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\n🧪 {set_name} — Evaluation against true labels:\")\n",
    "        print(classification_report(df_test['label'], df_test[f\"{set_name}_match\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74c64ae-cf2c-4636-955e-28a123de12fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Set_1 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.914     0.914       362\n",
      "           1      0.774     0.774     0.774       137\n",
      "\n",
      "    accuracy                          0.876       499\n",
      "   macro avg      0.844     0.844     0.844       499\n",
      "weighted avg      0.876     0.876     0.876       499\n",
      "\n",
      "\n",
      "🧪 Set_2 — Evaluation against true labels:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.859     0.994     0.922       362\n",
      "           1      0.975     0.569     0.719       137\n",
      "\n",
      "    accuracy                          0.878       499\n",
      "   macro avg      0.917     0.782     0.820       499\n",
      "weighted avg      0.891     0.878     0.866       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_test.columns = ['question', 'label']\n",
    "df_test = df_test.dropna(subset=['question'])\n",
    "df_test['question'] = df_test['question'].astype(str)\n",
    "\n",
    "# Define regex sets\n",
    "statistical_sets = {\n",
    "    \"Set_1\": [\n",
    "        r\"\\b(hoeveel|aantal|percentage van|percentage|cijfer over|data over|statistieken van)\\b\",\n",
    "        r\"\\b(trend in|evolutie van|groei van|toename van|afname van|ontwikkeling van)\\b\",\n",
    "        r\"\\b(?:verschaffen|geven|tonen|lijst|overzicht van)?\\s*(de|een)?\\s*(gegevens|statistieken|cijfers)\\b\"\n",
    "    ],\n",
    "    \"Set_2\": [\n",
    "        r\"^(Hoeveel)\\b\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to apply regex rules\n",
    "def matches_any(question, patterns):\n",
    "    return any(re.search(pat, question, flags=re.IGNORECASE) for pat in patterns)\n",
    "\n",
    "# Save classification reports\n",
    "metrics_list = []\n",
    "\n",
    "# Apply and evaluate each regex set\n",
    "for set_name, patterns in statistical_sets.items():\n",
    "    col_name = f\"{set_name}_match\"\n",
    "    df_test[col_name] = df_test['question'].apply(lambda q: int(matches_any(q, patterns)))\n",
    "\n",
    "    if 'label' in df_test.columns:\n",
    "        print(f\"\\n🧪 {set_name} — Evaluation against true labels:\")\n",
    "        report = classification_report(df_test['label'], df_test[col_name], digits=3, output_dict=True)\n",
    "        print(classification_report(df_test['label'], df_test[col_name], digits=3))\n",
    "        \n",
    "        # Save key metrics\n",
    "        prf = precision_recall_fscore_support(df_test['label'], df_test[col_name], average='binary')\n",
    "        metrics_list.append({\n",
    "            'Regex_Set': set_name,\n",
    "            'Precision': prf[0],\n",
    "            'Recall': prf[1],\n",
    "            'F1-score': prf[2],\n",
    "            'Support': prf[3]\n",
    "        })\n",
    "\n",
    "# Save predictions and metrics\n",
    "df_test.to_csv(\"regex_predictions.csv\", index=False)\n",
    "pd.DataFrame(metrics_list).to_csv(\"regex_evaluation_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad22a47-c250-427c-a25d-7af07874767e",
   "metadata": {},
   "source": [
    "**Traditionals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1d4fb5-856b-4218-8f73-b7f6b3f96fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Naive_Bayes (Training time: 0.00 seconds):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.802     0.953     0.871       362\n",
      "           1      0.754     0.380     0.505       137\n",
      "\n",
      "    accuracy                          0.796       499\n",
      "   macro avg      0.778     0.666     0.688       499\n",
      "weighted avg      0.789     0.796     0.771       499\n",
      "\n",
      "\n",
      "🧪 Logistic_Regression (Training time: 0.03 seconds):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.931     0.936     0.934       362\n",
      "           1      0.830     0.818     0.824       137\n",
      "\n",
      "    accuracy                          0.904       499\n",
      "   macro avg      0.880     0.877     0.879       499\n",
      "weighted avg      0.903     0.904     0.904       499\n",
      "\n",
      "\n",
      "🧪 SVM (Training time: 0.38 seconds):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.896     0.956     0.925       362\n",
      "           1      0.858     0.708     0.776       137\n",
      "\n",
      "    accuracy                          0.888       499\n",
      "   macro avg      0.877     0.832     0.851       499\n",
      "weighted avg      0.886     0.888     0.884       499\n",
      "\n",
      "\n",
      "🧪 Random_Forest (Training time: 7.44 seconds):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.905     0.970     0.936       362\n",
      "           1      0.901     0.730     0.806       137\n",
      "\n",
      "    accuracy                          0.904       499\n",
      "   macro avg      0.903     0.850     0.871       499\n",
      "weighted avg      0.904     0.904     0.900       499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefva\\anaconda3\\envs\\bert_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:58:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 XGBoost (Training time: 7.32 seconds):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.964     0.938       362\n",
      "           1      0.889     0.759     0.819       137\n",
      "\n",
      "    accuracy                          0.908       499\n",
      "   macro avg      0.901     0.862     0.879       499\n",
      "weighted avg      0.907     0.908     0.905       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure stopwords are downloaded\n",
    "# import nltk; nltk.download('stopwords')\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_excel(\"Trainig_data.xlsx\")\n",
    "df_test = pd.read_excel(\"Testset.xlsx\")\n",
    "df_train.columns = df_test.columns = ['question', 'label']\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=dutch_stopwords)\n",
    "X_train = vectorizer.fit_transform(df_train['question'])\n",
    "y_train = df_train['label']\n",
    "X_test = vectorizer.transform(df_test['question'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Store original test set\n",
    "results_df = df_test.copy()\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Naive_Bayes\": MultinomialNB(alpha=0.1),\n",
    "    \"Logistic_Regression\": LogisticRegression(\n",
    "        C=10, class_weight='balanced', penalty='l2', solver='saga', max_iter=1000\n",
    "    ),\n",
    "    \"SVM\": SVC(C=10, class_weight='balanced'),\n",
    "    \"Random_Forest\": RandomForestClassifier(\n",
    "        class_weight='balanced', max_features='sqrt', n_estimators=300\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.1, n_estimators=400,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train, evaluate, and store predictions\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    results_df[name + '_pred'] = preds\n",
    "\n",
    "    print(f\"\\n🧪 {name} (Training time: {training_time:.2f} seconds):\\n\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n",
    "\n",
    "# Save predictions to CSV\n",
    "results_df.to_csv(\"all_model_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b0a99d9-b2d3-4080-9038-a3af79d78e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the files\n",
    "# mbert_df = pd.read_csv(\"mbert_predictions.csv\")\n",
    "# robbert_df = pd.read_csv(\"robbert_predictions.csv\")\n",
    "# bert_df = pd.read_csv(\"bert_predictions.csv\")\n",
    "# models_df = pd.read_csv(\"all_model_predictions.csv\")\n",
    "# regex_df = pd.read_csv(\"regex_predictions.csv\")\n",
    "\n",
    "# # Add a temporary row index to preserve order\n",
    "# mbert_df[\"row_index\"] = range(len(mbert_df))\n",
    "# robbert_df[\"row_index\"] = range(len(robbert_df))\n",
    "# bert_df[\"row_index\"] = range(len(bert_df))\n",
    "# models_df[\"row_index\"] = range(len(models_df))\n",
    "# regex_df[\"row_index\"] = range(len(regex_df))\n",
    "\n",
    "# # Standardize column names\n",
    "# mbert_df = mbert_df.rename(columns={\"true_label\": \"label\"})\n",
    "# robbert_df = robbert_df.rename(columns={\"true_label\": \"label\"})\n",
    "# bert_df = bert_df.rename(columns={\"true_label\": \"label\"})\n",
    "# models_df = models_df.rename(columns={\"label\": \"label\"})\n",
    "# regex_df = regex_df.rename(columns={\"label\": \"label\"})\n",
    "\n",
    "# # Merge on row_index (not question/label) to preserve order\n",
    "# combined = pd.merge(bert_df, models_df.drop(columns=[\"question\", \"label\"]), on=\"row_index\")\n",
    "# combined = pd.merge(combined, regex_df.drop(columns=[\"question\", \"label\"]), on=\"row_index\")\n",
    "\n",
    "# # Drop row_index and reorder columns\n",
    "# combined = combined.drop(columns=[\"row_index\"])\n",
    "# cols = ['question', 'label'] + [col for col in combined.columns if col not in ['question', 'label']]\n",
    "# combined = combined[cols]\n",
    "\n",
    "# # Save to file\n",
    "# combined.to_csv(\"combined_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efff775f-177a-4b9e-8bc9-211eb52e74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved to combined_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Load files fresh ===\n",
    "mbert_df = pd.read_csv(\"mbert_predictions.csv\")\n",
    "robbert_df = pd.read_csv(\"robbert_predictions.csv\")\n",
    "bert_df = pd.read_csv(\"bert_predictions.csv\")\n",
    "models_df = pd.read_csv(\"all_model_predictions.csv\")\n",
    "regex_df = pd.read_csv(\"regex_predictions.csv\")\n",
    "\n",
    "# === 2. Remove any existing row_index columns, explicitly and thoroughly ===\n",
    "for df_name, df in zip([\"mbert_df\", \"robbert_df\", \"bert_df\", \"models_df\", \"regex_df\"],\n",
    "                       [mbert_df, robbert_df, bert_df, models_df, regex_df]):\n",
    "    if \"row_index\" in df.columns:\n",
    "        print(f\"⚠️ {df_name} contains row_index, dropping it.\")\n",
    "        df.drop(columns=[\"row_index\"], inplace=True)\n",
    "    df[\"row_index\"] = range(len(df))  # clean row_index added\n",
    "\n",
    "# === 3. Rename predicted_label columns ===\n",
    "mbert_df.rename(columns={\"true_label\": \"label\", \"predicted_label\": \"mBERT_pred\"}, inplace=True)\n",
    "robbert_df.rename(columns={\"true_label\": \"label\", \"predicted_label\": \"RobBERT_pred\"}, inplace=True)\n",
    "bert_df.rename(columns={\"true_label\": \"label\", \"predicted_label\": \"GroNLP_pred\"}, inplace=True)\n",
    "\n",
    "# === 4. Merge all models step by step ===\n",
    "combined = bert_df[[\"row_index\", \"question\", \"label\", \"GroNLP_pred\"]]\n",
    "combined = pd.merge(combined, mbert_df[[\"row_index\", \"mBERT_pred\"]], on=\"row_index\")\n",
    "combined = pd.merge(combined, robbert_df[[\"row_index\", \"RobBERT_pred\"]], on=\"row_index\")\n",
    "\n",
    "# Traditional model predictions\n",
    "traditional_cols = [col for col in models_df.columns if col not in [\"question\", \"label\", \"row_index\"]]\n",
    "combined = pd.merge(combined, models_df[[\"row_index\"] + traditional_cols], on=\"row_index\")\n",
    "\n",
    "# Regex predictions\n",
    "regex_cols = [col for col in regex_df.columns if col not in [\"question\", \"label\", \"row_index\"]]\n",
    "combined = pd.merge(combined, regex_df[[\"row_index\"] + regex_cols], on=\"row_index\")\n",
    "\n",
    "# === 5. Clean up and reorder ===\n",
    "combined.drop(columns=[\"row_index\"], inplace=True)\n",
    "cols = ['question', 'label'] + [col for col in combined.columns if col not in ['question', 'label']]\n",
    "combined = combined[cols]\n",
    "\n",
    "# === 6. Save final merged file ===\n",
    "combined.to_csv(\"combined_predictions.csv\", index=False)\n",
    "print(\"✅ Successfully saved to combined_predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d92c438-e937-49b7-9e3c-1a1b1fd80b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question', 'label', 'Naive_Bayes_pred', 'Logistic_Regression_pred',\n",
      "       'SVM_pred', 'Random_Forest_pred', 'XGBoost_pred', 'row_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(models_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "831104bc-5906-4d95-ae26-d4ab9854e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic = 122.442, p-value = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "models = [col for col in df.columns if col.endswith('_pred') or col.endswith('_match')]\n",
    "\n",
    "# Binary correct predictions\n",
    "correct_matrix = np.array([(df[model] == df['label']).astype(int) for model in models]).T\n",
    "\n",
    "# Friedman test\n",
    "stat, p = friedmanchisquare(*correct_matrix.T)\n",
    "print(f\"Friedman test statistic = {stat:.3f}, p-value = {p:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e2a21bf-9115-46ba-9c11-b913609a51b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 McNemar's test p-values (lower = significant difference):\n",
      "                          GroNLP_pred  mBERT_pred  RobBERT_pred  \\\n",
      "GroNLP_pred                    1.0000      0.0652        0.0002   \n",
      "mBERT_pred                     0.0652      1.0000        0.1038   \n",
      "RobBERT_pred                   0.0002      0.1038        1.0000   \n",
      "Naive_Bayes_pred               0.0000      0.0000        0.0000   \n",
      "Logistic_Regression_pred       0.0107      0.7428        0.2221   \n",
      "SVM_pred                       0.0001      0.1263        1.0000   \n",
      "Random_Forest_pred             0.0081      0.7493        0.1996   \n",
      "XGBoost_pred                   0.0241      1.0000        0.1352   \n",
      "Set_1_match                    0.0000      0.0270        0.5901   \n",
      "Set_2_match                    0.0001      0.0402        0.6271   \n",
      "\n",
      "                          Naive_Bayes_pred  Logistic_Regression_pred  \\\n",
      "GroNLP_pred                         0.0000                    0.0107   \n",
      "mBERT_pred                          0.0000                    0.7428   \n",
      "RobBERT_pred                        0.0000                    0.2221   \n",
      "Naive_Bayes_pred                    1.0000                    0.0000   \n",
      "Logistic_Regression_pred            0.0000                    1.0000   \n",
      "SVM_pred                            0.0000                    0.2005   \n",
      "Random_Forest_pred                  0.0000                    1.0000   \n",
      "XGBoost_pred                        0.0000                    0.8642   \n",
      "Set_1_match                         0.0001                    0.0436   \n",
      "Set_2_match                         0.0000                    0.1112   \n",
      "\n",
      "                          SVM_pred  Random_Forest_pred  XGBoost_pred  \\\n",
      "GroNLP_pred                 0.0001              0.0081        0.0241   \n",
      "mBERT_pred                  0.1263              0.7493        1.0000   \n",
      "RobBERT_pred                1.0000              0.1996        0.1352   \n",
      "Naive_Bayes_pred            0.0000              0.0000        0.0000   \n",
      "Logistic_Regression_pred    0.2005              1.0000        0.8642   \n",
      "SVM_pred                    1.0000              0.0963        0.1325   \n",
      "Random_Forest_pred          0.0963              1.0000        0.8555   \n",
      "XGBoost_pred                0.1325              0.8555        1.0000   \n",
      "Set_1_match                 0.4799              0.0541        0.0293   \n",
      "Set_2_match                 0.5515              0.0470        0.0315   \n",
      "\n",
      "                          Set_1_match  Set_2_match  \n",
      "GroNLP_pred                    0.0000       0.0001  \n",
      "mBERT_pred                     0.0270       0.0402  \n",
      "RobBERT_pred                   0.5901       0.6271  \n",
      "Naive_Bayes_pred               0.0001       0.0000  \n",
      "Logistic_Regression_pred       0.0436       0.1112  \n",
      "SVM_pred                       0.4799       0.5515  \n",
      "Random_Forest_pred             0.0541       0.0470  \n",
      "XGBoost_pred                   0.0293       0.0315  \n",
      "Set_1_match                    1.0000       1.0000  \n",
      "Set_2_match                    1.0000       1.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from itertools import combinations\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "models = [col for col in df.columns if col.endswith('_label') or col.endswith('_pred') or col.endswith('_match')]\n",
    "\n",
    "# Ground truth\n",
    "y_true = df['label'].values\n",
    "\n",
    "# Prepare p-value result matrix\n",
    "p_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
    "\n",
    "# McNemar test function\n",
    "def mcnemar_p(y_true, pred1, pred2):\n",
    "    table = [[0, 0], [0, 0]]\n",
    "    for y, a, b in zip(y_true, pred1, pred2):\n",
    "        correct_a = int(a == y)\n",
    "        correct_b = int(b == y)\n",
    "        table[correct_b][correct_a] += 1\n",
    "    result = mcnemar(table, exact=True)\n",
    "    return result.pvalue\n",
    "\n",
    "# Compare all model pairs\n",
    "for model1, model2 in combinations(models, 2):\n",
    "    p = mcnemar_p(y_true, df[model1], df[model2])\n",
    "    p_matrix.loc[model1, model2] = p\n",
    "    p_matrix.loc[model2, model1] = p  # symmetric\n",
    "\n",
    "# Fill diagonal with 1s\n",
    "np.fill_diagonal(p_matrix.values, 1.0)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 McNemar's test p-values (lower = significant difference):\")\n",
    "print(p_matrix.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "799caec6-8068-4997-83e3-49d65e7b149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Wilcoxon test p-values (lower = significant difference in per-class F1):\n",
      "                          GroNLP_pred  mBERT_pred  RobBERT_pred  \\\n",
      "GroNLP_pred                       1.0         0.5           0.5   \n",
      "mBERT_pred                        0.5         1.0           0.5   \n",
      "RobBERT_pred                      0.5         0.5           1.0   \n",
      "Naive_Bayes_pred                  0.5         0.5           0.5   \n",
      "Logistic_Regression_pred          0.5         0.5           0.5   \n",
      "SVM_pred                          0.5         0.5           0.5   \n",
      "Random_Forest_pred                0.5         0.5           0.5   \n",
      "XGBoost_pred                      0.5         1.0           0.5   \n",
      "Set_1_match                       0.5         0.5           0.5   \n",
      "Set_2_match                       0.5         0.5           0.5   \n",
      "\n",
      "                          Naive_Bayes_pred  Logistic_Regression_pred  \\\n",
      "GroNLP_pred                            0.5                       0.5   \n",
      "mBERT_pred                             0.5                       0.5   \n",
      "RobBERT_pred                           0.5                       0.5   \n",
      "Naive_Bayes_pred                       1.0                       0.5   \n",
      "Logistic_Regression_pred               0.5                       1.0   \n",
      "SVM_pred                               0.5                       0.5   \n",
      "Random_Forest_pred                     0.5                       1.0   \n",
      "XGBoost_pred                           0.5                       1.0   \n",
      "Set_1_match                            0.5                       0.5   \n",
      "Set_2_match                            0.5                       0.5   \n",
      "\n",
      "                          SVM_pred  Random_Forest_pred  XGBoost_pred  \\\n",
      "GroNLP_pred                    0.5                 0.5           0.5   \n",
      "mBERT_pred                     0.5                 0.5           1.0   \n",
      "RobBERT_pred                   0.5                 0.5           0.5   \n",
      "Naive_Bayes_pred               0.5                 0.5           0.5   \n",
      "Logistic_Regression_pred       0.5                 1.0           1.0   \n",
      "SVM_pred                       1.0                 0.5           0.5   \n",
      "Random_Forest_pred             0.5                 1.0           0.5   \n",
      "XGBoost_pred                   0.5                 0.5           1.0   \n",
      "Set_1_match                    0.5                 0.5           0.5   \n",
      "Set_2_match                    0.5                 0.5           0.5   \n",
      "\n",
      "                          Set_1_match  Set_2_match  \n",
      "GroNLP_pred                       0.5          0.5  \n",
      "mBERT_pred                        0.5          0.5  \n",
      "RobBERT_pred                      0.5          0.5  \n",
      "Naive_Bayes_pred                  0.5          0.5  \n",
      "Logistic_Regression_pred          0.5          0.5  \n",
      "SVM_pred                          0.5          0.5  \n",
      "Random_Forest_pred                0.5          0.5  \n",
      "XGBoost_pred                      0.5          0.5  \n",
      "Set_1_match                       1.0          1.0  \n",
      "Set_2_match                       1.0          1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "models = [col for col in df.columns if col.endswith('_label') or col.endswith('_pred') or col.endswith('_match')]\n",
    "\n",
    "# Ground truth\n",
    "y_true = df[\"label\"].values\n",
    "\n",
    "# Get per-class F1 scores\n",
    "def get_f1_per_class(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    return pd.Series(\n",
    "        {label: metrics[\"f1-score\"]\n",
    "         for label, metrics in report.items()\n",
    "         if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]}\n",
    "    )\n",
    "\n",
    "# Prepare matrix\n",
    "p_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
    "\n",
    "# Compare all model pairs\n",
    "for model1, model2 in combinations(models, 2):\n",
    "    f1_1 = get_f1_per_class(y_true, df[model1])\n",
    "    f1_2 = get_f1_per_class(y_true, df[model2])\n",
    "\n",
    "    # Ensure same class order\n",
    "    f1_1, f1_2 = f1_1.align(f1_2, join='inner')\n",
    "\n",
    "    try:\n",
    "        stat, p = wilcoxon(f1_1, f1_2)\n",
    "    except ValueError:\n",
    "        # If samples are too small or identical\n",
    "        p = np.nan\n",
    "\n",
    "    p_matrix.loc[model1, model2] = p\n",
    "    p_matrix.loc[model2, model1] = p\n",
    "\n",
    "# Fill diagonal with 1s\n",
    "np.fill_diagonal(p_matrix.values, 1.0)\n",
    "\n",
    "# Print nicely\n",
    "print(\"\\n📊 Wilcoxon test p-values (lower = significant difference in per-class F1):\")\n",
    "print(p_matrix.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cfe5b69-87bb-4961-aebd-02090c5229ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_contingency(y_true, pred1, pred2, name1, name2):\n",
    "    table = [[0, 0], [0, 0]]\n",
    "    for y, a, b in zip(y_true, pred1, pred2):\n",
    "        correct_a = int(a == y)\n",
    "        correct_b = int(b == y)\n",
    "        table[correct_b][correct_a] += 1\n",
    "    print(f\"\\n{name1} vs {name2} contingency table (McNemar):\")\n",
    "    print(pd.DataFrame(table, columns=[f\"{name2} correct\", f\"{name2} wrong\"], \n",
    "                             index=[f\"{name1} correct\", f\"{name1} wrong\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b128eba0-b1c9-420f-b012-cd1b572721e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m print_contingency(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic_Regression_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogReg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_label'"
     ]
    }
   ],
   "source": [
    "print_contingency(df['label'], df['predicted_label'], df['Logistic_Regression_pred'], 'BERT', 'LogReg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb8b09-8df0-4940-9e4e-203fc8f9a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "# Count correct predictions by BERT\n",
    "bert_correct = (df['predicted_label'] == df['label']).sum()\n",
    "total = len(df)\n",
    "\n",
    "print(f\"BERT predicted correctly on {bert_correct} out of {total} samples.\")\n",
    "print(f\"Accuracy: {bert_correct / total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877dc26-1b0d-4301-aefa-a0ae371fdfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Load your combined predictions\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "# Extract relevant predictions\n",
    "y_true = df['label']\n",
    "bert = df['predicted_label']\n",
    "logreg = df['Logistic_Regression_pred']\n",
    "\n",
    "# Count how many predictions each model got correct\n",
    "bert_correct = (bert == y_true)\n",
    "logreg_correct = (logreg == y_true)\n",
    "\n",
    "print(f\"BERT correct: {bert_correct.sum()} / {len(df)}\")\n",
    "print(f\"LogReg correct: {logreg_correct.sum()} / {len(df)}\")\n",
    "\n",
    "# Build McNemar contingency table from scratch\n",
    "# Rows: BERT (correct vs wrong)\n",
    "# Cols: LogReg (correct vs wrong)\n",
    "table = pd.crosstab(bert_correct, logreg_correct)\n",
    "table.index = ['BERT wrong', 'BERT correct']\n",
    "table.columns = ['LogReg wrong', 'LogReg correct']\n",
    "\n",
    "print(\"\\n✅ Contingency Table:\")\n",
    "print(table)\n",
    "\n",
    "# Optional: McNemar p-value\n",
    "print(\"\\nMcNemar's test p-value:\", mcnemar(table.values, exact=True).pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332aa6a-d32d-41bb-a8af-453fd045f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_f1_diff(y_true, y_pred1, y_pred2, n=1000):\n",
    "    diffs = []\n",
    "    for _ in range(n):\n",
    "        idx = np.random.choice(len(y_true), len(y_true), replace=True)\n",
    "        f1_1 = f1_score(y_true[idx], y_pred1[idx])\n",
    "        f1_2 = f1_score(y_true[idx], y_pred2[idx])\n",
    "        diffs.append(f1_1 - f1_2)\n",
    "    return np.mean(diffs), np.percentile(diffs, [2.5, 97.5])\n",
    "\n",
    "# Example use\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"combined_predictions.csv\")\n",
    "\n",
    "y_true = df['label'].values\n",
    "bert = df['predicted_label'].values\n",
    "logreg = df['Logistic_Regression_pred'].values\n",
    "\n",
    "mean_diff, ci = bootstrap_f1_diff(y_true, bert, logreg)\n",
    "\n",
    "print(f\"🔍 BERT vs Logistic Regression — F1 difference:\")\n",
    "print(f\"Mean diff: {mean_diff:.4f}\")\n",
    "print(f\"95% CI:    [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283eefea-c588-4237-a5b9-9227cdfbb374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
