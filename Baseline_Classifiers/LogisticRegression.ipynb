{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7410b026-af28-4d8d-8409-1210b2f944d8",
   "metadata": {},
   "source": [
    "**Baseline classifier model. Op basis van TF-IDF Logistische regressie. Moet class imbalance nog aanpakken.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdbe6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nwat gebeurd is: eerst GridSearchCV gerund en daarna gezien dat deze configuratie met l1 dus de beste is\"\\ndaarna dus de l1 configuratie gebruikt en de beste parameters nog aan het zoeken\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "wat gebeurd is: eerst GridSearchCV gerund en daarna gezien dat deze configuratie met l1 dus de beste is\"\n",
    "daarna dus de l1 configuratie gebruikt en de beste parameters nog aan het zoeken\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ea7584-6167-4c51-9619-0dfedff4f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfc791d-5b32-4ae9-9440-c3b62b0aa114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset overzicht:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92503 entries, 0 to 92502\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   context      92010 non-null  object\n",
      " 1   question     92503 non-null  object\n",
      " 2   statistical  92503 non-null  int64 \n",
      " 3   theme        92503 non-null  object\n",
      " 4   file_name    92503 non-null  object\n",
      " 5   clean_text   92503 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.2+ MB\n",
      "None\n",
      "                                             context  \\\n",
      "0  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "1  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "2  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "3  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "4  Ondertussen is de eerstelijnszone BruZEL al me...   \n",
      "\n",
      "                                            question  statistical  \\\n",
      "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...            0   \n",
      "1                            a)Wat liep er moeilijk?            0   \n",
      "2  Met welke  uitdagingen  werd BruZEL het afgelo...            0   \n",
      "3      Hoe kunnen  die uitdagingen worden aangepakt?            0   \n",
      "4                                b)Wat liep er goed?            0   \n",
      "\n",
      "                        theme    file_name  \\\n",
      "0  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "1  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "2  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "3  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "4  Brussel en de Vlaamse Rand  1752898.txt   \n",
      "\n",
      "                                          clean_text  \n",
      "0  Zoals alle eerstelijnszones kreeg ook BruZEL h...  \n",
      "1                              Wat liep er moeilijk?  \n",
      "2  Met welke uitdagingen werd BruZEL het afgelope...  \n",
      "3       Hoe kunnen die uitdagingen worden aangepakt?  \n",
      "4                                  Wat liep er goed?  \n"
     ]
    }
   ],
   "source": [
    "script_dir = os.getcwd() # Ga Ã©Ã©n map omhoog om 'baseline' te verwijderen en ga naar 'Data'\n",
    "project_root = os.path.dirname(script_dir)  # Dit verwijdert 'baseline' van het script_dir\n",
    "data_folder = os.path.join(project_root, \"Data\")\n",
    "\n",
    "# 1. Dataset inladen\n",
    "file_path = os.path.join(data_folder, \"Grote_data_NoDupsLessThemes.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 2. Data verkennen\n",
    "print(\"Dataset overzicht:\")\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394b4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 92503\n",
      "Original dataset size: 74002\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "print(f\"Number of rows after filtering: {len(df)}\")\n",
    "# Convert text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df[\"question\"])\n",
    "y = df[\"theme\"]\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original dataset size: {X_train.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c7b2e3-b6b9-4ebc-85a9-5ed73d65e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV wordt uitgevoerd... Dit kan even duren ðŸš€\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Beste parameters: {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                   Begroting       0.78      0.58      0.66       272\n",
      "           Bestuur en Beleid       0.67      0.61      0.64      1147\n",
      "  Brussel en de Vlaamse Rand       0.76      0.41      0.53       165\n",
      "     Cultuur en Communicatie       0.63      0.53      0.58       629\n",
      "          Economie en Arbeid       0.65      0.65      0.65      1575\n",
      "                     Energie       0.67      0.53      0.59       621\n",
      "                   FinanciÃ«n       0.77      0.59      0.67       233\n",
      "       Internationaal Beleid       0.73      0.56      0.63       390\n",
      "      Justitie en Handhaving       0.72      0.43      0.54       260\n",
      "          Milieu en Landbouw       0.60      0.66      0.63      2906\n",
      "Mobiliteit en Infrastructuur       0.74      0.84      0.78      4985\n",
      "    Onderwijs en Samenleving       0.63      0.65      0.64      2129\n",
      "           Onroerend erfgoed       0.74      0.40      0.52       216\n",
      "                       Sport       0.71      0.29      0.42       102\n",
      "                    Toerisme       0.73      0.47      0.57       292\n",
      "       Welzijn en Gezondheid       0.60      0.62      0.61      1913\n",
      "     Wetenschap en Innovatie       0.45      0.11      0.18        44\n",
      "                       Wonen       0.61      0.52      0.56       622\n",
      "\n",
      "                    accuracy                           0.67     18501\n",
      "                   macro avg       0.68      0.52      0.58     18501\n",
      "                weighted avg       0.67      0.67      0.66     18501\n",
      "\n",
      "      Metric     Score\n",
      "0   Accuracy  0.666234\n",
      "1  Precision  0.667156\n",
      "2     Recall  0.666234\n",
      "3         F1  0.661593\n"
     ]
    }
   ],
   "source": [
    "# âœ… Hyperparameter tuning met GridSearchCV\n",
    "# Conditional grid - alleen 'elasticnet' krijgt l1_ratio\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Minder waarden testen\n",
    "    'penalty': ['l1', 'l2'],  # ElasticNet weglaten als dat niet veel verschil maakt\n",
    "    'solver': ['saga']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=500),  # Model\n",
    "    param_grid,  # Zoekruimte\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"f1_weighted\",  # Optimaliseer voor F1-score\n",
    "    n_jobs=-1,  # Parallel berekenen\n",
    "    verbose=2  # âœ… Geeft gedetailleerde logs per iteratie\n",
    ")\n",
    "\n",
    "print(\"GridSearchCV wordt uitgevoerd... Dit kan even duren ðŸš€\")\n",
    "grid.fit(X_train, y_train)  # GridSearch starten\n",
    "\n",
    "# âœ… Print beste parameters en scores\n",
    "print(f\"Beste parameters: {grid.best_params_}\")\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# âœ… Evaluatie op de testset\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# âœ… Compute Accuracy, Precision, Recall, and F1-score (same format as BERT output)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "\n",
    "# âœ… Format Output as DataFrame\n",
    "baseline_results = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Score\": [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# âœ… Display results in a table format\n",
    "print(baseline_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663f8b1-55b7-4db8-8a76-487c75c74975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
