{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3388dcc-a590-4949-9893-fdb6222f5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982c9929-d9a8-41d8-9354-3f450ff95040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All question texts are identical across models.\n",
      "✅ File saved to: C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "# # === Step 1: Load predictions ===\n",
    "# svm_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\Dataset3_no_context\\collection_svm\\SVM_test_predictions.xlsx\")\n",
    "# lr_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\Dataset3_no_context\\collection_logreg\\logreg_test_predictions.xlsx\")\n",
    "# nb_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\Dataset3_no_context\\collection_NB\\NaiveBayes_test_predictions.xlsx\")\n",
    "# rf_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\Dataset3_no_context\\collection_rf\\RF_test_predictions.xlsx\")\n",
    "# xgb_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\Dataset3_no_context\\collection_xgb\\xgb_test_predictions.xlsx\")\n",
    "\n",
    "# # === Step 2: Load label mappings ===\n",
    "# with open(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\final\\mBERT\\Run_2025-05-08_13-53\\label_mappings.json\", encoding=\"utf-8\") as f:\n",
    "#     mappings = json.load(f)\n",
    "\n",
    "# theme_to_id = mappings[\"theme_to_id\"]\n",
    "\n",
    "# # === Step 3: Check if all questions are identical ===\n",
    "# all_questions_equal = (\n",
    "#     (svm_df[\"Question\"] == lr_df[\"Question\"]).all() and\n",
    "#     (svm_df[\"Question\"] == nb_df[\"Question\"]).all() and\n",
    "#     (svm_df[\"Question\"] == rf_df[\"Question\"]).all() and\n",
    "#     (svm_df[\"Question\"] == xgb_df[\"Question\"]).all()\n",
    "# )\n",
    "\n",
    "\n",
    "# if not all_questions_equal:\n",
    "#     raise ValueError(\"❌ The question texts are not identical across all prediction files!\")\n",
    "\n",
    "# print(\"✅ All question texts are identical across models.\")\n",
    "\n",
    "# # === Step 4: Convert predictions to theme IDs ===\n",
    "# def map_preds(df):\n",
    "#     return df[\"Predicted Label\"].map(theme_to_id)\n",
    "\n",
    "# combined_df = pd.DataFrame({\n",
    "#     \"question\": svm_df[\"Question\"],\n",
    "#     \"true_label\": svm_df[\"True Label\"].map(theme_to_id),  # assumes true label in same format\n",
    "#     \"LR\": map_preds(lr_df),\n",
    "#     \"SVM\": map_preds(svm_df),\n",
    "#     \"NB\": map_preds(nb_df),\n",
    "#     \"RF\": map_preds(rf_df),\n",
    "#     \"XGB\": map_preds(xgb_df),\n",
    "# })\n",
    "\n",
    "# # === Step 5: Export to Excel ===\n",
    "# output_path = r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\"\n",
    "# combined_df.to_excel(output_path, index=False)\n",
    "# print(f\"✅ File saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36bd78ac-52bd-456c-977a-1d646278480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All question texts match.\n",
      "✅ BERT predictions appended and saved to: C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # === Step 1: Load the existing predictions file ===\n",
    "# output_path = r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\"\n",
    "# existing_df = pd.read_excel(output_path)\n",
    "\n",
    "# # === Step 2: Load BERT model prediction files ===\n",
    "# bert_df = pd.read_csv(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\final\\GroNLP\\Run_2025-04-23_18-28\\test_predictions.csv\")\n",
    "# robb_df = pd.read_csv(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\final\\roBERT\\Run_2025-05-04_22-35\\test_predictions.csv\")\n",
    "# mbert_df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\results\\final\\mBERT\\Run_2025-05-08_13-53\\predictions.xlsx\")\n",
    "\n",
    "# # === Step 3: Check if questions align ===\n",
    "# if not (\n",
    "#     (existing_df[\"question\"] == bert_df[\"text\"]).all() and\n",
    "#     (existing_df[\"question\"] == robb_df[\"text\"]).all() and\n",
    "#     (existing_df[\"question\"] == mbert_df[\"clean_text\"]).all()\n",
    "# ):\n",
    "#     raise ValueError(\"❌ Question texts do not match across all files.\")\n",
    "\n",
    "# print(\"✅ All question texts match.\")\n",
    "\n",
    "# # === Step 4: Add BERT model predictions as new columns ===\n",
    "# existing_df[\"BERT\"] = bert_df[\"predicted_label\"]\n",
    "# existing_df[\"RobBERT\"] = robb_df[\"predicted_label\"]\n",
    "# existing_df[\"mBERT\"] = mbert_df[\"mBERT\"]\n",
    "\n",
    "# # === Step 5: Save back to same Excel file ===\n",
    "# existing_df.to_excel(output_path, index=False)\n",
    "# print(f\"✅ BERT predictions appended and saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e076dd-a2c1-485a-8895-abfa1f47881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model A  Model B  Wilcoxon Statistic        p-value  \\\n",
      "0        LR      SVM            202020.0   4.860995e-01   \n",
      "1        LR       NB            711483.5   4.853962e-02   \n",
      "2        LR       RF            696580.5   1.885173e-08   \n",
      "3        LR      XGB            719352.0   2.022954e-02   \n",
      "4        LR     BERT            445284.0  2.065536e-150   \n",
      "5        LR  RobBERT            458398.5  8.753036e-144   \n",
      "6        LR    mBERT            507448.5  1.045389e-125   \n",
      "7       SVM       NB            601500.0   1.009409e-02   \n",
      "8       SVM       RF            693420.0   2.636533e-07   \n",
      "9       SVM      XGB            562504.5   5.263321e-02   \n",
      "10      SVM     BERT            422052.5  1.509307e-148   \n",
      "11      SVM  RobBERT            413553.0  5.158760e-144   \n",
      "12      SVM    mBERT            430508.0  3.125149e-128   \n",
      "13       NB       RF            716485.0   1.172775e-13   \n",
      "14       NB      XGB            665464.0   1.615547e-05   \n",
      "15       NB     BERT            607950.0  2.400090e-152   \n",
      "16       NB  RobBERT            590881.5  1.826353e-148   \n",
      "17       NB    mBERT            636957.0  1.390012e-131   \n",
      "18       RF      XGB            440300.0   1.634242e-04   \n",
      "19       RF     BERT            476238.0  1.396030e-102   \n",
      "20       RF  RobBERT            507841.5   1.066487e-95   \n",
      "21       RF    mBERT            570994.5   4.009191e-80   \n",
      "22      XGB     BERT            442753.5  1.475624e-131   \n",
      "23      XGB  RobBERT            445299.0  3.898559e-126   \n",
      "24      XGB    mBERT            486591.0  2.176875e-109   \n",
      "25     BERT  RobBERT            230351.0   3.867204e-01   \n",
      "26     BERT    mBERT            278159.0   2.899454e-03   \n",
      "27  RobBERT    mBERT            270327.0   2.816656e-02   \n",
      "\n",
      "   Statistically Different (α = 0.05)  \n",
      "0                                  No  \n",
      "1                                 Yes  \n",
      "2                                 Yes  \n",
      "3                                 Yes  \n",
      "4                                 Yes  \n",
      "5                                 Yes  \n",
      "6                                 Yes  \n",
      "7                                 Yes  \n",
      "8                                 Yes  \n",
      "9                                  No  \n",
      "10                                Yes  \n",
      "11                                Yes  \n",
      "12                                Yes  \n",
      "13                                Yes  \n",
      "14                                Yes  \n",
      "15                                Yes  \n",
      "16                                Yes  \n",
      "17                                Yes  \n",
      "18                                Yes  \n",
      "19                                Yes  \n",
      "20                                Yes  \n",
      "21                                Yes  \n",
      "22                                Yes  \n",
      "23                                Yes  \n",
      "24                                Yes  \n",
      "25                                 No  \n",
      "26                                Yes  \n",
      "27                                Yes  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "\n",
    "# === Load your data\n",
    "df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\")  # Update path if needed\n",
    "\n",
    "# === Define model columns\n",
    "model_cols = ['LR', 'SVM', 'NB', 'RF', 'XGB', 'BERT', 'RobBERT', 'mBERT']\n",
    "\n",
    "# === Convert predictions to binary correctness (1 = correct, 0 = incorrect)\n",
    "correct_matrix = {\n",
    "    model: (df[model] == df[\"True Label\"]).astype(int) for model in model_cols\n",
    "}\n",
    "correct_df = pd.DataFrame(correct_matrix)\n",
    "\n",
    "# === Wilcoxon tests with α = 0.05 (no correction)\n",
    "alpha = 0.05\n",
    "results = []\n",
    "\n",
    "for model_a, model_b in combinations(model_cols, 2):\n",
    "    try:\n",
    "        stat, p_value = wilcoxon(correct_df[model_a], correct_df[model_b])\n",
    "        significant = \"Yes\" if p_value < alpha else \"No\"\n",
    "        results.append({\n",
    "            \"Model A\": model_a,\n",
    "            \"Model B\": model_b,\n",
    "            \"Wilcoxon Statistic\": stat,\n",
    "            \"p-value\": p_value,\n",
    "            \"Statistically Different (α = 0.05)\": significant\n",
    "        })\n",
    "    except ValueError as e:\n",
    "        results.append({\n",
    "            \"Model A\": model_a,\n",
    "            \"Model B\": model_b,\n",
    "            \"Wilcoxon Statistic\": None,\n",
    "            \"p-value\": None,\n",
    "            \"Statistically Different (α = 0.05)\": \"Error\",\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# === Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"wilcoxon_pairwise_no_correction.xlsx\", index=False)\n",
    "\n",
    "# === Print some of the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75195de2-b498-4016-b2d7-0a6e15b92a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model A  Model B  Wilcoxon Statistic   p-value  \\\n",
      "0        LR      SVM               106.0  0.196369   \n",
      "1        LR       NB               134.0  0.017117   \n",
      "2        LR       RF                17.0  0.999355   \n",
      "3        LR      XGB                47.0  0.955116   \n",
      "4        LR     BERT                 0.0  1.000000   \n",
      "5        LR  RobBERT                 0.0  1.000000   \n",
      "6        LR    mBERT                 0.0  1.000000   \n",
      "7       SVM       NB                89.0  0.449287   \n",
      "8       SVM       RF                23.0  0.997997   \n",
      "9       SVM      XGB                38.0  0.982883   \n",
      "10      SVM     BERT                 0.0  1.000000   \n",
      "11      SVM  RobBERT                 0.0  1.000000   \n",
      "12      SVM    mBERT                 0.0  1.000000   \n",
      "13       NB       RF                 3.0  0.999749   \n",
      "14       NB      XGB                16.0  0.997908   \n",
      "15       NB     BERT                 0.0  1.000000   \n",
      "16       NB  RobBERT                 0.0  1.000000   \n",
      "17       NB    mBERT                 0.0  1.000000   \n",
      "18       RF      XGB               144.0  0.000698   \n",
      "19       RF     BERT                 0.0  1.000000   \n",
      "20       RF  RobBERT                 0.0  1.000000   \n",
      "21       RF    mBERT                 2.0  0.999992   \n",
      "22      XGB     BERT                 0.0  1.000000   \n",
      "23      XGB  RobBERT                 0.0  1.000000   \n",
      "24      XGB    mBERT                 0.0  1.000000   \n",
      "25     BERT  RobBERT               130.0  0.026928   \n",
      "26     BERT    mBERT               131.0  0.024139   \n",
      "27  RobBERT    mBERT                97.0  0.319847   \n",
      "\n",
      "   Statistically Different (α = 0.05)  \n",
      "0                                  No  \n",
      "1                                 Yes  \n",
      "2                                  No  \n",
      "3                                  No  \n",
      "4                                  No  \n",
      "5                                  No  \n",
      "6                                  No  \n",
      "7                                  No  \n",
      "8                                  No  \n",
      "9                                  No  \n",
      "10                                 No  \n",
      "11                                 No  \n",
      "12                                 No  \n",
      "13                                 No  \n",
      "14                                 No  \n",
      "15                                 No  \n",
      "16                                 No  \n",
      "17                                 No  \n",
      "18                                Yes  \n",
      "19                                 No  \n",
      "20                                 No  \n",
      "21                                 No  \n",
      "22                                 No  \n",
      "23                                 No  \n",
      "24                                 No  \n",
      "25                                Yes  \n",
      "26                                Yes  \n",
      "27                                 No  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "\n",
    "# === Load your data\n",
    "df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\")\n",
    "\n",
    "# === Define model columns\n",
    "model_cols = ['LR', 'SVM', 'NB', 'RF', 'XGB', 'BERT', 'RobBERT', 'mBERT']\n",
    "true_labels = df[\"True Label\"]\n",
    "classes = sorted(true_labels.unique())\n",
    "\n",
    "# === Wilcoxon test on per-class F1-scores\n",
    "alpha = 0.05\n",
    "results = []\n",
    "\n",
    "for model_a, model_b in combinations(model_cols, 2):\n",
    "    # Compute classification reports\n",
    "    report_a = classification_report(true_labels, df[model_a], labels=classes, output_dict=True, zero_division=0)\n",
    "    report_b = classification_report(true_labels, df[model_b], labels=classes, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Extract F1-scores per class\n",
    "    f1_a = [report_a[str(label)][\"f1-score\"] for label in classes]\n",
    "    f1_b = [report_b[str(label)][\"f1-score\"] for label in classes]\n",
    "    \n",
    "    # Run Wilcoxon test\n",
    "    try:\n",
    "        stat, p_value = wilcoxon(f1_a, f1_b, alternative ='greater')\n",
    "        significant = \"Yes\" if p_value < alpha else \"No\"\n",
    "    except ValueError as e:\n",
    "        stat, p_value = None, None\n",
    "        significant = \"Error\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Model A\": model_a,\n",
    "        \"Model B\": model_b,\n",
    "        \"Wilcoxon Statistic\": stat,\n",
    "        \"p-value\": p_value,\n",
    "        \"Statistically Different (α = 0.05)\": significant\n",
    "    })\n",
    "\n",
    "# === Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"wilcoxon_per_class_f1_no_correction.xlsx\", index=False)\n",
    "\n",
    "# === Print some of the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ea582b-cae1-481c-9bde-c016fc5fd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class F1-score comparison:\n",
      "Brussel en de Vlaamse Rand     BERT: 0.74 | RobBERT: 0.76 | Δ: -0.03\n",
      "Energie                        BERT: 0.75 | RobBERT: 0.74 | Δ: +0.01\n",
      "Milieu en Landbouw             BERT: 0.76 | RobBERT: 0.76 | Δ: -0.00\n",
      "Toerisme                       BERT: 0.65 | RobBERT: 0.64 | Δ: +0.01\n",
      "Economie en Arbeid             BERT: 0.77 | RobBERT: 0.76 | Δ: +0.01\n",
      "Sport                          BERT: 0.64 | RobBERT: 0.62 | Δ: +0.02\n",
      "Bestuur en Beleid              BERT: 0.73 | RobBERT: 0.74 | Δ: -0.01\n",
      "Justitie en Handhaving         BERT: 0.68 | RobBERT: 0.65 | Δ: +0.03\n",
      "Cultuur en Communicatie        BERT: 0.68 | RobBERT: 0.67 | Δ: +0.01\n",
      "Mobiliteit en Infrastructuur   BERT: 0.87 | RobBERT: 0.87 | Δ: +0.00\n",
      "Welzijn en Gezondheid          BERT: 0.74 | RobBERT: 0.75 | Δ: -0.01\n",
      "Begroting                      BERT: 0.77 | RobBERT: 0.73 | Δ: +0.04\n",
      "Wonen                          BERT: 0.71 | RobBERT: 0.67 | Δ: +0.04\n",
      "Onderwijs en Samenleving       BERT: 0.73 | RobBERT: 0.73 | Δ: -0.00\n",
      "Internationaal Beleid          BERT: 0.73 | RobBERT: 0.70 | Δ: +0.03\n",
      "Onroerend erfgoed              BERT: 0.68 | RobBERT: 0.68 | Δ: +0.00\n",
      "Financiën                      BERT: 0.73 | RobBERT: 0.73 | Δ: -0.00\n",
      "Wetenschap en Innovatie        BERT: 0.28 | RobBERT: 0.19 | Δ: +0.09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === Load your data\n",
    "df = pd.read_excel(r\"C:\\Users\\jefva\\Documents\\Master\\Thesis_s2\\Code\\Baseline_Classifiers\\traditional_predictions_table.xlsx\")\n",
    "\n",
    "# === Setup\n",
    "true_labels = df[\"True Label\"]\n",
    "model_a = \"BERT\"\n",
    "model_b = \"RobBERT\"\n",
    "classes = sorted(true_labels.unique())\n",
    "\n",
    "# === Generate classification reports\n",
    "report_a = classification_report(true_labels, df[model_a], labels=classes, output_dict=True, zero_division=0)\n",
    "report_b = classification_report(true_labels, df[model_b], labels=classes, output_dict=True, zero_division=0)\n",
    "\n",
    "# === Optional: map ID to theme name (update if you have your own mapping)\n",
    "theme_to_id = {\n",
    "    \"Brussel en de Vlaamse Rand\": 0,\n",
    "    \"Energie\": 1,\n",
    "    \"Milieu en Landbouw\": 2,\n",
    "    \"Toerisme\": 3,\n",
    "    \"Economie en Arbeid\": 4,\n",
    "    \"Sport\": 5,\n",
    "    \"Bestuur en Beleid\": 6,\n",
    "    \"Justitie en Handhaving\": 7,\n",
    "    \"Cultuur en Communicatie\": 8,\n",
    "    \"Mobiliteit en Infrastructuur\": 9,\n",
    "    \"Welzijn en Gezondheid\": 10,\n",
    "    \"Begroting\": 11,\n",
    "    \"Wonen\": 12,\n",
    "    \"Onderwijs en Samenleving\": 13,\n",
    "    \"Internationaal Beleid\": 14,\n",
    "    \"Onroerend erfgoed\": 15,\n",
    "    \"Financiën\": 16,\n",
    "    \"Wetenschap en Innovatie\": 17\n",
    "}\n",
    "id_to_theme = {v: k for k, v in theme_to_id.items()}\n",
    "unique_themes = [id_to_theme[i] for i in classes]\n",
    "\n",
    "# === Print F1-scores side by side\n",
    "print(\"\\nPer-class F1-score comparison:\")\n",
    "for theme, i in zip(unique_themes, classes):\n",
    "    f1_a = report_a[str(i)][\"f1-score\"]\n",
    "    f1_b = report_b[str(i)][\"f1-score\"]\n",
    "    diff = f1_a - f1_b\n",
    "    print(f\"{theme:30s} BERT: {f1_a:.2f} | RobBERT: {f1_b:.2f} | Δ: {diff:+.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d5279-721c-4b0c-826f-c0bec649df7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
