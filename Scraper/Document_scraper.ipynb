{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24c7fc1-0423-4d7f-8a38-8c00f7000096",
   "metadata": {},
   "source": [
    "**Code die scraped op de site van het Vlaams Parlement. De code scraped de vragen en interpellaties in een gegeven tijdsinterval. Extract ook het thema van de vraag, de pdf-link en de link van de webfiche. Converteert het document naar TXT files. Ook de metadata wordt geplaatst in het begin van deze TXT files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e560fa-095c-4084-828b-6eb6680f868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=548&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=549&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=550&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=551&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=552&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=553&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=554&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=555&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=556&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=557&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=558&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Skipping non-PDF or bad response from https://docs.vlaamsparlement.be/pfile?id=1747252 (Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=559&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=560&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=561&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=562&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=563&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=564&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=565&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=566&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=567&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=568&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=569&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=570&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Skipping non-PDF or bad response from https://docs.vlaamsparlement.be/pfile?id=1755283 (Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=571&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=572&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=573&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=574&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=575&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=576&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=577&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=578&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Skipping non-PDF or bad response from https://docs.vlaamsparlement.be/pfile?id=1763047 (Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=579&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=580&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=581&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=582&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=583&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=584&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=585&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=586&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Skipping non-PDF or bad response from https://docs.vlaamsparlement.be/pfile?id=1751405 (Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=587&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=588&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=589&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=590&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=591&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=592&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=593&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=594&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=595&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=596&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=597&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=598&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=599&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=600&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=601&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=602&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=603&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=604&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=605&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=606&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=607&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=608&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=609&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=610&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=611&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=612&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=613&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=614&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=615&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=616&period=custom&start_period=2019-03-01&end_period=2023-02-28&aggregaat[]=Vraag of interpellatie\n",
      "Skipping non-PDF or bad response from https://docs.vlaamsparlement.be/pfile?id=1735447 (Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n",
      "An error occurred while scraping https://www.vlaamsparlement.be/parlementaire-documenten/schriftelijke-vragen/1534678: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00708073+60707]\n",
      "\tGetHandleVerifier [0x007080B4+60772]\n",
      "\t(No symbol) [0x005304FE]\n",
      "\t(No symbol) [0x0056B898]\n",
      "\t(No symbol) [0x0059CF06]\n",
      "\t(No symbol) [0x005989D5]\n",
      "\t(No symbol) [0x00597F66]\n",
      "\t(No symbol) [0x005036E5]\n",
      "\t(No symbol) [0x00503C3E]\n",
      "\t(No symbol) [0x005040CD]\n",
      "\tGetHandleVerifier [0x0094BB53+2435075]\n",
      "\tGetHandleVerifier [0x009470F3+2416035]\n",
      "\tGetHandleVerifier [0x0096349C+2531660]\n",
      "\tGetHandleVerifier [0x0071F145+155125]\n",
      "\tGetHandleVerifier [0x00725AED+182173]\n",
      "\t(No symbol) [0x005033B0]\n",
      "\t(No symbol) [0x00502BC3]\n",
      "\tGetHandleVerifier [0x00A6D23C+3620588]\n",
      "\tBaseThreadInitThunk [0x75C95D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77A6CF0B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77A6CE91+561]\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='docs.vlaamsparlement.be', port=443): Max retries exceeded with url: /pfile?id=1735555 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000186EE2B3F40>: Failed to resolve 'docs.vlaamsparlement.be' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000186EE2B3F40>: Failed to resolve 'docs.vlaamsparlement.be' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='docs.vlaamsparlement.be', port=443): Max retries exceeded with url: /pfile?id=1735555 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000186EE2B3F40>: Failed to resolve 'docs.vlaamsparlement.be' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 184\u001b[0m\n\u001b[0;32m    175\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.vlaamsparlement.be/nl/parlementaire-documenten\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m548\u001b[39m,\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregaat[]\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVraag of interpellatie\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m }\n\u001b[1;32m--> 184\u001b[0m \u001b[43miterate_scraper_over_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 162\u001b[0m, in \u001b[0;36miterate_scraper_over_pages\u001b[1;34m(base_url, query_params)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m combined_links:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_link\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[1;32m--> 162\u001b[0m         \u001b[43mdownload_and_convert_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdf_link\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument_fiche\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverslag_link\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[0;32m    164\u001b[0m         download_verslag(driver, entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverslag_link\u001b[39m\u001b[38;5;124m\"\u001b[39m], download_dir, entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_fiche\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 92\u001b[0m, in \u001b[0;36mdownload_and_convert_file\u001b[1;34m(driver, url, download_dir, thema_link)\u001b[0m\n\u001b[0;32m     89\u001b[0m thema_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(thema) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m file_id \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 92\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     95\u001b[0m content_type \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='docs.vlaamsparlement.be', port=443): Max retries exceeded with url: /pfile?id=1735555 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000186EE2B3F40>: Failed to resolve 'docs.vlaamsparlement.be' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import io\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "######### Functions to scrape links for downloadable PDF and HTML files #########\n",
    "\n",
    "def scrape_combined_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    combined_links = []\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    for article in articles:\n",
    "        fiche_link = article.find(\"a\", string=re.compile(\"Bekijk documentenfiche\", re.IGNORECASE))\n",
    "        pdf_link = article.find(\"a\", string=re.compile(\"Download pdf\", re.IGNORECASE))\n",
    "        verslag_link = article.find(\"a\", string=re.compile(\"Bekijk verslag\", re.IGNORECASE))\n",
    "\n",
    "        if fiche_link:\n",
    "            fiche_url = urljoin(url, fiche_link[\"href\"])\n",
    "            combined_entry = {\"document_fiche\": fiche_url}\n",
    "\n",
    "            if pdf_link:\n",
    "                combined_entry[\"pdf_link\"] = urljoin(url, pdf_link[\"href\"])\n",
    "            if verslag_link:\n",
    "                combined_entry[\"verslag_link\"] = urljoin(url, verslag_link[\"href\"])\n",
    "\n",
    "            combined_links.append(combined_entry)\n",
    "\n",
    "    return combined_links\n",
    "\n",
    "########## Function to extract title and thema from the fiche ##########\n",
    "\n",
    "def scrape_title_and_thema_from_fiche(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow time for page to load\n",
    "\n",
    "        # Extract title from the page-subtitle class\n",
    "        try:\n",
    "            title_element = driver.find_element(By.CSS_SELECTOR, \".page-subtitle\")\n",
    "            title = title_element.text.strip()\n",
    "        except:\n",
    "            title = \"No Title Found\"\n",
    "\n",
    "        # Extract thema (topics)\n",
    "        thema_elements = driver.find_elements(By.CSS_SELECTOR, \"li.meeting-details__thema a\")\n",
    "        themas = [element.text.strip() for element in thema_elements]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n",
    "        title = \"Error Retrieving Title\"\n",
    "        themas = []\n",
    "\n",
    "    return title, themas\n",
    "\n",
    "########## Function to extract text from PDFs ##########\n",
    "\n",
    "def extract_text_from_pdf(pdf_content):\n",
    "    pdf_content = io.BytesIO(pdf_content)\n",
    "    reader = PyPDF2.PdfReader(pdf_content)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in range(len(reader.pages)):\n",
    "        page_text = reader.pages[page].extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "########## Function to download and extract PDF content ##########\n",
    "\n",
    "def download_and_convert_file(driver, url, download_dir, thema_link):\n",
    "    title, thema = scrape_title_and_thema_from_fiche(driver, thema_link)\n",
    "    thema_str = \"\\n\".join(thema) + \"\\n\"\n",
    "\n",
    "    file_id = url.split(\"=\")[-1]\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "\n",
    "    content_type = response.headers.get('content-type', '')\n",
    "    if 'application/pdf' in content_type and response.ok:\n",
    "        try:\n",
    "            text = extract_text_from_pdf(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract PDF at {url}: {e}\")\n",
    "            text = \"\"\n",
    "    else:\n",
    "        print(f\"Skipping non-PDF or bad response from {url} (Content-Type: {content_type})\")\n",
    "        text = \"\"\n",
    "\n",
    "    text_filename = os.path.join(download_dir, f\"{file_id}.txt\")\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(f\"title: {title}\\n\")\n",
    "        text_file.write(\"thema: \" + thema_str)\n",
    "        text_file.write(f\"pdf link: {url}\\n\")\n",
    "        text_file.write(f\"thema link: {thema_link}\\n\")\n",
    "        text_file.write(text)\n",
    "\n",
    "    return file_id, text\n",
    "\n",
    "########## Function to download and extract HTML report content ##########\n",
    "\n",
    "def download_verslag(driver, page_url, download_dir, thema_link):\n",
    "    title, thema = scrape_title_and_thema_from_fiche(driver, thema_link)\n",
    "    thema_str = \"\\n\".join(thema) + \"\\n\"\n",
    "\n",
    "    verslag_id = page_url.split(\"/\")[-1]\n",
    "    \n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    text = \"\\n\".join([p.get_text() for p in soup.find_all('p')])\n",
    "\n",
    "    text_filename = os.path.join(download_dir, f\"{verslag_id}.txt\")\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(f\"title: {title}\\n\")\n",
    "        text_file.write(\"thema: \" + thema_str)\n",
    "        text_file.write(f\"verslag link: {page_url}\\n\")\n",
    "        text_file.write(f\"thema link: {thema_link}\\n\")\n",
    "        text_file.write(text)\n",
    "\n",
    "########## Function to iterate over pages and scrape links ##########\n",
    "\n",
    "def iterate_scraper_over_pages(base_url, query_params):\n",
    "    download_dir = \"Test_data\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        page_num = query_params[\"page\"]\n",
    "        while True:\n",
    "            query_params[\"page\"] = page_num\n",
    "            url = base_url + \"?\" + \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "            print(f\"Scraping page: {url}\")\n",
    "\n",
    "            combined_links = scrape_combined_links(url)\n",
    "\n",
    "            if combined_links:\n",
    "                for entry in combined_links:\n",
    "                    if \"pdf_link\" in entry:\n",
    "                        download_and_convert_file(driver, entry[\"pdf_link\"], download_dir, entry[\"document_fiche\"])\n",
    "                    if \"verslag_link\" in entry:\n",
    "                        download_verslag(driver, entry[\"verslag_link\"], download_dir, entry[\"document_fiche\"])\n",
    "            else:\n",
    "                print(f\"No more links found on page {page_num}.\")\n",
    "                break\n",
    "\n",
    "            page_num += 1\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "########## Main Execution ##########\n",
    "\n",
    "base_url = \"https://www.vlaamsparlement.be/nl/parlementaire-documenten\"\n",
    "query_params = {\n",
    "    \"page\": 548,\n",
    "    \"period\": \"custom\",\n",
    "    \"start_period\": \"2019-03-01\",\n",
    "    \"end_period\": \"2023-02-28\",\n",
    "    \"aggregaat[]\": \"Vraag of interpellatie\"\n",
    "}\n",
    "\n",
    "iterate_scraper_over_pages(base_url, query_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35966160-3d37-4ce6-8481-5bb2f7492bcc",
   "metadata": {},
   "source": [
    "**Stuk hieronder is om te proberen namen van vraagsteller en ondervraagde minister mee te nemen maar werkt niet omdat ergens in de voorbije jaren de fiche van lay-out is veranderd**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2d8332-f17a-47e0-8267-1023c4e8874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://www.vlaamsparlement.be/nl/parlementaire-documenten?page=28&period=custom&start_period=2024-06-01&end_period=2024-11-30&aggregaat[]=Vraag of interpellatie\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 201\u001b[0m\n\u001b[0;32m    192\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.vlaamsparlement.be/nl/parlementaire-documenten\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m28\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregaat[]\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVraag of interpellatie\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m }\n\u001b[1;32m--> 201\u001b[0m \u001b[43miterate_scraper_over_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 179\u001b[0m, in \u001b[0;36miterate_scraper_over_pages\u001b[1;34m(base_url, query_params)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m combined_links:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_link\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[1;32m--> 179\u001b[0m         \u001b[43mdownload_and_convert_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdf_link\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument_fiche\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverslag_link\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[0;32m    181\u001b[0m         download_verslag(driver, entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverslag_link\u001b[39m\u001b[38;5;124m\"\u001b[39m], download_dir, entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_fiche\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 100\u001b[0m, in \u001b[0;36mdownload_and_convert_file\u001b[1;34m(driver, url, download_dir, thema_link)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_convert_file\u001b[39m(driver, url, download_dir, thema_link):\n\u001b[1;32m--> 100\u001b[0m     title, thema, vraagsteller, minister \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_fiche_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthema_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     thema_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(thema) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mscrape_fiche_details\u001b[1;34m(driver, url)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Allow time for page to load\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Extract title\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import io\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "######### Function to scrape links for downloadable PDF and HTML files #########\n",
    "\n",
    "def scrape_combined_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    combined_links = []\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    for article in articles:\n",
    "        fiche_link = article.find(\"a\", string=re.compile(\"Bekijk documentenfiche\", re.IGNORECASE))\n",
    "        pdf_link = article.find(\"a\", string=re.compile(\"Download pdf\", re.IGNORECASE))\n",
    "        verslag_link = article.find(\"a\", string=re.compile(\"Bekijk verslag\", re.IGNORECASE))\n",
    "\n",
    "        if fiche_link:\n",
    "            fiche_url = urljoin(url, fiche_link[\"href\"])\n",
    "            combined_entry = {\"document_fiche\": fiche_url}\n",
    "\n",
    "            if pdf_link:\n",
    "                combined_entry[\"pdf_link\"] = urljoin(url, pdf_link[\"href\"])\n",
    "            if verslag_link:\n",
    "                combined_entry[\"verslag_link\"] = urljoin(url, verslag_link[\"href\"])\n",
    "\n",
    "            combined_links.append(combined_entry)\n",
    "\n",
    "    return combined_links\n",
    "\n",
    "########## Function to extract title, thema, vraagsteller, and ondervraagde minister ##########\n",
    "\n",
    "def scrape_fiche_details(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow time for page to load\n",
    "\n",
    "        # Extract title\n",
    "        try:\n",
    "            title_element = driver.find_element(By.CSS_SELECTOR, \".page-subtitle\")\n",
    "            title = title_element.text.strip()\n",
    "        except:\n",
    "            title = \"No Title Found\"\n",
    "\n",
    "        # Extract thema\n",
    "        thema_elements = driver.find_elements(By.CSS_SELECTOR, \"li.meeting-details__thema a\")\n",
    "        themas = [element.text.strip() for element in thema_elements]\n",
    "\n",
    "        # Extract Vraagsteller and Ondervraagde Minister\n",
    "        vraagsteller, minister = \"Unknown\", \"Unknown\"\n",
    "        labels = driver.find_elements(By.CSS_SELECTOR, \".meeting-details__label\")\n",
    "        values = driver.find_elements(By.CSS_SELECTOR, \".meeting-details__value\")\n",
    "\n",
    "        for label, value in zip(labels, values):\n",
    "            label_text = label.text.strip()\n",
    "            value_text = value.text.strip()\n",
    "\n",
    "            if label_text == \"Vraagsteller\":\n",
    "                vraagsteller = value_text\n",
    "            elif label_text == \"Ondervraagde minister\":\n",
    "                minister = value_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        title, themas, vraagsteller, minister = \"Error\", [], \"Unknown\", \"Unknown\"\n",
    "\n",
    "    return title, themas, vraagsteller, minister\n",
    "\n",
    "########## Function to extract text from PDFs ##########\n",
    "\n",
    "def extract_text_from_pdf(pdf_content):\n",
    "    pdf_content = io.BytesIO(pdf_content)\n",
    "    reader = PyPDF2.PdfReader(pdf_content)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in range(len(reader.pages)):\n",
    "        page_text = reader.pages[page].extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "########## Function to download and extract PDF content ##########\n",
    "\n",
    "def download_and_convert_file(driver, url, download_dir, thema_link):\n",
    "    title, thema, vraagsteller, minister = scrape_fiche_details(driver, thema_link)\n",
    "    thema_str = \"\\n\".join(thema) + \"\\n\"\n",
    "\n",
    "    file_id = url.split(\"=\")[-1]\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "\n",
    "    content_type = response.headers.get('content-type', '')\n",
    "    if 'application/pdf' in content_type and response.ok:\n",
    "        try:\n",
    "            text = extract_text_from_pdf(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract PDF at {url}: {e}\")\n",
    "            text = \"\"\n",
    "    else:\n",
    "        print(f\"Skipping non-PDF or bad response from {url} (Content-Type: {content_type})\")\n",
    "        text = \"\"\n",
    "\n",
    "\n",
    "    text_filename = os.path.join(download_dir, f\"{file_id}.txt\")\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(f\"title: {title}\\n\")\n",
    "        text_file.write(\"thema: \" + thema_str)\n",
    "        text_file.write(f\"pdf link: {url}\\n\")\n",
    "        text_file.write(f\"thema link: {thema_link}\\n\")\n",
    "        text_file.write(f\"Vraagsteller: {vraagsteller}\\n\")\n",
    "        text_file.write(f\"Ondervraagde minister: {minister}\\n\")\n",
    "        text_file.write(text)\n",
    "\n",
    "    return file_id, text\n",
    "\n",
    "########## Function to download and extract HTML report content ##########\n",
    "\n",
    "def download_verslag(driver, page_url, download_dir, thema_link):\n",
    "    title, thema, vraagsteller, minister = scrape_fiche_details(driver, thema_link)\n",
    "    thema_str = \"\\n\".join(thema) + \"\\n\"\n",
    "\n",
    "    verslag_id = page_url.split(\"/\")[-1]\n",
    "    \n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    text = \"\\n\".join([p.get_text() for p in soup.find_all('p')])\n",
    "\n",
    "    text_filename = os.path.join(download_dir, f\"{verslag_id}.txt\")\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(f\"title: {title}\\n\")\n",
    "        text_file.write(\"thema: \" + thema_str)\n",
    "        text_file.write(f\"verslag link: {page_url}\\n\")\n",
    "        text_file.write(f\"thema link: {thema_link}\\n\")\n",
    "        text_file.write(f\"Vraagsteller: {vraagsteller}\\n\")\n",
    "        text_file.write(f\"Ondervraagde minister: {minister}\\n\")\n",
    "        text_file.write(text)\n",
    "\n",
    "########## Function to iterate over pages and scrape links ##########\n",
    "\n",
    "def iterate_scraper_over_pages(base_url, query_params):\n",
    "    download_dir = \"ScrapeddocumentsCorneel1202\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        page_num = query_params[\"page\"]\n",
    "        while True:\n",
    "            query_params[\"page\"] = page_num\n",
    "            url = base_url + \"?\" + \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "            print(f\"Scraping page: {url}\")\n",
    "\n",
    "            combined_links = scrape_combined_links(url)\n",
    "\n",
    "            if combined_links:\n",
    "                for entry in combined_links:\n",
    "                    if \"pdf_link\" in entry:\n",
    "                        download_and_convert_file(driver, entry[\"pdf_link\"], download_dir, entry[\"document_fiche\"])\n",
    "                    if \"verslag_link\" in entry:\n",
    "                        download_verslag(driver, entry[\"verslag_link\"], download_dir, entry[\"document_fiche\"])\n",
    "            else:\n",
    "                print(f\"No more links found on page {page_num}.\")\n",
    "                break\n",
    "\n",
    "            page_num += 1\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "########## Main Execution ##########\n",
    "\n",
    "base_url = \"https://www.vlaamsparlement.be/nl/parlementaire-documenten\"\n",
    "query_params = {\n",
    "    \"page\": 28,\n",
    "    \"period\": \"custom\",\n",
    "    \"start_period\": \"2024-06-01\",\n",
    "    \"end_period\": \"2024-11-30\",\n",
    "    \"aggregaat[]\": \"Vraag of interpellatie\"\n",
    "}\n",
    "\n",
    "iterate_scraper_over_pages(base_url, query_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a8b12-1c86-4198-876d-0d2995cfb2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
